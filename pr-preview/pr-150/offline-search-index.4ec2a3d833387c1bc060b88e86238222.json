[{"body":"Idea This proof of concept demonstrates containerized bots battling each other in a 1v1 match within a 2-dimensional space. To evaluate the optimal architecture for the final BattleBots platform, both a client/server implementation and a peer-to-peer implementation will be completed and compared.\nRequirements Client/Server Architecture Implement a client/server architecture to evaluate its suitability for the final BattleBots platform.\nPeer-to-Peer Architecture Implement a peer-to-peer architecture to compare against the client/server approach.\n1v1 Battle Bots compete in a 1v1 battle within a 2-dimensional space.\nContainerized Bots Each bot should be a container to ensure isolation and portability.\nLanguage-Agnostic Bot Implementation The game logic should be independent of each bot so that any programming language can be used to implement a bot.\nObservability Observability signals should be captured so the battle can be monitored in real-time.\nBattle Visualization A battle visualization should be implemented to display the battle state and actions.\nPending ADRs ADR-NNNN: Client/Server Architecture This ADR will document the design decisions for the client/server implementation, including the server’s responsibilities for game state management, turn coordination, and validation of bot actions.\nADR-NNNN: Peer-to-Peer Architecture This ADR will document the design decisions for the peer-to-peer implementation, including consensus mechanisms for game state, conflict resolution, and how bots communicate directly with each other.\nADR-NNNN: Game Runtime Architecture This ADR will define the “game loop” and core game mechanics, including turn-based vs. real-time gameplay, tick rates, state updates, and the overall flow of battle execution.\nADR-NNNN: Bot to Battle Server Interface This ADR will define the communication protocol between bots and the battle server or between bots in P2P mode, evaluating options such as gRPC, HTTP, or custom TCP/UDP packets.\nADR-NNNN: Observability Stack This ADR will document the observability architecture, including metrics collection, logging, tracing, and how battle telemetry is captured and exposed for monitoring and analysis.\nADR-NNNN: Battle Visualization This ADR will document the design of the battle visualization system, including the rendering approach, real-time updates, and how observability data is translated into visual representations.\n","categories":"","description":"Documents the proof of concept for running a 1v1 battle between two containerized bots using podman-compose\n","excerpt":"Documents the proof of concept for running a 1v1 battle between two …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/0001-poc/","tags":"","title":"[0001] Proof of Concept - 1v1 Battle"},{"body":" Context and Problem Statement As the project grows, architectural decisions are made that have long-term impacts on the system’s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.\nHow should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?\nDecision Drivers Need for clear documentation of architectural decisions and their rationale Easy accessibility and searchability of past decisions Low barrier to entry for creating and maintaining decision records Integration with existing documentation workflow Version control friendly format Industry-standard approach that team members may already be familiar with Considered Options MADR (Markdown Architectural Decision Records) ADR using custom format Wiki-based documentation No formal ADR process Decision Outcome Chosen option: “MADR (Markdown Architectural Decision Records)”, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.\nConsequences Good, because MADR is a widely adopted standard with clear documentation and examples Good, because markdown files are easy to create, edit, and review through pull requests Good, because ADRs will be version-controlled alongside code, maintaining historical context Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions Good, because team members can easily search and reference past decisions Neutral, because requires discipline to maintain and update ADR status as decisions evolve Bad, because team members need to learn and follow the MADR format conventions Confirmation Compliance will be confirmed through:\nCode reviews ensuring new architectural decisions are documented as ADRs ADRs are stored in docs/content/r\u0026d/adrs/ following the naming convention NNNN-title-with-dashes.md Regular reviews during architecture discussions to reference and update existing ADRs Pros and Cons of the Options MADR (Markdown Architectural Decision Records) MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.\nGood, because it’s a well-established standard with extensive documentation Good, because markdown is simple, portable, and version-control friendly Good, because it provides a clear structure while remaining flexible Good, because it integrates with static site generators and documentation tools Good, because it’s lightweight and doesn’t require special tools Neutral, because it requires some initial learning of the format Neutral, because maintaining consistency requires discipline ADR using custom format Create our own custom format for architectural decision records.\nGood, because we can tailor it exactly to our needs Bad, because it requires defining and maintaining our own standard Bad, because new team members won’t be familiar with the format Bad, because we lose the benefits of community knowledge and tooling Bad, because it may evolve inconsistently over time Wiki-based documentation Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.\nGood, because wikis provide easy editing and hyperlinking Good, because some team members may be familiar with wiki tools Neutral, because it may or may not integrate with version control Bad, because content may not be version-controlled alongside code Bad, because it creates a separate system to maintain Bad, because it’s harder to review changes through standard PR process Bad, because portability and long-term accessibility may be concerns No formal ADR process Continue without a structured approach to documenting architectural decisions.\nGood, because it requires no additional overhead Bad, because context and rationale for decisions are lost over time Bad, because new team members struggle to understand why decisions were made Bad, because it leads to repeated discussions of previously settled questions Bad, because it makes it difficult to track when decisions should be revisited More Information MADR 4.0.0 specification: https://adr.github.io/madr/ ADRs will be categorized as: strategic, user-journey, or api-design ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX All ADRs are stored in docs/content/r\u0026d/adrs/ directory ","categories":"","description":"Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.\n","excerpt":"Adopt Markdown Architectural Decision Records (MADR) as the standard …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/0001-use-madr-for-architecture-decision-records/","tags":"","title":"[0001] Use MADR for Architecture Decision Records"},{"body":"User Journeys This section contains detailed user journey documentation that defines how users interact with the Battlebots platform. Each journey document includes:\nUser personas and their goals Step-by-step flow diagrams Technical requirements (access control, analytics, etc.) Success metrics These documents serve as the foundation for feature development and help ensure a consistent, user-centered experience.\n","categories":"","description":"Documentation of user flows and experiences for the Battlebots platform\n","excerpt":"Documentation of user flows and experiences for the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/","tags":"","title":"User Journeys"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/","tags":"","title":"Analysis"},{"body":"Architecture Decision Records (ADRs) This section contains architectural decision records that document the key design choices made for the Battlebots platform. Each ADR follows the MADR 4.0.0 format and includes:\nContext and problem statement Decision drivers and constraints Considered options with pros and cons Decision outcome and rationale Consequences (positive and negative) Confirmation methods ADR Categories ADRs are classified into three categories:\nStrategic - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices. User Journey - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features. API Design - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation. Status Values Each ADR has a status that reflects its current state:\nproposed - Decision is under consideration accepted - Decision has been approved and should be implemented rejected - Decision was considered but not approved deprecated - Decision is no longer relevant or has been superseded superseded by ADR-XXXX - Decision has been replaced by a newer ADR These records provide historical context for architectural decisions and help ensure consistency across the platform.\n","categories":"","description":"Documentation of architectural decisions made in the Battlebots platform using MADR 4.0.0 standard\n","excerpt":"Documentation of architectural decisions made in the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/","tags":"","title":"Architecture Decision Records"},{"body":"Welcome! Battle Bots is a game in which you, the human, implement an autonomous “bot” to do battle with “bots” implemented by other humans.\nWhat is a Bot? A bot is a independent piece of software which is programmed to battle other bots by reacting to state updates (e.g. bot B moved to point A) and performing its own actions (e.g. fire missile at point A).\n","categories":"","description":"Battle Bots is a PVP game for autonomous players","excerpt":"Battle Bots is a PVP game for autonomous players","ref":"/battlebots/pr-preview/pr-150/","tags":"","title":"Battle Bots"},{"body":"R\u0026D Process The Research \u0026 Design process follows a structured workflow to ensure comprehensive analysis and documentation of user experiences, technical solutions, and implementation details.\nProcess Steps Document the User Journey\nCreate a user journey document for the specific user experience Include flow diagrams using Mermaid to visualize user interactions Define prioritized technical requirements (P0/P1/P2) Use the /new-user-journey command to create standardized documentation Design the Solution\nCreate an ADR that designs a solution to implement the user journey Identify and document: Additional ADRs needed for specific components APIs that need to be defined User interface flows (mobile, web, etc.) Data flow from user to end systems (database, notification system, etc.) Capture the complete system architecture and integration points Document Component ADRs\nCreate ADRs for specific technical components identified in the solution design Examples: authentication strategy, session management, account linking, data storage Use the /new-adr command to create standardized MADR 4.0.0 format documents Document technical decisions with context, considered options, and consequences Document Required APIs\nFor each API endpoint identified in the solution, create comprehensive API documentation Use the /new-api-doc command to create standardized documentation Include: Request/response schemas Authentication requirements Business logic flows (Mermaid diagrams) Error responses and status codes Example curl requests Document API Implementation\nFor each documented API, create an ADR describing the implementation approach Document technical decisions including: Programming language selection Framework and libraries Architecture patterns Testing strategy Example: ADR-0006 documents the tech stack for API development (z5labs/humus framework) Design User Interface\nCreate UI/UX designs for the user journey Ensure designs align with the documented user flows and API contracts Consider platform-specific requirements (mobile, web, desktop) Documentation Structure The R\u0026D documentation is organized into the following sections:\nUser Journeys - User experience flows with technical requirements ADRs - Architectural Decision Records documenting technical decisions APIs - REST API endpoint documentation with schemas and examples Analysis - Research and analysis of technologies and solutions ","categories":"","description":"","excerpt":"R\u0026D Process The Research \u0026 Design process follows a structured …","ref":"/battlebots/pr-preview/pr-150/research_and_development/","tags":"","title":"Research \u0026 Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/categories/","tags":"","title":"Categories"},{"body":"Overview This section contains research and analysis of observability solutions for the BattleBots platform. Observability is critical for:\nMonitoring real-time battle events and game state Tracking bot performance and system health Debugging issues in distributed game architecture Analyzing player behavior and system usage patterns Ensuring reliable service operation Components OpenTelemetry Collector Analysis of the OpenTelemetry Collector, a vendor-agnostic telemetry data pipeline that can receive, process, and export logs, metrics, and traces to multiple backends.\nThe OpenTelemetry Collector serves as a centralized telemetry hub, providing:\nVendor neutrality for any observability backend Protocol translation between Prometheus, Jaeger, Zipkin, and OTLP Unified collection pipeline for logs, metrics, and traces Flexible deployment in agent, gateway, or hybrid modes Signal correlation linking traces, metrics, and logs Includes detailed analysis of:\nArchitecture and core components Logs, metrics, and traces support Self-monitoring and operational considerations BattleBots platform integration patterns Future ADR Dependencies This analysis will inform:\nADR-NNNN: Observability Stack Selection - Which backends to use (Loki, Prometheus, Jaeger, etc.) ADR-NNNN: Telemetry Collection Strategy - Agent vs. gateway deployment, sampling policies ADR-NNNN: Telemetry Data Retention - Storage duration and cost management Related Documentation R\u0026D Documentation User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of observability solutions for the BattleBots platform.\n","excerpt":"Research and analysis of observability solutions for the BattleBots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/","tags":"","title":"Observability Analysis"},{"body":"Overview The OpenTelemetry Collector serves as a centralized telemetry hub, removing the need to run multiple agents or collectors for different formats and backends. It provides:\nVendor neutrality: Works with any observability backend Protocol translation: Converts between Prometheus, Jaeger, Zipkin, and OTLP formats Unified collection: Single pipeline for logs, metrics, and traces Flexible deployment: Agent mode, gateway mode, or hybrid Signal correlation: Links traces, metrics, and logs through shared context Document Structure The analysis is organized into the following documents:\nOpenTelemetry Collector Overview High-level architectural overview covering:\nCore components (receivers, processors, exporters, extensions) Pipeline-based architecture and data flow Deployment patterns (agent, gateway, hybrid) Configuration fundamentals When to use the Collector vs. direct exports Audience: Everyone—provides foundational understanding for all subsequent documents.\nLogs Support Deep dive into log data handling:\nOTLP logs data model and structure Log receivers (filelog, syslog, OTLP) Log processors (attributes, filter, transform) Log exporters (Loki, Elasticsearch, OTLP) Log correlation with traces and metrics Configuration patterns for log collection Audience: Developers implementing log collection, operations teams configuring log pipelines.\nMetrics Support Deep dive into metrics data handling:\nOpenTelemetry metrics data model Metric types (counters, gauges, histograms, summaries) Temporality (delta vs. cumulative) Metrics receivers (Prometheus, hostmetrics, OTLP) Metrics processors and exporters Performance and cardinality considerations Audience: Developers instrumenting applications, SREs monitoring infrastructure.\nTraces Support Deep dive into distributed tracing:\nTrace and span data model Context propagation mechanisms Trace receivers (OTLP, Jaeger, Zipkin) Sampling strategies (head vs. tail sampling) Trace processors and exporters Multi-backend routing Audience: Developers implementing distributed tracing, architects designing observability strategy.\nSelf-Monitoring How to observe the Collector itself:\nInternal metrics and telemetry Extensions (health_check, zpages, pprof) Debugging and troubleshooting techniques Performance monitoring and optimization Production monitoring best practices Audience: Operations teams, SREs responsible for Collector reliability.\nBattleBots Platform Context For the BattleBots platform, the OpenTelemetry Collector would support:\nGame Event Observability Logs: Battle events, bot actions, game state transitions, error conditions Metrics: Match duration, action rates, player counts, system resource usage Traces: Request flows from player action to state update to broadcast Infrastructure Monitoring Host metrics: Server CPU, memory, disk, network utilization Application metrics: Go runtime metrics, HTTP latency, WebSocket connections Container metrics: Resource limits, restart counts, health status Cross-Signal Correlation The Collector enables powerful debugging workflows:\nAlert fires on high error rate (metrics) Drill down to traces showing failing requests View logs associated with failing trace spans Identify root cause with full context This unified observability is particularly valuable during live battles when quick diagnosis is essential.\nImplementation Considerations Deployment Architecture For BattleBots, a recommended deployment would include:\nAgent Mode:\nCollectors running alongside each game server Local log file collection with filelog receiver Host metrics collection for server monitoring OTLP receiver for application telemetry Gateway Mode:\nCentralized collectors receiving data from agents Tail sampling for intelligent trace retention Multi-backend routing (analytics, debugging, long-term storage) Buffering and retry for backend resilience Signal-Specific Patterns Logs:\nCollect structured JSON logs from game servers Parse and enrich with resource attributes Filter debug logs in production Route to Loki or Elasticsearch Metrics:\nScrape Prometheus metrics from Go services Collect host metrics from servers Aggregate and downsample for cost efficiency Export to Prometheus or cloud backends Traces:\nInstrument Go services with OpenTelemetry SDK Use head sampling for baseline reduction (10%) Apply tail sampling to always capture errors Export to Jaeger or Grafana Tempo External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of the OpenTelemetry Collector for logs, metrics, and traces collection and processing.\n","excerpt":"Research and analysis of the OpenTelemetry Collector for logs, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/","tags":"","title":"OpenTelemetry Collector"},{"body":"Overview The OpenTelemetry Collector is a vendor-agnostic application that receives, processes, and exports telemetry data (traces, metrics, and logs). It serves as a centralized component in observability architectures, removing the need to run multiple agents or collectors for different telemetry formats and backends.\nThe Collector supports open-source observability data formats including Jaeger, Prometheus, Fluent Bit, and others, while providing a unified approach to telemetry handling. It enables services to offload telemetry data quickly while the Collector handles retries, batching, encryption, and sensitive data filtering.\nKey benefits include vendor independence, reduced operational complexity, and the ability to route telemetry data to multiple backends simultaneously without modifying application code.\nKey Concepts The Collector is built around five guiding principles:\nUsability: Provides functional defaults with support for popular protocols out-of-the-box Performance: Maintains stability under varying loads with predictable resource usage Observability: Designed as an observable service itself, exposing its own metrics and health status Extensibility: Allows customization through plugins without requiring modifications to core code Unification: Single codebase supporting all three telemetry signals (traces, metrics, logs) Core Architecture The Collector uses a pipeline-based architecture where data flows through three primary component types, orchestrated by a configuration file.\ngraph LR A[Telemetry Sources] --\u003e B[Receivers] B --\u003e C[Processors] C --\u003e D[Exporters] D --\u003e E[Observability Backends] F[Extensions] -.Optional.-\u003e B F -.Optional.-\u003e C F -.Optional.-\u003e D style A fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style C fill:#f3e5f5 style D fill:#e8f5e9 style F fill:#fce4ec Data Flow Telemetry data flows unidirectionally through the Collector:\nReceivers accept incoming telemetry in various formats (OTLP, Jaeger, Prometheus) Processors transform, filter, or enrich the data in a sequential chain Exporters send processed data to one or more backend destinations This pipeline architecture allows the same data to be simultaneously:\nSampled differently for different backends Enriched with environment-specific attributes Routed to multiple observability platforms Components Receivers Receivers gather telemetry data through two mechanisms:\nPush-based: Listen on network endpoints for incoming data Pull-based: Actively scrape metrics from instrumented services Multiple receivers can feed into a single pipeline, with their outputs merged before reaching processors. Common receivers include:\notlp - OpenTelemetry Protocol (gRPC/HTTP) prometheus - Prometheus metrics scraping jaeger - Jaeger trace data filelog - Log file collection For comprehensive receiver documentation, see the OpenTelemetry Collector Receivers.\nProcessors Processors form a sequential chain that transforms data between receivers and exporters. Each processor in the chain operates on the data before passing it to the next processor.\nCommon operations include:\nAdding or removing attributes Sampling (probabilistic, tail-based) Batching for efficient transmission Filtering unwanted telemetry Resource detection (cloud provider, Kubernetes metadata) Important: Each pipeline maintains independent processor instances, even when referencing the same configuration name across multiple pipelines. This prevents unintended state sharing between pipelines.\nFor comprehensive processor documentation, see the OpenTelemetry Collector Processors.\nExporters Exporters forward processed data to external destinations, typically by sending to network endpoints or writing to logging systems. Multiple exporters can receive identical data copies through a fan-out mechanism, enabling simultaneous transmission to different backends.\nCommon exporters include:\notlp - OpenTelemetry Protocol destinations prometheus - Prometheus remote write jaeger - Jaeger backend logging - Standard output for debugging file - Local file storage For comprehensive exporter documentation, see the OpenTelemetry Collector Exporters.\nConnectors Connectors enable inter-pipeline communication and data flow between different telemetry signal types. A connector acts as both an exporter (for one pipeline) and a receiver (for another pipeline).\nUse cases include:\nGenerating metrics from trace spans Creating logs from metric anomalies Correlating signals for enhanced observability For more information, see the OpenTelemetry Collector Connectors.\nExtensions Extensions provide auxiliary functionality that doesn’t directly process telemetry data but supports Collector operations:\nhealth_check - HTTP endpoint for health monitoring pprof - Go profiling endpoint for performance analysis zpages - In-process debugging pages ballast - Memory ballast for GC optimization Extensions are optional but highly recommended for production deployments. See self-monitoring documentation for more details.\nConfiguration The Collector uses YAML configuration files (default: /etc/otelcol/config.yaml) with four main sections:\nreceivers: # Define how to collect telemetry processors: # Define how to transform telemetry exporters: # Define where to send telemetry service: pipelines: # Connect receivers → processors → exporters Pipeline Types Three pipeline types handle different telemetry signals:\ntraces: Distributed tracing data metrics: Time-series measurements logs: Event records Each pipeline independently connects receivers to exporters through an optional processor chain. The same receiver can feed multiple pipelines simultaneously, though this creates a potential bottleneck if one processor blocks.\nConfiguration Best Practices Validate configurations before deployment: otelcol validate --config=file.yaml Use environment variables for sensitive values: ${env:API_KEY} Bind endpoints to localhost for local-only access Apply TLS certificates for production environments Use the type/name syntax for multiple instances of the same component type For detailed configuration guidance, see the Configuration Documentation.\nDeployment Patterns The Collector supports multiple deployment models based on operational requirements and scale.\nAgent Mode In agent mode, lightweight Collector instances run as daemons alongside applications (on the same host, container, or Kubernetes pod). This pattern:\nCollects telemetry from co-located services Performs local sampling and aggregation Reduces network overhead by local processing Simplifies application configuration (default OTLP exporters assume localhost:4317) Use when: You need local collection with per-host resource limits and want to offload telemetry handling from application processes.\nGateway Mode In gateway mode, centralized Collector instances receive data from distributed agents and application libraries, then route to backend systems. This pattern:\nProvides centralized control over data transformation Enables consistent sampling decisions across services Supports complex routing logic to multiple backends Allows for sensitive data filtering before egress Use when: You need centralized policy enforcement, multi-backend routing, or want to isolate backend credentials from application environments.\nHybrid Mode Many production deployments use both agent and gateway patterns:\nAgents perform local collection and basic processing Gateways handle aggregation, complex transformations, and backend routing This hybrid approach balances local efficiency with centralized control.\nNo Collector For development environments or initial experimentation, services can export directly to backends using OTLP. However, this approach loses the benefits of buffering, retries, and transformation capabilities.\nWhen to Use the Collector Recommended Use Cases Deploy a Collector when you need to:\nSupport multiple telemetry formats (Jaeger, Prometheus, custom formats) Route telemetry to multiple backends simultaneously Perform data transformation or enrichment before export Sample or filter telemetry based on configurable rules Isolate backend credentials from application deployments Buffer telemetry during backend outages Offload retry and batching logic from applications Direct Export Scenarios Direct service-to-backend export may be sufficient for:\nDevelopment and testing environments Single-backend deployments with no transformation needs Prototyping and proof-of-concept work Very small-scale deployments Integration Points BattleBots Observability Requirements For the BattleBots platform, the Collector would support:\nBattle Event Tracking: Collecting logs of bot actions and game state changes Performance Metrics: Gathering metrics on bot response times and system resource usage Distributed Tracing: Tracking request flows across client/server or P2P architectures See User Journey 0001: POC for observability requirements context.\nMulti-Signal Correlation The Collector enables correlation between:\nTrace spans and related logs (via trace context) Metrics and traces (via exemplars) Resource attributes across all signals This correlation is particularly valuable for debugging battle scenarios where you need to understand both the timeline of events (traces), the quantitative measurements (metrics), and the detailed context (logs).\nFurther Reading Official Documentation OpenTelemetry Collector Main Documentation Collector Architecture Configuration Reference Component Registry Specifications OTLP Specification OpenTelemetry Specification Implementation Resources Collector GitHub Repository Collector Contrib Repository (additional components) Related Analysis Documents Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"High-level architectural overview of the OpenTelemetry Collector, covering core components, deployment patterns, and use cases.\n","excerpt":"High-level architectural overview of the OpenTelemetry Collector, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/opentelemetry-collector-overview/","tags":"","title":"OpenTelemetry Collector Overview"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting log data from various sources to multiple backends. Unlike traditional logging agents that focus on specific formats or destinations, the Collector treats logs as first-class observability signals alongside metrics and traces.\nThe Collector’s log support enables correlation between logs and traces through shared execution context (TraceId and SpanId), allowing unified observability across all three signal types. This correlation is particularly valuable for debugging complex distributed systems where understanding both the quantitative measurements and the detailed event context is essential.\nKey capabilities include parsing structured and unstructured log formats, enriching logs with resource attributes, filtering and transforming log content, and routing logs to multiple backends simultaneously.\nKey Concepts OTLP Logs Data Model OpenTelemetry defines a standardized log data model to establish a common understanding of what a LogRecord is and what data needs to be recorded, transferred, stored, and interpreted by logging systems.\nLogRecord Structure:\nA LogRecord contains several key components:\nTimestamp: The moment in time when the event occurred TraceId and SpanId: Execution context identifiers enabling correlation between logs and traces Resource: Describes the origin of the log (host name, container name, pod name, etc.) Instrumentation Scope: Identifies the library or component that generated the log Severity: Indicates the importance or criticality of the log entry Body: The actual log message content (string, structured data, or binary) Attributes: Structured key-value pairs providing additional context This standardized model allows logs from different sources to be processed uniformly while preserving correlation capabilities.\nLog Correlation The logs data model enables correlation across three dimensions:\nTemporal correlation: Based on timestamp alignment Execution context correlation: Using TraceId and SpanId to link logs to specific trace spans Origin correlation: Through Resource context describing the source infrastructure and application This unified approach allows observability backends to perform exact and unambiguous correlation between logs, metrics, and traces.\nLog Receivers Receivers collect log data from various sources and convert it into the OpenTelemetry logs data model.\nFilelog Receiver The filelog receiver tails and parses logs from files, making it ideal for collecting logs from applications that write to local disk.\nKey Capabilities:\nFile monitoring: Tracks multiple log files using glob patterns for inclusion and exclusion Automatic rotation handling: Detects and follows rotated log files and symlinks Compression support: Reads gzip-compressed files with auto-detection Multiline support: Combines log entries spanning multiple lines using custom patterns Format parsing: Built-in parsers for JSON, regex patterns, and structured text Metadata extraction: Parses timestamps, severity levels, and custom fields Persistent offsets: Maintains file positions across collector restarts Example use cases:\nCollecting application logs written to /var/log/ Parsing container logs from Kubernetes Reading structured JSON logs from microservices Configuration note: The filelog receiver uses a pipeline of operators that transform raw file content into structured LogRecords. Each operator performs a specific transformation (parsing, extraction, modification) before passing data to the next operator.\nOTLP Receiver The OTLP receiver accepts log data transmitted using the OpenTelemetry Protocol.\nSupported transports:\ngRPC (default port 4317): Uses unary requests with ExportLogsServiceRequest messages HTTP (default port 4318): POST requests to /v1/logs endpoint Encoding formats:\nBinary Protobuf (application/x-protobuf) JSON Protobuf (proto3 JSON mapping) Use when: Collecting logs directly from applications instrumented with OpenTelemetry SDKs or from upstream OpenTelemetry Collectors in a multi-tier deployment.\nSyslog Receiver The syslog receiver listens for syslog messages over TCP or UDP, supporting RFC 3164 and RFC 5424 formats.\nUse cases:\nCollecting system logs from Linux/Unix hosts Receiving logs from network devices (routers, switches, firewalls) Integrating with legacy applications that use syslog Other Log Receivers The OpenTelemetry Collector ecosystem includes receivers for:\njournald: Reads logs from systemd journal tcplog/udplog: Generic TCP/UDP log receivers windowseventlog: Collects Windows Event Logs kafka: Consumes logs from Kafka topics For a comprehensive list, see the Receiver Components documentation.\nLog Processors Processors transform, filter, and enrich log data as it flows through the pipeline.\nAttributes Processor The attributes processor modifies attributes of log records.\nCapabilities:\nInsert new attributes Update existing attributes Delete attributes Hash attribute values for privacy Extract values from one attribute to another Common use cases:\nAdding environment labels (e.g., environment=production) Removing sensitive data from log attributes Normalizing attribute names across different sources For more details, see the Mastering the OpenTelemetry Attributes Processor guide.\nFilter Processor The filter processor drops log records that match specified conditions using the OpenTelemetry Transformation Language (OTTL).\nCapabilities:\nFilter by log severity level Drop logs matching specific patterns Exclude logs from certain resources Reduce log volume by dropping debug logs in production Example scenarios:\nDropping health check logs to reduce noise Filtering out logs below a certain severity threshold Excluding logs from specific namespaces or services See also the Filter Processor for OpenTelemetry Collector documentation.\nTransform Processor The transform processor modifies log records using OTTL statements.\nCapabilities:\nParse log body content into structured attributes Modify log severity based on content Extract values using regex or JSON path Compute new attributes from existing ones Normalize timestamps Use cases:\nConverting unstructured log messages to structured attributes Extracting user IDs or request IDs from log text Standardizing log formats from multiple sources For transformation guidance, see Transforming telemetry.\nResource Detection Processor The resource detection processor enriches logs with metadata about their execution environment:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information (Docker, containerd) Host information (hostname, OS, architecture) This automatic enrichment enables filtering and grouping logs by infrastructure context without manual configuration.\nBatch Processor The batch processor groups log records before sending to exporters, improving throughput and reducing network overhead.\nConfiguration considerations:\nTimeout: Maximum time to wait before sending a batch Batch size: Number of log records per batch Memory limits: Prevents excessive memory usage Batching is recommended for production deployments to optimize resource usage.\nLog Exporters Exporters send processed log data to observability backends and storage systems.\nOTLP HTTP Exporter The OTLP HTTP exporter is the recommended exporter for modern observability backends that support native OTLP ingestion.\nSupported destinations:\nGrafana Loki (v3+): Send logs to Loki’s native OTLP endpoint at http://loki:3100/otlp Elasticsearch: Use OTLP-compatible endpoints Commercial backends: Datadog, New Relic, Honeycomb, Dynatrace Important: The Loki-specific exporter is deprecated. Use the standard otlphttp/logs exporter for Loki v3+ which supports native OTLP ingestion.\nFor setup guidance, see Getting started with the OpenTelemetry Collector and Loki tutorial.\nElasticsearch Exporter The Elasticsearch exporter sends logs, metrics, traces, and profiles directly to Elasticsearch.\nSupported versions:\nElasticsearch 7.17.x Elasticsearch 8.x Elasticsearch 9.x Features:\nIndex routing based on log attributes Dynamic index naming with time-based patterns Bulk API for efficient ingestion File Exporter The file exporter writes logs to local files, useful for:\nDebugging collector pipelines Creating log archives Forwarding to systems that process files Note: Not recommended for production logging backends—use OTLP or dedicated exporters instead.\nLogging Exporter The logging (debug) exporter writes logs to the collector’s standard output. Use this for:\nDevelopment and testing Troubleshooting pipeline configuration Verifying data transformation For additional exporters, see the Exporter Components documentation.\nLog Pipeline Flow A typical log pipeline in the Collector follows this pattern:\ngraph LR A[Log Files] --\u003e B[Filelog Receiver] C[OTLP SDK] --\u003e D[OTLP Receiver] B --\u003e E[Resource Detection] D --\u003e E E --\u003e F[Transform Processor] F --\u003e G[Filter Processor] G --\u003e H[Attributes Processor] H --\u003e I[Batch Processor] I --\u003e J[OTLP HTTP Exporter] J --\u003e K[Loki] I --\u003e L[Elasticsearch Exporter] L --\u003e M[Elasticsearch] style A fill:#e1f5ff style C fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#e8f5e9 style L fill:#e8f5e9 style K fill:#e1f5ff style M fill:#e1f5ff Configuration Considerations Multiline Log Handling Many applications emit logs spanning multiple lines (e.g., stack traces, JSON objects). The filelog receiver supports multiline patterns to combine these entries:\nreceivers: filelog: include: [/var/log/app/*.log] multiline: line_start_pattern: '^\\d{4}-\\d{2}-\\d{2}' Parsing Structured Logs For JSON-formatted logs, configure the filelog receiver with JSON parsing:\nreceivers: filelog: include: [/var/log/app/*.log] operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' Log Volume Management High log volumes can overwhelm collectors and backends. Strategies include:\nSampling: Use the probabilistic sampler processor to keep a percentage of logs Filtering: Drop debug/trace logs in production using the filter processor Batching: Configure appropriate batch sizes to balance latency and throughput Tail sampling: Keep only logs associated with interesting traces Performance Tuning For high-throughput log collection:\nIncrease the number of concurrent file readers in filelog receiver Tune batch processor settings (size, timeout) Use multiple collector instances with load balancing Consider gateway deployment to centralize processing Integration Points BattleBots Log Collection For the BattleBots platform, log collection would capture:\nGame events: Bot actions, state transitions, match outcomes System logs: Server startup, configuration changes, errors Client logs: User actions, connection events, performance issues The filelog receiver can parse structured JSON logs from the game server while the OTLP receiver collects logs directly from instrumented Go services.\nLog-Trace Correlation When both logs and traces are collected, correlation enables:\nFinding all logs for a specific trace (query by TraceId) Jumping from a log entry to its parent trace span Identifying logs that occurred during slow requests This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically.\nCross-Signal Analysis Logs complement metrics and traces:\nMetrics show aggregate patterns (error rate spike) Traces show request flow (which service failed) Logs show detailed context (exception message, variable values) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Logs Specification OTLP Specification Receiver Components Processor Components Exporter Components Transforming Telemetry Component-Specific Resources Filelog Receiver Attributes Processor Guide Filter Processor Guide Transform Processor Elasticsearch Exporter Integration Guides Ingesting logs to Loki using OpenTelemetry Collector Getting started with the OpenTelemetry Collector and Loki tutorial Honeycomb Filter Processor Documentation Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles log data, including receivers, processors, exporters, and the OTLP logs data model.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles log data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-logs/","tags":"","title":"OpenTelemetry Collector: Logs Support"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting metrics data from diverse sources to multiple backends. It serves as a bridge between different metrics ecosystems, enabling seamless integration of Prometheus metrics, host system metrics, and custom application metrics within a unified observability platform.\nThe Collector’s metrics support emphasizes signal correlation—connecting metrics to traces through exemplars and enriching attributes via Baggage and Context. This enables powerful observability patterns such as jumping from a metric anomaly to related traces or finding metrics that explain slow trace spans.\nKey capabilities include scraping Prometheus endpoints, collecting host system metrics, transforming metric formats, aggregating data points, and routing metrics to multiple backends simultaneously while handling different temporality preferences.\nKey Concepts OpenTelemetry Metrics Data Model The OpenTelemetry Metrics data model defines how metrics are represented and processed. The data model serves to:\nCapture raw measurements efficiently and simultaneously Decouple instrumentation from the SDK implementation Enable correlation with traces and logs Support migration from OpenCensus and Prometheus Architecture layers:\nMeterProvider \u0026 Instruments: Applications collect measurements through Meters and their associated Instruments In-Memory Aggregation: Measurements aggregate into an intermediate representation MetricReader: Processes aggregated metrics for export to backends Metric Types OpenTelemetry supports four primary metric types, each suited for different measurement scenarios.\nCounter (Sum) Counters represent cumulative or delta measurements that can only increase over time (or be reset to zero). Common examples include:\nRequest count Error count Bytes transmitted Items processed Characteristics:\nMonotonically increasing Supports both delta and cumulative temporality Can be aggregated across instances Typically visualized as rate-of-change For detailed specifications, see OTLP Metrics Types.\nGauge Gauges represent sampled values that can arbitrarily increase or decrease over time. Unlike counters, gauges are not cumulative—they reflect the current value at the time of measurement.\nCommon examples include:\nCPU usage percentage Memory utilization Queue depth Active connection count Temperature readings Characteristics:\nNon-monotonic (can increase or decrease) No aggregation temporality (uses “last sample value”) Represents point-in-time state Cannot be meaningfully aggregated across instances without additional context Histogram Histograms convey a population of recorded measurements in a compressed format by grouping measurements into configurable buckets. This enables statistical analysis without storing individual data points.\nCommon examples include:\nRequest latency distribution Response size distribution Query execution time Message size distribution Characteristics:\nProvides count, sum, and bucket distributions Supports both delta and cumulative temporality Enables percentile calculations (p50, p95, p99) More efficient than storing individual measurements Histograms are particularly valuable for understanding the distribution of latency or size measurements, revealing whether most requests are fast with occasional slow outliers, or if performance degrades uniformly.\nSummary Summaries provide pre-calculated quantile values (percentiles) over a time window. Unlike histograms, which send bucket distributions for backend calculation, summaries compute quantiles client-side.\nImportant: Summary points cannot always be merged meaningfully. This point type is not recommended for new applications and exists primarily for compatibility with other formats like Prometheus summaries.\nFor comprehensive metric type details, see OpenTelemetry Metrics.\nTemporality Temporality defines how metric values are accumulated and reported over time. OpenTelemetry supports two aggregation temporality modes:\nDelta Temporality Delta temporality reports the change since the last collection period. Each data point represents only new measurements since the previous export.\nCharacteristics:\nNon-overlapping time windows Measures rate of change Preferred by some backends (StatsD, Carbon) Requires stateless aggregation Example: A request counter shows +100 requests in period 1, then +150 requests in period 2.\nCumulative Temporality Cumulative temporality reports the total value since process start (or a fixed start point). Each data point includes all measurements from the beginning.\nCharacteristics:\nOverlapping time windows from fixed start Accumulates over application lifetime Preferred by Prometheus Resilient to collection gaps Example: A request counter shows 100 total requests in period 1, then 250 total requests in period 2.\nImportant note: Set the temporality preference to DELTA when possible, as setting it to CUMULATIVE may discard some data points during application or collector startup. However, Prometheus backends require cumulative temporality.\nFor more details, see OpenTelemetry Metrics Aggregation.\nMetrics Receivers Receivers collect metrics data from various sources and convert it into the OpenTelemetry metrics data model.\nPrometheus Receiver The Prometheus receiver enables the OpenTelemetry Collector to act as a Prometheus server by scraping Prometheus-compatible endpoints, then converting the metrics into OTLP format.\nKey capabilities:\nScrapes any Prometheus /metrics endpoint Supports service discovery mechanisms Converts Prometheus metrics to OpenTelemetry format Handles metric relabeling and filtering Scales with Target Allocator for large deployments Configuration example:\nreceivers: prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 15s static_configs: - targets: ['localhost:8888'] Scaling with Target Allocator:\nThe Target Allocator decouples service discovery and metric collection, allowing independent scaling. Each Collector pod registers with the Target Allocator, which uses consistent hashing to distribute discovered targets evenly among active Collectors, ensuring each target is scraped exactly once without overlap.\nFor comprehensive guidance, see Prometheus and OpenTelemetry Collector Integration.\nHost Metrics Receiver The hostmetrics receiver collects comprehensive system-level metrics from host machines, providing visibility into infrastructure health.\nAvailable scrapers:\ncpu: CPU utilization, time, and frequency disk: Disk I/O operations and throughput filesystem: Filesystem usage and available space memory: Memory utilization and swap usage network: Network interface statistics and errors load: System load averages paging: Paging and swapping activity processes: Process count and resource usage Configuration example:\nreceivers: hostmetrics: collection_interval: 30s scrapers: cpu: disk: filesystem: memory: network: The hostmetrics receiver is essential for infrastructure monitoring and provides context for application-level metrics. When deployed in Kubernetes, appropriate volumes and volumeMounts are automatically configured when the hostMetrics preset is enabled.\nOTLP Receiver The OTLP receiver accepts metrics data transmitted using the OpenTelemetry Protocol from instrumented applications or upstream collectors.\nSupported transports:\ngRPC (default port 4317) HTTP (default port 4318, endpoint /v1/metrics) Use cases:\nCollecting metrics directly from OpenTelemetry SDKs Multi-tier collector deployments (agent → gateway) Receiving metrics from serverless functions Other Metrics Receivers The ecosystem includes receivers for diverse metrics sources:\nstatsd: Receives StatsD protocol metrics kafka: Consumes metrics from Kafka topics influxdb: Receives InfluxDB line protocol carbon: Receives Graphite carbon metrics collectd: Receives collectd metrics postgresql: Scrapes PostgreSQL metrics redis: Scrapes Redis metrics mongodb: Scrapes MongoDB metrics For a complete list, see the Receiver Components documentation.\nMetrics Processors Processors transform and enrich metrics data as it flows through pipelines.\nMetrics Transform Processor The metrics transform processor modifies metric names, types, and attributes using transformation rules.\nCommon operations:\nRename metrics for consistency Change metric types (e.g., gauge to counter) Add or modify resource attributes Aggregate metrics across dimensions Filter Processor The filter processor drops metrics matching specified conditions, reducing data volume and costs.\nUse cases:\nDropping debug metrics in production Filtering metrics from test environments Excluding high-cardinality metrics Removing specific metric names or attribute values Cumulative to Delta Processor This processor converts cumulative temporality metrics to delta temporality, useful when backends prefer delta metrics.\nAttributes Processor Adds, updates, or deletes metric attributes and resource attributes, enabling:\nEnvironment labeling (environment=production) Team ownership tags (team=platform) Cost allocation labels Normalization across different metric sources Batch Processor Groups metrics before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nConfiguration considerations:\nBatch size: Number of metric data points per batch Timeout: Maximum wait before sending partial batch Memory limits: Prevents unbounded memory growth Metrics Exporters Exporters send processed metrics to observability backends and time-series databases.\nOTLP Exporter The OTLP exporter sends metrics using the OpenTelemetry Protocol to OTLP-compatible backends.\nSupported destinations:\nOpenTelemetry-native backends Cloud vendor endpoints (AWS CloudWatch, Google Cloud Monitoring, Azure Monitor) Commercial observability platforms (Datadog, New Relic, Honeycomb) Important: OTLP is now the recommended protocol for sending metrics to modern backends. For example, Prometheus can now accept OTLP directly:\nexport OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://localhost:9090/api/v1/otlp/v1/metrics Prometheus Remote Write Exporter The Prometheus Remote Write exporter sends OpenTelemetry metrics to Prometheus remote write compatible backends such as:\nCortex Grafana Mimir Thanos Amazon Managed Service for Prometheus Google Cloud Managed Prometheus Capabilities:\nTLS support (required by default) Queued retry mechanisms Authentication options (basic auth, bearer token, OAuth2) Important limitation: Non-cumulative monotonic, histogram summary, and exponential histogram OTLP metrics are dropped by this exporter.\nFor Grafana Mimir specifically, it’s recommended to use OTLP rather than Prometheus remote write.\nPrometheus Exporter The Prometheus exporter exposes metrics in Prometheus format on an HTTP endpoint for Prometheus servers to scrape.\nUse cases:\nExisting Prometheus deployments Push-based collection converted to pull-based Multi-backend export (push to one, expose for scraping by another) File Exporter Writes metrics to local files for debugging, archival, or processing by batch systems.\nFor additional exporters, see the Exporter Components documentation.\nMetrics Pipeline Flow A typical metrics pipeline demonstrates collection, processing, and export to multiple backends:\ngraph LR A[Prometheus Endpoints] --\u003e B[Prometheus Receiver] C[Host System] --\u003e D[Host Metrics Receiver] E[OTLP SDK] --\u003e F[OTLP Receiver] B --\u003e G[Attributes Processor] D --\u003e G F --\u003e G G --\u003e H[Filter Processor] H --\u003e I[Transform Processor] I --\u003e J[Batch Processor] J --\u003e K[OTLP Exporter] K --\u003e L[Prometheus Server] J --\u003e M[Prometheus Remote Write] M --\u003e N[Grafana Mimir] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#e8f5e9 style M fill:#e8f5e9 style L fill:#e1f5ff style N fill:#e1f5ff Configuration Considerations Temporality Management Different backends have different temporality preferences:\nPrometheus: Requires cumulative temporality StatsD-like systems: Prefer delta temporality Cloud vendors: Often accept both Configure the Collector to convert between temporalities based on backend requirements using the cumulative-to-delta processor.\nCardinality Control High cardinality metrics (many unique label combinations) can overwhelm backends and increase costs. Strategies include:\nFiltering high-cardinality dimensions Aggregating metrics before export Dropping rarely-used labels Using metric relabeling to reduce dimensions Scrape Interval Tuning Balance data freshness with resource consumption:\nShort intervals (5-15s): Real-time monitoring, higher costs Medium intervals (30-60s): Standard monitoring Long intervals (5m+): Capacity planning, cost optimization Exemplar Support Exemplars link metrics to traces by attaching trace IDs to specific metric data points. This enables:\nJumping from a high-latency histogram bucket to example slow traces Finding traces that contributed to an error rate spike Correlating metrics anomalies with detailed trace analysis Enable exemplar support in the Prometheus receiver and ensure trace context propagation in applications.\nIntegration Points BattleBots Metrics Collection For the BattleBots platform, metrics collection would capture:\nGame Metrics:\nMatch duration and outcome distribution Bot action rates (attacks, defenses, moves) Game state transition frequency Performance Metrics:\nRequest latency percentiles WebSocket connection counts Message throughput rates Server CPU and memory usage Business Metrics:\nActive user count Matches per hour Bot creation rate The combination of Prometheus receiver (for Go runtime metrics), hostmetrics receiver (for infrastructure), and OTLP receiver (for custom metrics) provides comprehensive visibility.\nMetrics-Trace Correlation Connecting metrics and traces enables powerful workflows:\nAlerting on metrics: High error rate triggers investigation Drill-down to traces: Click exemplar to see example failing requests Root cause analysis: Examine detailed trace spans to identify cause Fix validation: Monitor metrics to confirm fix effectiveness This requires:\nApplications emit both metrics and traces Exemplars enabled in metric collection Unified storage backend (or cross-backend linking) Cross-Signal Analysis Metrics complement logs and traces:\nMetrics identify anomalies at scale (response time spike) Traces show affected request flows (which service is slow) Logs provide detailed context (exception messages, stack traces) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Metrics Specification Metrics Data Model Receiver Components Processor Components Exporter Components Integration Guides Collecting Prometheus Metrics with the OpenTelemetry Collector Prometheus and OpenTelemetry Collector Integration OpenTelemetry Host Metrics receiver Using Prometheus as your OpenTelemetry backend Configure the OpenTelemetry Collector to write metrics into Mimir How to collect Prometheus metrics with the OpenTelemetry Collector and Grafana Component-Specific Resources Prometheus Remote Write Exporter OpenTelemetry Collector Chart (Kubernetes) Prometheus and OpenTelemetry - Better Together Analysis and Best Practices OTLP Metrics Types Understanding OpenTelemetry Metrics OpenTelemetry Metrics Aggregation How Prometheus Exporters Work With OpenTelemetry Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles metrics data, including the metrics data model, receivers, processors, exporters, and temporality concepts.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles metrics data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-metrics/","tags":"","title":"OpenTelemetry Collector: Metrics Support"},{"body":"Overview The OpenTelemetry Collector is designed as an observable service itself, following the principle that observability infrastructure must be observable. The Collector exposes its own telemetry (metrics, logs, and optionally traces) to enable monitoring health, diagnosing issues, and optimizing performance.\nSelf-monitoring is critical for production deployments—without visibility into the Collector’s operation, data loss or performance degradation can go undetected. The Collector provides built-in telemetry, diagnostic extensions, and debugging capabilities to ensure reliable operation at scale.\nKey capabilities include internal metrics exposed via Prometheus endpoints, health check endpoints for liveness/readiness probes, diagnostic extensions for real-time inspection, and structured logging for troubleshooting pipeline issues.\nKey Concepts Internal Telemetry The OpenTelemetry Collector generates internal telemetry by default to expose its operational state. This self-generated observability data helps operators monitor Collector health and performance.\nTelemetry types:\nMetrics: Quantitative measurements of Collector operation (default: Prometheus format on port 8888) Logs: Structured event records emitted to stderr by default Traces: Optional internal tracing of data flow through pipelines Internal telemetry enables:\nReal-time health monitoring Capacity planning and resource optimization Troubleshooting data loss or pipeline issues Performance profiling and bottleneck identification Observability vs. Debugging The Collector provides two complementary approaches:\nObservability (production):\nContinuous metrics collection Health check endpoints Structured logging External monitoring integration Debugging (development/troubleshooting):\nzPages for live data inspection pprof for performance profiling Debug exporters for pipeline validation Verbose logging modes Internal Metrics The Collector exposes comprehensive metrics about its operation through a Prometheus-compatible endpoint.\nMetrics Endpoint By default, the Collector exposes metrics at http://localhost:8888/metrics in Prometheus format. This endpoint can be scraped by Prometheus or any compatible metrics collector.\nConfiguration:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed # Options: none, basic, normal, detailed Key Metrics Categories Data Ingress Metrics Monitor data received by receivers:\notelcol_receiver_accepted_log_records - Log records accepted otelcol_receiver_accepted_spans - Spans accepted otelcol_receiver_accepted_metric_points - Metric points accepted otelcol_receiver_refused_* - Refused data (errors) These metrics help identify:\nData ingestion rates Receiver errors or rejections Source-specific throughput patterns Data Egress Metrics Monitor data sent by exporters:\notelcol_exporter_sent_log_records - Log records sent otelcol_exporter_sent_spans - Spans sent otelcol_exporter_sent_metric_points - Metric points sent otelcol_exporter_send_failed_* - Failed export attempts These metrics reveal:\nExport success rates Backend connectivity issues Data loss from failed exports Queue Metrics Monitor internal buffer state:\notelcol_exporter_queue_capacity - Queue capacity in batches otelcol_exporter_queue_size - Current queue utilization otelcol_exporter_enqueue_failed_* - Failed enqueues (buffer full) Alert on: Queue size approaching capacity indicates backpressure from slow exporters or high ingestion rates.\nProcessor Metrics Monitor processor operation:\notelcol_processor_batch_batch_send_size - Batch sizes sent otelcol_processor_batch_timeout_trigger_send - Timeouts triggering sends Processor-specific metrics (sampling rates, dropped data, etc.) Resource Metrics Monitor Collector resource usage:\nprocess_runtime_* - Go runtime metrics (memory, goroutines) process_cpu_seconds_total - CPU time consumed process_resident_memory_bytes - Memory usage For comprehensive metric details, see Internal telemetry and How to Monitor Open Telemetry Collector Performance.\nSelf-Monitoring Dashboards Several platforms provide pre-built dashboards for Collector monitoring:\nDynatrace OpenTelemetry Collector Self-Monitoring (June 2025 release) Grafana dashboards from the community Vendor-specific monitoring integrations Extensions for Observability Extensions provide auxiliary functionality that supports Collector operation and debugging.\nHealth Check Extension The health_check extension enables an HTTP endpoint that can be probed to check the Collector’s status.\nConfiguration:\nextensions: health_check: endpoint: 0.0.0.0:13133 path: /health/status check_collector_pipeline: enabled: false # Not recommended—use at own risk service: extensions: [health_check] Endpoints:\n/health/status - Returns 200 OK if the Collector is running Important note: The check_collector_pipeline feature is not working as expected and should not be used. Use metrics-based monitoring instead for pipeline health.\nUse cases:\nKubernetes liveness probes Load balancer health checks Container orchestration health monitoring Service mesh integration For more details, see Health Check Monitoring With OpenTelemetry.\nzPages Extension The zpages extension serves HTTP endpoints that provide live data for debugging different components without depending on external backends.\nConfiguration:\nextensions: zpages: endpoint: 0.0.0.0:55679 service: extensions: [zpages] Available pages:\n/debug/servicez - Service summary and version information /debug/pipelinez - Pipeline configuration and status /debug/extensionz - Loaded extensions /debug/tracez - Sample trace data (if internal tracing enabled) Use cases:\nInspecting live data flowing through pipelines Validating configuration changes Debugging data transformation issues In-process diagnostics during development zPages are particularly useful for answering questions like “Is the Collector receiving data?” and “What does the data look like after processing?”\nFor detailed usage, see Monitoring and Debugging the OpenTelemetry Collector.\npprof Extension The pprof extension enables the Go net/http/pprof endpoint for performance profiling.\nConfiguration:\nextensions: pprof: endpoint: 0.0.0.0:1777 service: extensions: [pprof] Available profiles:\n/debug/pprof/profile - CPU profile /debug/pprof/heap - Memory allocation profile /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/block - Blocking profile /debug/pprof/mutex - Mutex contention profile Use cases:\nInvestigating CPU hotspots Analyzing memory leaks Identifying goroutine leaks Profiling lock contention Collecting profiles:\n# CPU profile (30 seconds) curl http://localhost:1777/debug/pprof/profile?seconds=30 \u003e cpu.prof # Heap profile curl http://localhost:1777/debug/pprof/heap \u003e heap.prof # Analyze with pprof go tool pprof cpu.prof Security note: pprof endpoints should only be exposed internally, never to the public internet, as they can reveal sensitive information and consume resources.\nLogging and Debugging Structured Logging The Collector emits structured logs to stderr by default, which can be redirected to files or collected by log aggregation systems.\nLog levels:\ndebug - Verbose debugging information info - General operational messages (default) warn - Warning conditions error - Error conditions Configuration:\nservice: telemetry: logs: level: info encoding: json # Options: json, console Common log patterns:\nReceiver connection failures Exporter send failures Processor errors Configuration validation warnings Debug Exporter The debug (logging) exporter writes telemetry data to the Collector’s standard output, useful for confirming that data is being received, processed, and exported correctly.\nConfiguration:\nexporters: debug: verbosity: detailed # Options: basic, normal, detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, otlp] # Add debug alongside production exporters Use cases:\nValidating pipeline configuration Inspecting data transformations Troubleshooting receiver issues Confirming data format Warning: Debug exporters should be removed or disabled in production due to performance impact and log volume.\nFor troubleshooting guidance, see the Troubleshooting documentation.\nCommon Issues and Troubleshooting Data Loss Symptoms:\notelcol_exporter_send_failed_* metrics increasing Queue size approaching capacity Export errors in logs Common causes:\nExporter destination unavailable or slow Collector under-provisioned (insufficient CPU/memory) Network connectivity issues Backend rate limiting Solutions:\nIncrease queue size and retry parameters Scale Collector instances horizontally Add buffering through load balancers Implement backpressure handling High Memory Usage Symptoms:\nprocess_resident_memory_bytes continuously increasing OOM kills in container environments Slow garbage collection Common causes:\nLarge batch sizes Tail sampling buffer accumulation Queue size too large Memory leaks in processors Solutions:\nReduce batch size and timeout Tune tail sampling buffer limits Enable memory limiting in batch processor Update to latest Collector version (bug fixes) Use pprof to identify memory leaks Note: Memory usage increases in steps due to Go’s garbage collection characteristics, which is normal.\nCPU Spikes Symptoms:\nprocess_cpu_seconds_total rate spikes Request latency increases Throttled container CPU Common causes:\nBatch processing overhead Complex processor logic High ingestion rates Inefficient regex patterns in processors Solutions:\nOptimize processor configuration Distribute load across multiple instances Use simpler transformation patterns Profile with pprof to identify hotspots For detailed debugging workflows, see Guide — How to Debug OpenTelemetry Pipelines.\nSelf-Monitoring Architecture A production self-monitoring setup exports Collector telemetry to external systems:\ngraph TB A[OTel Collector Instance] --\u003e|Internal Metrics| B[Prometheus Exporter :8888] A --\u003e|Internal Logs| C[Stderr Logs] A --\u003e|Health Checks| D[Health Check Extension :13133] A --\u003e|Debugging| E[zPages Extension :55679] A --\u003e|Profiling| F[pprof Extension :1777] B --\u003e|Scrape| G[Prometheus/Metrics Backend] C --\u003e|Collect| H[Log Aggregation System] D --\u003e|Probe| I[Kubernetes/Load Balancer] G --\u003e J[Alerting \u0026 Dashboards] H --\u003e J K[Monitoring Collector] --\u003e|Scrape :8888| B K --\u003e|Send to Backends| L[External Observability Platform] style A fill:#e1f5ff style B fill:#fff4e6 style C fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#e8f5e9 style H fill:#e8f5e9 style I fill:#e8f5e9 style J fill:#ffe6e6 style K fill:#e1f5ff style L fill:#e8f5e9 Configuration Best Practices Production Monitoring Setup 1. Enable comprehensive internal metrics:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed logs: level: info encoding: json 2. Deploy monitoring Collector:\nCreate a dedicated Collector instance to scrape other Collectors:\nreceivers: prometheus: config: scrape_configs: - job_name: otel-collector scrape_interval: 15s static_configs: - targets: ['collector-1:8888', 'collector-2:8888'] exporters: otlp: endpoint: monitoring-backend:4317 service: pipelines: metrics: receivers: [prometheus] exporters: [otlp] 3. Configure health checks:\nextensions: health_check: endpoint: 0.0.0.0:13133 service: extensions: [health_check] 4. Set up alerts:\nKey alerts to configure:\nQueue size \u003e 80% capacity Export failure rate \u003e 1% Memory usage \u003e 80% limit CPU throttling detected Receiver refused rate \u003e 0 Development/Debugging Setup Enable all diagnostic extensions:\nextensions: health_check: endpoint: 0.0.0.0:13133 zpages: endpoint: 0.0.0.0:55679 pprof: endpoint: 0.0.0.0:1777 service: extensions: [health_check, zpages, pprof] telemetry: logs: level: debug Add debug exporters:\nexporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, jaeger] # Debug alongside production Security Considerations Restrict extension endpoints to internal networks only Never expose pprof to the internet Use TLS for metrics endpoints in production Implement authentication for sensitive endpoints Rate limit health check endpoints to prevent DoS Integration Points BattleBots Collector Monitoring For the BattleBots platform, Collector self-monitoring would track:\nOperational metrics:\nGame event ingestion rate (log records/second) Battle trace throughput (spans/second) Bot performance metric collection (metric points/second) Health indicators:\nExport success rate to observability backends Queue utilization during peak match activity Resource usage (CPU, memory) per Collector instance Alerting scenarios:\nQueue capacity exceeded during tournament events Export failures to game analytics backend High latency in telemetry pipeline affecting real-time dashboards Kubernetes Integration In Kubernetes deployments:\nLiveness probe:\nlivenessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 30 periodSeconds: 10 Readiness probe:\nreadinessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 5 periodSeconds: 5 Metrics scraping:\nannotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8888\" prometheus.io/path: \"/metrics\" Further Reading Official Documentation Internal telemetry Troubleshooting Configuration Extensions README Extension Documentation Health Check Extension zPages Extension Collector Observability Documentation Guides and Best Practices Monitoring and Debugging the OpenTelemetry Collector How to Monitor Open Telemetry Collector Performance Guide — How to Debug OpenTelemetry Pipelines Health Check Monitoring With OpenTelemetry Vendor Resources Dynatrace: Introducing OpenTelemetry Collector Self-Monitoring Dashboards Dynatrace: OpenTelemetry Collector self-monitoring OpenTelemetry Collector from A to Z: A Production-Ready Guide Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces ","categories":"","description":"How to observe and debug the OpenTelemetry Collector itself through internal telemetry, extensions, and monitoring strategies.\n","excerpt":"How to observe and debug the OpenTelemetry Collector itself through …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-self-monitoring/","tags":"","title":"OpenTelemetry Collector: Self-Monitoring"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for distributed tracing, enabling collection, processing, and export of trace data from multiple sources to various backend systems. Distributed tracing tracks requests as they flow through distributed systems, providing visibility into service interactions, latency bottlenecks, and error propagation paths.\nThe Collector acts as a central hub for trace data, accepting traces in multiple formats (OTLP, Jaeger, Zipkin), performing intelligent sampling decisions, and routing to multiple tracing backends simultaneously. This unified approach simplifies observability infrastructure while preserving the ability to use best-of-breed tools for different use cases.\nKey capabilities include protocol translation between trace formats, sophisticated sampling strategies (head-based and tail-based), trace enrichment with resource and span attributes, and correlation with metrics and logs through shared context identifiers.\nKey Concepts Traces and Spans A trace represents the full journey of one request or transaction across services, while a span is a timed unit of work inside that journey such as a function call, database query, or external API call.\nTrace structure:\nA trace consists of one or more spans organized in a tree structure Each span represents an operation with a start time and duration Spans have parent-child relationships forming the call graph The root span represents the initial request entry point Span characteristics:\nName: Describes the operation (e.g., “GET /api/battles”) Start time and duration: Timing information Status: Success, error, or unset Span kind: Client, server, internal, producer, or consumer Span Context and Propagation Span context is the portion of a span that must be serialized and propagated between services to maintain trace continuity.\nContext components:\nTraceId: Unique identifier for the entire trace (shared across all spans) SpanId: Unique identifier for the specific span TraceFlags: Sampling and other flags TraceState: System-specific trace state values Propagation mechanism:\nContext propagation transmits context between services via protocols such as HTTP headers, gRPC metadata, or message queues. The default propagator uses the W3C TraceContext specification with traceparent and tracestate headers.\nExample HTTP headers:\ntraceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01 tracestate: vendor1=value1,vendor2=value2 For detailed propagation concepts, see An overview of Context Propagation in OpenTelemetry.\nSpan Attributes Attributes provide additional context about the operation represented by a span. They are key-value pairs that describe request parameters, database queries, HTTP methods, status codes, and other relevant details.\nCommon attribute categories:\nHTTP attributes: http.method, http.status_code, http.route Database attributes: db.system, db.statement, db.name RPC attributes: rpc.service, rpc.method Network attributes: net.peer.name, net.peer.port Best practice: Set attributes at span creation rather than later, since samplers can only consider information present during span creation.\nSpan Events Span events are structured log messages or annotations on a span, typically used to denote meaningful singular points in time during the span’s duration.\nUse cases:\nException events (including stack traces) Checkpoint markers in long operations State transitions Cache hits/misses Retry attempts Events include a name, timestamp, and optional attributes, providing detailed debugging context without creating separate spans for every sub-operation.\nSpan Links Span links establish relationships between spans in different traces or between causally-related but non-parent-child spans. Common scenarios include:\nBatch processing where one span processes multiple input messages Following redirects across multiple traces Async operations spawned from a parent request Trace Receivers Receivers collect trace data from various sources and convert it into the OpenTelemetry traces data model.\nOTLP Receiver The OTLP receiver accepts trace data transmitted using the OpenTelemetry Protocol, the native and recommended format for OpenTelemetry traces.\nSupported transports:\ngRPC (default port 4317): High-performance binary protocol HTTP (default port 4318): RESTful endpoint at /v1/traces Configuration example:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 Use cases:\nCollecting traces from OpenTelemetry-instrumented applications Multi-tier collector deployments (agent → gateway) Modern observability architectures Important: Jaeger V2 natively supports OTLP, making OTLP the recommended protocol for Jaeger backends.\nJaeger Receiver The Jaeger receiver receives trace data in Jaeger format and translates it to OpenTelemetry format. This enables migration from Jaeger-instrumented applications without requiring code changes.\nSupported protocols:\ngRPC (default port 14250): Binary Jaeger protocol thrift_compact (default port 6831): UDP-based compact Thrift thrift_http (default port 14268): HTTP-based Thrift thrift_binary: TCP-based binary Thrift Configuration example:\nreceivers: jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_http: endpoint: 0.0.0.0:14268 thrift_compact: endpoint: 0.0.0.0:6831 Use cases:\nMigrating from Jaeger agent/collector infrastructure Supporting legacy applications instrumented with Jaeger SDKs Gradual transition to OpenTelemetry Zipkin Receiver The Zipkin receiver receives spans in Zipkin V1 and V2 formats and translates them to OpenTelemetry format.\nConfiguration example:\nreceivers: zipkin: endpoint: 0.0.0.0:9411 Use cases:\nMigrating from Zipkin instrumentation Supporting applications instrumented with Zipkin libraries Integration with Zipkin-compatible systems Protocol Translation The Collector acts as a protocol translator, accepting traces in one format and exporting in another. This enables:\nJaeger-instrumented apps → OTLP export to modern backends OpenTelemetry apps → Zipkin export for legacy systems Unified collection from heterogeneous instrumentation Sampling Strategies Sampling controls which traces are retained for analysis, balancing observability value with storage costs and performance impact.\nHead Sampling Head sampling makes sampling decisions at trace creation time, before seeing the complete trace. The decision applies to the entire trace and propagates to downstream services.\nCommon algorithms:\nAlways On: Sample 100% of traces (development/debugging) Always Off: Sample 0% of traces (disable tracing) TraceID Ratio: Sample a percentage based on TraceId hash (e.g., 10%) Rate Limiting: Sample at most N traces per second Advantages:\nLow latency decision (immediate) Low memory overhead (no buffering) Consistent across distributed services Limitations:\nCannot make decisions based on complete trace data Cannot guarantee capturing all error traces Cannot sample based on span attributes or duration For consistent probability sampling details, see OpenTelemetry Sampling.\nTail Sampling Tail sampling makes sampling decisions after seeing all or most spans in a trace, enabling more intelligent sampling based on trace characteristics.\nAvailable policies:\nLatency: Sample traces exceeding duration threshold Status code: Always sample traces with errors Numeric attribute: Sample based on attribute values (min/max thresholds) Probabilistic: Sample a percentage of traces String attribute: Sample traces matching string attributes Rate limiting: Limit traces per second per policy Composite: Combine multiple policies (AND/OR logic) Configuration example:\nprocessors: tail_sampling: policies: - name: errors-policy type: status_code status_code: status_codes: [ERROR] - name: slow-requests type: latency latency: threshold_ms: 1000 - name: sample-10-percent type: probabilistic probabilistic: sampling_percentage: 10 Architecture requirements:\nAll spans for a given trace MUST be received by the same collector instance for effective sampling decisions. This requires:\nLoad balancing exporter: Routes spans by TraceId to consistent collectors Two-tier architecture: Agent collectors → tail sampling gateway collectors For implementation guidance, see Tail Sampling with OpenTelemetry and New Relic and Sampling at scale with OpenTelemetry.\nAdvantages:\nSample all error traces regardless of volume Capture slow requests while dropping fast ones Make sampling decisions based on complete trace data Challenges:\nHigher memory overhead (buffering complete traces) Increased latency (waiting for trace completion) Requires stateful, coordinated collectors Sampling Best Practices For production deployments:\nUse head sampling for baseline traffic reduction (e.g., 10% sampling) Add tail sampling to always capture errors and slow traces Implement two-tier architecture for tail sampling at scale Monitor sampled vs. unsampled trace ratios Adjust policies based on traffic patterns and costs For recent sampling updates, see OpenTelemetry Sampling update.\nTrace Processors Processors transform and enrich trace data as it flows through pipelines.\nSpan Processor The span processor modifies span names, attributes, and other properties.\nCommon operations:\nRename spans for consistency Add/remove span attributes Set span status Modify span kind Attributes Processor Adds, updates, or deletes span and resource attributes, enabling:\nEnvironment labeling Team ownership tags PII removal Attribute normalization Resource Detection Processor Enriches traces with environment metadata:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information Host details This automatic enrichment enables filtering and grouping traces by infrastructure context.\nBatch Processor Groups spans before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nService Graph Processor Generates metrics representing service call relationships from trace data, creating:\nRequest rate between services Error rate between services Latency between services These derived metrics enable service dependency visualization without query-time trace aggregation.\nTrace Exporters Exporters send processed trace data to observability backends and storage systems.\nOTLP Exporter (Recommended) The OTLP exporter sends traces using the OpenTelemetry Protocol to OTLP-compatible backends. OTLP is the recommended choice for new deployments as it’s designed with the OpenTelemetry data model in mind, emitting trace data without loss of information.\nSupported destinations:\nJaeger V2 (native OTLP support) Commercial platforms (Datadog, New Relic, Honeycomb, Dynatrace) Cloud vendor endpoints (AWS X-Ray, Google Cloud Trace, Azure Monitor) Open source backends (Uptrace, Grafana Tempo) Configuration example:\nexporters: otlp: endpoint: jaeger:4317 tls: insecure: false For Jaeger V2 integration, see Using OpenTelemetry to send traces to Jaeger V2.\nJaeger Exporter The Jaeger exporter sends traces to Jaeger backends using the Jaeger gRPC protocol.\nNote: For Jaeger V2, use the OTLP exporter instead. The dedicated Jaeger exporter is maintained for backward compatibility with Jaeger V1 deployments.\nZipkin Exporter The Zipkin exporter sends traces to Zipkin-compatible backends.\nUse cases:\nLegacy Zipkin deployments Systems expecting Zipkin format Gradual migration scenarios Logging Exporter Writes traces to collector standard output for debugging and development.\nFor comprehensive exporter documentation, see OpenTelemetry Collector Exporters.\nTrace Pipeline Flow A sophisticated trace pipeline with multiple receivers, sampling, and multi-backend export:\ngraph LR A[OTLP SDK] --\u003e B[OTLP Receiver] C[Jaeger SDK] --\u003e D[Jaeger Receiver] E[Zipkin SDK] --\u003e F[Zipkin Receiver] B --\u003e G[Resource Detection] D --\u003e G F --\u003e G G --\u003e H[Attributes Processor] H --\u003e I{Load Balancer} I --\u003e|By TraceId| J[Tail Sampling Processor] J --\u003e K[Batch Processor] K --\u003e L[OTLP Exporter] L --\u003e M[Jaeger V2] K --\u003e N[OTLP Exporter] N --\u003e O[Cloud Backend] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#f3e5f5 style L fill:#e8f5e9 style N fill:#e8f5e9 style M fill:#e1f5ff style O fill:#e1f5ff Configuration Considerations Context Propagation Ensure consistent propagation across all services:\nConfigure the same propagators in all SDKs Use W3C TraceContext (standard default) Include Baggage propagation if using cross-cutting concerns Test propagation across language boundaries Sampling Trade-offs Balance observability and cost:\nHigh sampling (50-100%): Development, debugging, low-traffic systems Medium sampling (10-30%): Production with moderate traffic Low sampling (1-10%): High-traffic production systems Tail sampling: Always capture errors regardless of base rate Performance Tuning For high-throughput trace collection:\nEnable batching with appropriate size/timeout Use multiple collector instances with load balancing Configure adequate memory for tail sampling buffers Monitor collector CPU and memory usage Consider two-tier architecture (agent + gateway) Storage Optimization Manage trace storage costs:\nImplement retention policies in backends Use sampling to reduce volume Drop high-cardinality attributes if needed Compress trace data before export Integration Points BattleBots Trace Collection For the BattleBots platform, distributed tracing would track:\nRequest flows:\nClient WebSocket connection → authentication → game state sync Player action → validation → state update → broadcast Match creation → bot pairing → game initialization Service interactions:\nAPI gateway → game service → persistence layer Event publisher → message broker → subscriber services Load balancer → multiple game server instances Timing analysis:\nEnd-to-end battle action latency Database query performance WebSocket message propagation time The OTLP receiver collects traces from Go services instrumented with the OpenTelemetry Go SDK, while Jaeger/Zipkin receivers support any legacy instrumentation.\nTrace-Log Correlation Connecting traces and logs enables powerful debugging workflows:\nStart with trace: Identify slow or failing request Find associated logs: Query logs by TraceId and SpanId Examine context: Read detailed log messages and exceptions Understand causation: See timeline of events leading to issue This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically when both signals are instrumented.\nTrace-Metric Correlation Link traces and metrics through exemplars:\nHistogram buckets contain sample trace IDs Click from high-latency metric to example slow trace Correlate error rate spike with specific failing traces Validate fixes by monitoring metrics and inspecting traces Further Reading Official Documentation OpenTelemetry Traces Specification Context Propagation Sampling Receiver Components Processor Components Exporter Components Sampling Resources Tail Sampling Processor OpenTelemetry Sampling Tail Sampling with OpenTelemetry and New Relic Sampling at scale with OpenTelemetry OpenTelemetry Sampling update Integration Guides Getting Started with the Jaeger and Zipkin Receivers Using OpenTelemetry to send traces to Jaeger V2 OpenTelemetry Collector Exporters Context Propagation Deep Dives An overview of Context Propagation in OpenTelemetry OpenTelemetry Context Propagation Explained Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles distributed tracing data, including the span model, receivers, sampling strategies, and exporters.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles distributed …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-traces/","tags":"","title":"OpenTelemetry Collector: Traces Support"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/tags/","tags":"","title":"Tags"}]