[{"body":"Idea This proof of concept demonstrates containerized bots battling each other in a 1v1 match within a 2-dimensional space. To evaluate the optimal architecture for the final BattleBots platform, both a client/server implementation and a peer-to-peer implementation will be completed and compared.\nRequirements Client/Server Architecture Implement a client/server architecture to evaluate its suitability for the final BattleBots platform.\nPeer-to-Peer Architecture Implement a peer-to-peer architecture to compare against the client/server approach.\n1v1 Battle Bots compete in a 1v1 battle within a 2-dimensional space.\nContainerized Bots Each bot should be a container to ensure isolation and portability.\nLanguage-Agnostic Bot Implementation The game logic should be independent of each bot so that any programming language can be used to implement a bot.\nObservability Observability signals should be captured so the battle can be monitored in real-time.\nBattle Visualization A battle visualization should be implemented to display the battle state and actions.\nPending ADRs ADR-NNNN: Client/Server Architecture This ADR will document the design decisions for the client/server implementation, including the server’s responsibilities for game state management, turn coordination, and validation of bot actions.\nADR-NNNN: Peer-to-Peer Architecture This ADR will document the design decisions for the peer-to-peer implementation, including consensus mechanisms for game state, conflict resolution, and how bots communicate directly with each other.\nADR-NNNN: Game Runtime Architecture This ADR will define the “game loop” and core game mechanics, including turn-based vs. real-time gameplay, tick rates, state updates, and the overall flow of battle execution.\nADR-NNNN: Bot to Battle Server Interface This ADR will define the communication protocol between bots and the battle server or between bots in P2P mode, evaluating options such as gRPC, HTTP, or custom TCP/UDP packets.\nADR-NNNN: Observability Stack This ADR will document the observability architecture, including metrics collection, logging, tracing, and how battle telemetry is captured and exposed for monitoring and analysis.\nADR-NNNN: Battle Visualization This ADR will document the design of the battle visualization system, including the rendering approach, real-time updates, and how observability data is translated into visual representations.\n","categories":"","description":"Documents the proof of concept for running a 1v1 battle between two containerized bots using podman-compose\n","excerpt":"Documents the proof of concept for running a 1v1 battle between two …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/0001-poc/","tags":"","title":"[0001] Proof of Concept - 1v1 Battle"},{"body":" Context and Problem Statement As the project grows, architectural decisions are made that have long-term impacts on the system’s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.\nHow should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?\nDecision Drivers Need for clear documentation of architectural decisions and their rationale Easy accessibility and searchability of past decisions Low barrier to entry for creating and maintaining decision records Integration with existing documentation workflow Version control friendly format Industry-standard approach that team members may already be familiar with Considered Options MADR (Markdown Architectural Decision Records) ADR using custom format Wiki-based documentation No formal ADR process Decision Outcome Chosen option: “MADR (Markdown Architectural Decision Records)”, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.\nConsequences Good, because MADR is a widely adopted standard with clear documentation and examples Good, because markdown files are easy to create, edit, and review through pull requests Good, because ADRs will be version-controlled alongside code, maintaining historical context Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions Good, because team members can easily search and reference past decisions Neutral, because requires discipline to maintain and update ADR status as decisions evolve Bad, because team members need to learn and follow the MADR format conventions Confirmation Compliance will be confirmed through:\nCode reviews ensuring new architectural decisions are documented as ADRs ADRs are stored in docs/content/r\u0026d/adrs/ following the naming convention NNNN-title-with-dashes.md Regular reviews during architecture discussions to reference and update existing ADRs Pros and Cons of the Options MADR (Markdown Architectural Decision Records) MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.\nGood, because it’s a well-established standard with extensive documentation Good, because markdown is simple, portable, and version-control friendly Good, because it provides a clear structure while remaining flexible Good, because it integrates with static site generators and documentation tools Good, because it’s lightweight and doesn’t require special tools Neutral, because it requires some initial learning of the format Neutral, because maintaining consistency requires discipline ADR using custom format Create our own custom format for architectural decision records.\nGood, because we can tailor it exactly to our needs Bad, because it requires defining and maintaining our own standard Bad, because new team members won’t be familiar with the format Bad, because we lose the benefits of community knowledge and tooling Bad, because it may evolve inconsistently over time Wiki-based documentation Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.\nGood, because wikis provide easy editing and hyperlinking Good, because some team members may be familiar with wiki tools Neutral, because it may or may not integrate with version control Bad, because content may not be version-controlled alongside code Bad, because it creates a separate system to maintain Bad, because it’s harder to review changes through standard PR process Bad, because portability and long-term accessibility may be concerns No formal ADR process Continue without a structured approach to documenting architectural decisions.\nGood, because it requires no additional overhead Bad, because context and rationale for decisions are lost over time Bad, because new team members struggle to understand why decisions were made Bad, because it leads to repeated discussions of previously settled questions Bad, because it makes it difficult to track when decisions should be revisited More Information MADR 4.0.0 specification: https://adr.github.io/madr/ ADRs will be categorized as: strategic, user-journey, or api-design ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX All ADRs are stored in docs/content/r\u0026d/adrs/ directory ","categories":"","description":"Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.\n","excerpt":"Adopt Markdown Architectural Decision Records (MADR) as the standard …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/0001-use-madr-for-architecture-decision-records/","tags":"","title":"[0001] Use MADR for Architecture Decision Records"},{"body":"User Journeys This section contains detailed user journey documentation that defines how users interact with the Battlebots platform. Each journey document includes:\nUser personas and their goals Step-by-step flow diagrams Technical requirements (access control, analytics, etc.) Success metrics These documents serve as the foundation for feature development and help ensure a consistent, user-centered experience.\n","categories":"","description":"Documentation of user flows and experiences for the Battlebots platform\n","excerpt":"Documentation of user flows and experiences for the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/","tags":"","title":"User Journeys"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/","tags":"","title":"Analysis"},{"body":"Architecture Decision Records (ADRs) This section contains architectural decision records that document the key design choices made for the Battlebots platform. Each ADR follows the MADR 4.0.0 format and includes:\nContext and problem statement Decision drivers and constraints Considered options with pros and cons Decision outcome and rationale Consequences (positive and negative) Confirmation methods ADR Categories ADRs are classified into three categories:\nStrategic - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices. User Journey - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features. API Design - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation. Status Values Each ADR has a status that reflects its current state:\nproposed - Decision is under consideration accepted - Decision has been approved and should be implemented rejected - Decision was considered but not approved deprecated - Decision is no longer relevant or has been superseded superseded by ADR-XXXX - Decision has been replaced by a newer ADR These records provide historical context for architectural decisions and help ensure consistency across the platform.\n","categories":"","description":"Documentation of architectural decisions made in the Battlebots platform using MADR 4.0.0 standard\n","excerpt":"Documentation of architectural decisions made in the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/","tags":"","title":"Architecture Decision Records"},{"body":"Welcome! Battle Bots is a game in which you, the human, implement an autonomous “bot” to do battle with “bots” implemented by other humans.\nWhat is a Bot? A bot is a independent piece of software which is programmed to battle other bots by reacting to state updates (e.g. bot B moved to point A) and performing its own actions (e.g. fire missile at point A).\n","categories":"","description":"Battle Bots is a PVP game for autonomous players","excerpt":"Battle Bots is a PVP game for autonomous players","ref":"/battlebots/pr-preview/pr-150/","tags":"","title":"Battle Bots"},{"body":"R\u0026D Process The Research \u0026 Design process follows a structured workflow to ensure comprehensive analysis and documentation of user experiences, technical solutions, and implementation details.\nProcess Steps Document the User Journey\nCreate a user journey document for the specific user experience Include flow diagrams using Mermaid to visualize user interactions Define prioritized technical requirements (P0/P1/P2) Use the /new-user-journey command to create standardized documentation Design the Solution\nCreate an ADR that designs a solution to implement the user journey Identify and document: Additional ADRs needed for specific components APIs that need to be defined User interface flows (mobile, web, etc.) Data flow from user to end systems (database, notification system, etc.) Capture the complete system architecture and integration points Document Component ADRs\nCreate ADRs for specific technical components identified in the solution design Examples: authentication strategy, session management, account linking, data storage Use the /new-adr command to create standardized MADR 4.0.0 format documents Document technical decisions with context, considered options, and consequences Document Required APIs\nFor each API endpoint identified in the solution, create comprehensive API documentation Use the /new-api-doc command to create standardized documentation Include: Request/response schemas Authentication requirements Business logic flows (Mermaid diagrams) Error responses and status codes Example curl requests Document API Implementation\nFor each documented API, create an ADR describing the implementation approach Document technical decisions including: Programming language selection Framework and libraries Architecture patterns Testing strategy Example: ADR-0006 documents the tech stack for API development (z5labs/humus framework) Design User Interface\nCreate UI/UX designs for the user journey Ensure designs align with the documented user flows and API contracts Consider platform-specific requirements (mobile, web, desktop) Documentation Structure The R\u0026D documentation is organized into the following sections:\nUser Journeys - User experience flows with technical requirements ADRs - Architectural Decision Records documenting technical decisions APIs - REST API endpoint documentation with schemas and examples Analysis - Research and analysis of technologies and solutions ","categories":"","description":"","excerpt":"R\u0026D Process The Research \u0026 Design process follows a structured …","ref":"/battlebots/pr-preview/pr-150/research_and_development/","tags":"","title":"Research \u0026 Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/categories/","tags":"","title":"Categories"},{"body":"Overview Grafana Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. Unlike other log aggregation systems, Loki is designed to be cost-effective and easy to operate by not indexing the contents of logs, but rather a set of labels for each log stream.\nThis analysis explores Loki as a potential log storage backend for the BattleBots platform, focusing on its architecture, deployment options, OTLP compatibility, and integration with the OpenTelemetry Collector.\nWhy Research Loki? For the BattleBots observability stack, Loki offers several compelling advantages:\nNative OTLP Support: Loki v3+ provides native OTLP ingestion endpoints, enabling seamless integration with the OpenTelemetry Collector Cost-Effective Storage: Index-free approach dramatically reduces storage costs compared to full-text indexing systems Horizontal Scalability: Microservices architecture supports scaling from development to production workloads Grafana Integration: Tight integration with Grafana provides unified visualization of logs, metrics, and traces Cloud-Native Design: Built for containerized environments with Kubernetes-first deployment patterns Document Structure The Loki analysis is organized into the following documents:\nLoki Overview Comprehensive overview covering architecture, deployment, and operational best practices.\nTopics covered:\nWhat is Loki and its design philosophy (index-free, label-based querying) Core concepts: streams, labels, chunks, index, LogQL Architecture components: distributor, ingester, querier, compactor Deployment modes: monolithic, simple scalable, microservices How to run Loki with Docker/Podman Compose for POC Best practices for label strategy and configuration When to use Loki vs. alternatives like Elasticsearch Audience: Everyone—provides foundational understanding for evaluating Loki as a log backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration.\nTopics covered:\nNative OTLP support in Loki v3+ (endpoints, configuration) OTel Collector otlphttp exporter setup Resource attribute mapping to Loki labels Log-trace correlation via TraceID/SpanID Complete working configuration examples Authentication and multi-tenancy Troubleshooting common integration issues Audience: Developers and operators implementing the OTel Collector to Loki pipeline.\nBattleBots Integration Context For the BattleBots platform, Loki would serve as the centralized log storage backend, receiving logs from the OpenTelemetry Collector via OTLP. This enables:\nGame Event Logging Battle events (bot actions, damage calculations, victory conditions) Game state transitions and timing information Player actions and command processing Error conditions and system anomalies Infrastructure Logging Container logs from game servers System logs from host infrastructure Application logs from Go services Network and security logs Unified Observability Log-trace correlation through shared TraceID/SpanID Linking log events to metrics and distributed traces Grafana dashboards combining logs, metrics, and traces Streamlined debugging workflows across all telemetry signals Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the log storage backend for BattleBots. Key decision factors include:\nFunctional fit: Does Loki meet log storage, query, and correlation requirements? Operational complexity: How difficult is it to deploy, monitor, and maintain Loki? Cost: What are the infrastructure and operational costs at POC and production scale? Integration: How well does it integrate with OTel Collector, Grafana, and the broader observability stack? External Resources Grafana Loki Official Documentation Loki GitHub Repository Grafana Labs Blog CNCF Loki Project ","categories":"","description":"Research and analysis of Grafana Loki log aggregation system for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Loki log aggregation system for the …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/","tags":"","title":"Grafana Loki"},{"body":"Overview This section contains research and analysis of log storage solutions for the BattleBots platform. Effective log storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Log Backend Options Grafana Loki Analysis of Grafana Loki, a horizontally scalable, multi-tenant log aggregation system optimized for storing and querying log data.\nLoki uses a unique index-free approach that indexes only metadata labels rather than full log content, significantly reducing storage and operational costs compared to traditional log aggregation systems.\nKey features include:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and how to run Loki OTLP compatibility and OTel Collector integration Best practices for running and operating Loki Future Analysis Additional log backend options may be researched based on BattleBots requirements:\nElasticsearch/OpenSearch - Full-text search capabilities Cloud-native options - AWS CloudWatch Logs, Google Cloud Logging Self-hosted alternatives - ClickHouse, Vector Related Documentation R\u0026D Documentation OpenTelemetry Collector Analysis - Log collection and processing Observability Analysis - Overall observability strategy Future ADR on observability stack selection External Resources Grafana Loki Documentation OpenTelemetry Documentation CNCF Observability Projects ","categories":"","description":"Research and analysis of log storage backends for the BattleBots observability stack.\n","excerpt":"Research and analysis of log storage backends for the BattleBots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/","tags":"","title":"Log Storage Analysis"},{"body":"Overview Grafana Loki introduced native OpenTelemetry Protocol (OTLP) support in version 3.0, marking a significant advancement in how logs can be ingested into Loki. This native integration allows applications instrumented with OpenTelemetry to send logs directly to Loki using the standardized OTLP format, eliminating the need for format transformations and simplifying the observability pipeline.\nThe native OTLP endpoint provides a fully OpenTelemetry-compliant ingestion path where logs sent in OTLP format are stored directly in Loki without requiring conversion to JSON or logfmt blobs. This approach leverages Loki’s structured metadata feature, which stores log attributes and other OpenTelemetry LogRecord fields separately from the log body itself. The result is a more intuitive query experience and better performance, as queries no longer need to parse JSON at runtime to access fields.\nFor the BattleBots observability stack, native OTLP integration offers several advantages: unified telemetry collection across logs, metrics, and traces through the OpenTelemetry Collector; simplified configuration compared to legacy exporters; better correlation between logs and traces through preserved TraceId and SpanId fields; and vendor portability, making it easier to migrate between observability backends without changing instrumentation.\nOTLP Support in Loki Answer: YES - Loki versions 3.0 and later natively support the OpenTelemetry Protocol (OTLP) for log ingestion.\nNative OTLP Endpoint Loki exposes an OTLP-compliant endpoint at /otlp that accepts OpenTelemetry log data. When clients send logs to this endpoint, the collector automatically appends the appropriate path suffix (/v1/logs), resulting in requests to /otlp/v1/logs.\nSupported Protocols:\nHTTP: POST requests using HTTP/1.1 or HTTP/2 gRPC: Unary RPC calls using the OTLP service definition Default Port:\nLoki typically runs on port 3100 for all HTTP endpoints, including OTLP Version Requirements Minimum Loki Version: 3.0 or later\nSchema Requirements:\nSchema version: v13 or higher (required for structured metadata) Index type: tsdb (Time Series Database index required) Structured metadata is essential for OTLP ingestion because it stores the OpenTelemetry LogRecord fields (resource attributes, instrumentation scope, log attributes) separately from the log body. Without schema v13 and tsdb, Loki cannot properly handle OTLP data.\nBenefits of Native OTLP vs Legacy Loki Exporter The legacy lokiexporter component in the OpenTelemetry Collector encoded logs as JSON or logfmt blobs with Loki-specific label conventions. The native OTLP endpoint provides several improvements:\nSimplified Querying: No JSON parsing required at query time. Instead of {job=\"dev/auth\"} | json | severity=\"INFO\", you can query directly: {service_name=\"auth\"} | severity_text=\"INFO\"\nCleaner Log Bodies: The log message is stored as-is rather than wrapped in a JSON structure. A log “user logged in” is stored exactly as that string, with metadata in structured fields.\nStandard Resource Labels: Uses OpenTelemetry semantic conventions (service_name, service_namespace) instead of custom labels (job=service.namespace/service.name)\nBetter Performance: Structured metadata allows efficient filtering without parsing the entire log body\nVendor Portability: Standard OTLP configuration works across multiple backends without Loki-specific hints\nFuture-Proof: The native endpoint represents Grafana’s strategic direction for log ingestion\nOTLP Endpoint Configuration To enable OTLP ingestion in Loki, you must configure structured metadata support and optionally customize which resource attributes become index labels.\nEnabling Structured Metadata Structured metadata is enabled by default in Loki 3.0+, but you should explicitly configure it in your limits:\nlimits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 # Maximum metadata entries per log record Schema Configuration Your Loki schema must use version 13 or higher with the tsdb index:\nschema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: s3 # or filesystem, gcs, azure, etc. schema: v13 index: prefix: loki_index_ period: 24h Complete Loki Configuration Example Here’s a complete Loki configuration with OTLP support enabled:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 per_stream_rate_limit: 5MB per_stream_rate_limit_burst: 15MB # OTLP-specific configuration otlp_config: resource_attributes: # Configure which resource attributes become index labels attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.cluster.name - k8s.namespace.name - cloud.region - cloud.provider # Convert high-cardinality attributes to structured metadata - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 OTel Collector Export Configuration Answer: YES - The OpenTelemetry Collector can export logs to Loki using the otlphttp exporter.\nBasic OTLP HTTP Exporter The recommended exporter for Loki is otlphttp/logs:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true # For local development without TLS Important: Do not append /v1/logs to the endpoint URL. The OTLP exporter automatically adds the appropriate path suffix.\nComplete Exporter Configuration Here’s a production-ready configuration with retry, timeout, and queue settings:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" # TLS Configuration tls: insecure: false ca_file: /etc/ssl/certs/loki-ca.crt cert_file: /etc/ssl/certs/client.crt key_file: /etc/ssl/private/client.key min_version: \"1.2\" # Timeout for individual requests (default: 30s recommended) timeout: 30s # Retry configuration retry_on_failure: enabled: true initial_interval: 5s # Time to wait before first retry max_interval: 30s # Maximum backoff interval max_elapsed_time: 300s # Give up after 5 minutes # Queue configuration for reliability sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage # Reference to file_storage extension for persistence # Compression (gzip recommended for production) compression: gzip # Headers (for authentication, multi-tenancy) headers: X-Scope-OrgID: \"battlebots\" File Storage Extension for Persistence To persist queued logs across collector restarts, configure the file storage extension:\nextensions: file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s service: extensions: [file_storage] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Batch Processor Configuration Always include a batch processor before the exporter to optimize throughput:\nprocessors: batch: timeout: 10s # Send batch after this duration send_batch_size: 8192 # Send when batch reaches this size send_batch_max_size: 16384 # Never exceed this size Complete Service Pipeline service: extensions: [file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [resource_detection, batch, attributes] exporters: [otlphttp/logs] Resource Attribute Mapping When logs arrive via OTLP, resource attributes from the OpenTelemetry SDK map to either index labels or structured metadata in Loki.\nDefault Resource Attributes as Index Labels By default, Loki converts these 17 resource attributes to index labels:\nservice.name → service_name service.namespace → service_namespace service.instance.id → service_instance_id deployment.environment → deployment_environment cloud.region → cloud_region cloud.availability_zone → cloud_availability_zone cloud.platform → cloud_platform k8s.cluster.name → k8s_cluster_name k8s.namespace.name → k8s_namespace_name k8s.pod.name → k8s_pod_name k8s.container.name → k8s_container_name container.name → container_name k8s.replicaset.name → k8s_replicaset_name k8s.deployment.name → k8s_deployment_name k8s.statefulset.name → k8s_statefulset_name k8s.daemonset.name → k8s_daemonset_name k8s.cronjob.name → k8s_cronjob_name Note: Attribute names with dots (.) are converted to underscores (_) for Loki label compatibility.\nAttribute Transformation in Collector To add or modify resource attributes before sending to Loki:\nprocessors: resource: attributes: # Add static attributes - key: deployment.environment value: production action: insert # Copy attributes with character transformations - key: service_name from_attribute: service.name action: insert # Delete attributes you don't want - key: telemetry.sdk.version action: delete attributes: actions: # Add log-level attributes - key: environment value: production action: insert # Extract correlation IDs from log body - key: correlation_id pattern: \"correlation_id=([a-z0-9-]+)\" action: extract Example Attribute to Label Mapping Input (OpenTelemetry SDK):\n{ \"resource\": { \"attributes\": { \"service.name\": \"game-server\", \"service.namespace\": \"battlebots\", \"deployment.environment\": \"production\", \"k8s.pod.name\": \"game-server-5d7c8f9b-xq2wr\", \"k8s.namespace.name\": \"battlebots-prod\" } } } Output (Loki Labels):\n{ service_name=\"game-server\", service_namespace=\"battlebots\", deployment_environment=\"production\", k8s_namespace_name=\"battlebots-prod\" } Note: k8s.pod.name should be converted to structured metadata (see Label Strategy section).\nLabel Strategy and Best Practices Loki’s performance depends heavily on proper label cardinality management. Every unique combination of label values creates a new stream, and too many streams degrade performance significantly.\nAvoiding High Cardinality Issues Problem: High cardinality causes Loki to build a huge index and flush thousands of tiny chunks to object storage, resulting in poor performance and high costs.\nHigh-Cardinality Attributes (Avoid as Labels):\nk8s.pod.name - Each pod instance creates a new label value service.instance.id - Each service instance is unique process.pid - Changes on every process restart User IDs, request IDs, transaction IDs Timestamps, UUIDs, hashes Which Attributes to Use as Labels Good Label Candidates (Low Cardinality):\nservice.name - Limited number of services service.namespace - Few namespaces (dev, staging, prod) deployment.environment - Usually 3-5 values k8s.cluster.name - Fixed cluster names k8s.namespace.name - Limited namespaces per cluster cloud.region - Fixed set of regions log.severity or severity_text - Limited severity levels Cardinality Rule of Thumb: Keep total stream count under 10,000. With 5 labels averaging 10 values each, you get 10^5 = 100,000 streams (too many). Reduce to 3-4 labels with controlled values.\nWhich Attributes to Keep as Structured Metadata Configure high-cardinality attributes as structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid - process.command_line - host.id Structured metadata remains queryable but doesn’t create new streams:\n{service_name=\"game-server\"} | k8s_pod_name=\"game-server-5d7c8f9b-xq2wr\" Recommended Label Strategy for BattleBots limits_config: otlp_config: resource_attributes: attributes_config: # Index labels (low cardinality) - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region # Structured metadata (high cardinality or optional) - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name - process.pid Expected Cardinality:\nservice.name: ~10 services (game-server, matchmaker, auth, etc.) service.namespace: 1 value (battlebots) deployment.environment: 3 values (dev, staging, production) k8s.namespace.name: ~5 namespaces cloud.region: ~3 regions Total streams: 10 × 1 × 3 × 5 × 3 = 450 streams (excellent)\nLog-Trace Correlation OpenTelemetry’s unified data model enables seamless correlation between logs and traces through TraceId and SpanId fields embedded in log records.\nTraceID and SpanID Storage When applications instrumented with OpenTelemetry SDKs emit logs within a trace context, the SDK automatically includes:\nTraceId: Unique identifier for the entire trace SpanId: Unique identifier for the current span TraceFlags: Sampling and other flags Loki stores these fields as structured metadata, making them queryable without parsing the log body.\nQuerying Logs by Trace ID Find all logs for a specific trace:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Find logs with any trace context:\n{service_name=\"game-server\"} | trace_id != \"\" Find logs for a specific span:\n{service_name=\"game-server\"} | span_id=\"00f067aa0ba902b7\" Grafana Integration for Correlation Configure Grafana data source correlations to link Loki and Tempo:\nLoki Data Source Configuration:\n{ \"derivedFields\": [ { \"datasourceUid\": \"tempo-uid\", \"matcherRegex\": \"trace_id=(\\\\w+)\", \"name\": \"TraceID\", \"url\": \"${__value.raw}\" } ] } This creates clickable trace ID links in the Grafana Explore view, allowing you to:\nView a log entry in Loki Click the trace ID link Jump directly to the full trace in Tempo See the complete request flow with timing information Example Correlation Workflow Scenario: Investigating a slow game server response\nStart with metrics: Notice elevated response times in Prometheus metrics Query slow traces: Find traces with duration \u003e 1s in Tempo Jump to logs: Click trace ID to see all logs for that request Identify root cause: Read detailed error messages and debug logs Correlate with resources: Use k8s_pod_name metadata to check pod health Query pattern:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" | severity_text=\"ERROR\" Authentication \u0026 Multi-tenancy Loki supports both basic authentication and multi-tenant deployments for OTLP ingestion.\nBasic Authentication Setup Collector Configuration with Basic Auth:\nextensions: basicauth/otlp: client_auth: username: \"battlebots-collector\" password: \"${LOKI_PASSWORD}\" # Use environment variable exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" auth: authenticator: basicauth/otlp service: extensions: [basicauth/otlp] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Loki Configuration with Basic Auth:\nConfigure authentication in your reverse proxy (nginx, Envoy) or API gateway rather than directly in Loki:\nlocation /otlp { auth_basic \"Loki OTLP Endpoint\"; auth_basic_user_file /etc/nginx/.htpasswd; proxy_pass http://loki:3100; } Multi-tenant Headers For multi-tenant Loki deployments, use the X-Scope-OrgID header to specify the tenant:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"battlebots-production\" Dynamic Tenant Selection:\nRoute different services to different tenants using the resource processor:\nprocessors: resource: attributes: - key: loki.tenant from_attribute: deployment.environment action: insert exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"${LOKI_TENANT}\" Loki Multi-tenancy Configuration:\nauth_enabled: true limits_config: # Per-tenant rate limits ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 # Per-tenant OTLP configuration per_tenant_override_config: /etc/loki/overrides.yaml Per-tenant overrides (/etc/loki/overrides.yaml):\noverrides: battlebots-production: ingestion_rate_mb: 50 retention_period: 720h # 30 days battlebots-staging: ingestion_rate_mb: 20 retention_period: 168h # 7 days TLS Configuration Collector with TLS:\nexporters: otlphttp/logs: endpoint: \"https://loki.battlebots.example.com/otlp\" tls: insecure: false ca_file: /etc/ssl/certs/ca-bundle.crt cert_file: /etc/ssl/certs/collector-client.crt key_file: /etc/ssl/private/collector-client.key min_version: \"1.3\" server_name_override: loki.battlebots.example.com Loki TLS Configuration:\nserver: http_listen_port: 3100 grpc_listen_port: 9096 http_tls_config: cert_file: /etc/loki/tls/server.crt key_file: /etc/loki/tls/server.key client_auth_type: RequireAndVerifyClientCert client_ca_file: /etc/loki/tls/ca.crt Complete Configuration Example This section provides a full, working configuration for integrating OpenTelemetry Collector with Loki OTLP.\nFull OpenTelemetry Collector Configuration # /etc/otelcol/config.yaml extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 filelog: include: - /var/log/battlebots/*.log include_file_path: true include_file_name: false operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' processors: resourcedetection: detectors: [env, system, docker, kubernetes] timeout: 5s resource: attributes: - key: deployment.environment value: production action: insert - key: service.namespace value: battlebots action: insert attributes: actions: - key: loki.attribute.labels value: severity_text, service_name action: insert batch: timeout: 10s send_batch_size: 8192 send_batch_max_size: 16384 memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true timeout: 30s retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage compression: gzip headers: X-Scope-OrgID: \"battlebots\" debug: verbosity: detailed sampling_initial: 5 sampling_thereafter: 200 service: extensions: [health_check, pprof, zpages, file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [memory_limiter, resourcedetection, resource, attributes, batch] exporters: [otlphttp/logs] telemetry: logs: level: info metrics: address: 0.0.0.0:8888 Full Loki Configuration with OTLP # /etc/loki/config.yaml auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 50 ingestion_burst_size_mb: 100 per_stream_rate_limit: 10MB per_stream_rate_limit_burst: 20MB max_label_names_per_series: 15 otlp_config: resource_attributes: attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 Docker Compose Example version: '3.8' services: loki: image: grafana/loki:3.0.0 container_name: loki ports: - \"3100:3100\" - \"9096:9096\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml - loki-data:/loki command: -config.file=/etc/loki/config.yaml networks: - battlebots-observability otel-collector: image: otel/opentelemetry-collector-contrib:0.96.0 container_name: otel-collector ports: - \"4317:4317\" # OTLP gRPC receiver - \"4318:4318\" # OTLP HTTP receiver - \"8888:8888\" # Metrics endpoint - \"13133:13133\" # Health check volumes: - ./otel-config.yaml:/etc/otelcol/config.yaml - /var/lib/otelcol:/var/lib/otelcol command: [\"--config=/etc/otelcol/config.yaml\"] depends_on: - loki networks: - battlebots-observability grafana: image: grafana/grafana:10.4.0 container_name: grafana ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin - GF_USERS_ALLOW_SIGN_UP=false volumes: - grafana-data:/var/lib/grafana - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml depends_on: - loki networks: - battlebots-observability volumes: loki-data: grafana-data: networks: battlebots-observability: driver: bridge Grafana Data Source Configuration # grafana-datasources.yaml apiVersion: 1 datasources: - name: Loki type: loki access: proxy url: http://loki:3100 jsonData: derivedFields: - datasourceUid: tempo matcherRegex: \"trace_id=(\\\\w+)\" name: TraceID url: \"$${__value.raw}\" Step-by-Step Setup Create configuration files:\nmkdir -p battlebots-observability cd battlebots-observability # Copy the configurations above into: # - loki-config.yaml # - otel-config.yaml # - grafana-datasources.yaml # - docker-compose.yaml Start the stack:\ndocker-compose up -d Verify Loki is running:\ncurl http://localhost:3100/ready # Expected: ready Verify OTel Collector is running:\ncurl http://localhost:13133 # Expected: {\"status\":\"Server available\"} Send test logs:\ncurl -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d '{ \"resourceLogs\": [{ \"resource\": { \"attributes\": [{ \"key\": \"service.name\", \"value\": {\"stringValue\": \"test-service\"} }] }, \"scopeLogs\": [{ \"logRecords\": [{ \"timeUnixNano\": \"1640000000000000000\", \"severityText\": \"INFO\", \"body\": {\"stringValue\": \"Test log message\"} }] }] }] }' Query logs in Grafana:\nOpen http://localhost:3000 Login with admin/admin Navigate to Explore Select Loki data source Query: {service_name=\"test-service\"} Troubleshooting Connection Errors Problem: Collector cannot connect to Loki OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: Post \"http://loki:3100/otlp/v1/logs\": dial tcp: lookup loki: no such host Solutions:\nVerify Loki is running: curl http://loki:3100/ready Check DNS resolution: nslookup loki (or use IP address) Verify network connectivity: telnet loki 3100 Check Docker network configuration if using containers Verify endpoint URL doesn’t include /v1/logs suffix Problem: 404 Not Found on OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: HTTP 404 Not Found Solutions:\nVerify Loki version is 3.0 or later: curl http://loki:3100/loki/api/v1/status/buildinfo Check schema version is v13 in Loki config Verify allow_structured_metadata: true in limits_config Restart Loki after configuration changes Label Cardinality Problems Problem: Too many streams causing performance degradation\nSymptoms:\nlevel=warn msg=\"stream limit exceeded\" limit=10000 streams=15234 Solutions:\nReview current label cardinality:\ncurl http://localhost:3100/loki/api/v1/label Check value distribution per label:\ncurl http://localhost:3100/loki/api/v1/label/service_name/values Move high-cardinality attributes to structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id Set cardinality limits:\nlimits_config: max_label_names_per_series: 15 max_label_value_length: 2048 max_label_name_length: 1024 Structured Metadata Issues Problem: Structured metadata not appearing in queries\nSymptoms: Attributes missing from log entries in Grafana\nSolutions:\nVerify schema version 13 or higher Check allow_structured_metadata: true in limits Verify attributes aren’t being dropped by processors Query with explicit structured metadata filter: {service_name=\"game-server\"} | k8s_pod_name=\"pod-123\" Performance Issues Problem: Slow queries or high memory usage\nSymptoms: Grafana queries timeout or Loki OOM errors\nSolutions:\nEnable query limits:\nlimits_config: max_query_series: 500 max_query_lookback: 720h max_entries_limit_per_query: 5000 Optimize batch processor:\nprocessors: batch: timeout: 5s # Reduce for faster flushing send_batch_size: 4096 # Smaller batches Add memory limiter:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 Use query acceleration:\nlimits_config: bloom_gateway_enable_filtering: true Debugging Techniques Enable debug logging in Collector:\nexporters: debug: verbosity: detailed service: pipelines: logs: receivers: [otlp] processors: [batch] exporters: [debug, otlphttp/logs] # Add debug exporter telemetry: logs: level: debug # Collector internal logs Enable debug logging in Loki:\nserver: log_level: debug Check Loki metrics:\ncurl http://localhost:3100/metrics | grep loki_distributor Verify OTLP data structure:\n# Send test log and capture response curl -v -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d @test-log.json BattleBots Integration Points Observability Stack Architecture The Loki OTLP integration fits into the BattleBots observability stack as follows:\nGame Servers (Go) └─\u003e OpenTelemetry SDK └─\u003e OTLP/gRPC (4317) └─\u003e OTel Collector ├─\u003e Loki (Logs via OTLP) ├─\u003e Tempo (Traces via OTLP) └─\u003e Prometheus (Metrics via OTLP) └─\u003e Grafana (Visualization \u0026 Correlation) Collector → Loki Pipeline for Game Servers Recommended pipeline configuration:\nReceive logs from game servers via OTLP Detect resource attributes (Kubernetes, cloud provider) Add BattleBots-specific labels (environment, service namespace) Filter out verbose debug logs in production Batch and compress for efficiency Export to Loki via OTLP HTTP Log Types and Labeling Strategy Game Event Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" event_type: \"game_event\" # Custom label Structured Metadata: match_id: \"uuid\" player_count: 4 game_mode: \"elimination\" System Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" log_type: \"system\" Structured Metadata: k8s_pod_name: \"game-server-abc123\" severity_text: \"ERROR\" Client Connection Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" Structured Metadata: client_id: \"uuid\" connection_state: \"connected\" trace_id: \"trace-id\" # For correlation Example Queries for BattleBots Find all errors in production:\n{service_namespace=\"battlebots\", deployment_environment=\"production\"} | severity_text=\"ERROR\" Find logs for a specific match:\n{service_name=\"game-server\"} | match_id=\"550e8400-e29b-41d4-a716-446655440000\" Find all logs related to a slow trace:\n{service_namespace=\"battlebots\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Count game events by type:\nsum by (event_type) ( count_over_time({service_name=\"game-server\"}[5m]) ) Further Reading OTLP Specification OpenTelemetry Protocol Specification - Complete OTLP specification OTLP Logs Data Model - OpenTelemetry logs data model OTLP Exporter Configuration - SDK configuration guide Grafana Loki OTLP Documentation Ingesting logs to Loki using OpenTelemetry Collector - Official Loki OTLP guide Getting started with OTel Collector and Loki - Step-by-step tutorial Native OTLP vs Loki Exporter - Migration guide Loki 3.0 Release Notes - Features announcement Migration to Native OTLP Format - Cloud migration guide OpenTelemetry Collector Documentation Collector Configuration - General configuration guide Collector Resiliency - Retry and queue configuration Configuration Best Practices - Security and performance Exporter Helper Documentation - Retry and queue details Loki Configuration and Operations Understanding Labels - Label strategy guide Structured Metadata - Structured metadata overview Loki Schema Configuration - Schema v13 setup Upgrade to Loki 3.0 - Migration instructions Grafana Integration Trace Integration in Explore - Log-trace correlation Trace Correlations - Setting up correlations Configure Loki Data Source - Data source configuration Community Guides and Tutorials Grafana Loki 101: Ingesting with OTel Collector - Official blog tutorial Building a Logging Pipeline with OTel, Loki, and Grafana - Docker Compose guide Logging with OpenTelemetry and Loki - Practical implementation Efficient Application Log Collection - Analysis guide Troubleshooting and Best Practices OTel Batching Best Practices - Batch configuration Collector Persistence and Retry - Deep dive Label Cardinality Issues - Common problems and solutions Related BattleBots Documentation OpenTelemetry Collector: Logs Support - Collector log processing OpenTelemetry Collector: Overview - Collector architecture ","categories":"","description":"Detailed guide for integrating Grafana Loki with OpenTelemetry Collector via native OTLP support.\n","excerpt":"Detailed guide for integrating Grafana Loki with OpenTelemetry …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/loki-otlp-integration/","tags":"","title":"Loki: OTLP Integration"},{"body":"Overview Grafana Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system designed to be cost-effective and easy to operate. Inspired by Prometheus, Loki takes a fundamentally different approach to log storage compared to traditional systems like Elasticsearch.\nThe core innovation of Loki is its index-free architecture: instead of indexing the full contents of log lines, Loki only indexes metadata labels for each log stream. This design dramatically reduces storage costs, memory requirements, and operational complexity while still enabling fast queries through label-based filtering and grep-style text search.\nLoki integrates seamlessly with Grafana for visualization, supports native OTLP ingestion (v3+), and works alongside Prometheus and Tempo to provide a complete observability stack. The system is built for cloud-native environments with first-class support for Kubernetes, containerized workloads, and distributed architectures.\nKey Concepts Understanding Loki’s data model is essential for effective deployment and usage.\nStreams A stream is the fundamental data structure in Loki, representing a sequence of log entries that share the same set of labels. Each unique combination of labels creates a new stream. For example:\n{app=\"battleserver\", env=\"prod\", region=\"us-east\"} {app=\"battleserver\", env=\"prod\", region=\"us-west\"} {app=\"matchmaker\", env=\"prod\", region=\"us-east\"} These three label sets create three distinct streams. All log entries with identical labels are appended to the same stream in chronological order.\nLabels Labels are key-value pairs that categorize and identify log streams. Labels are the only metadata indexed by Loki, making label design the most critical aspect of Loki deployment.\nLabels should be:\nLow cardinality: Use labels that have bounded, predictable values (e.g., environment, service, host) Descriptive: Represent the source or context of logs (e.g., namespace, pod, container) Static: Avoid labels that change frequently or have unique values per log entry Anti-pattern: Using high-cardinality values like user IDs, trace IDs, or timestamps as labels will severely degrade performance.\n# Good label design (low cardinality) {service=\"game-server\", environment=\"production\", region=\"us-east-1\"} # Bad label design (high cardinality) {trace_id=\"abc123\", user_id=\"user456\", timestamp=\"2025-12-03T10:00:00Z\"} Loki recommends keeping total stream count below 10,000 for small deployments and under 100,000 active streams for larger deployments.\nChunks Chunks are compressed blocks of log data from a single stream. As logs arrive for a stream, the ingester accumulates them in memory, then periodically flushes completed chunks to object storage.\nKey chunk characteristics:\nContain only data from a single stream Compressed using LZ4 or Snappy Stored in object storage (S3, GCS, MinIO, filesystem) Typically span minutes to hours of log data Subject to configurable size and time limits The chunk format optimizes for sequential reads, making time-range queries efficient.\nIndex The index stores the mapping between label sets and their corresponding chunks. Unlike full-text indexes, Loki’s index only contains:\nLabel names and values Chunk references (location, time range) Stream metadata This minimal indexing approach is what makes Loki cost-effective. The index is stored separately from chunks, typically in a different backend (BoltDB, TSDB).\nLogQL LogQL is Loki’s query language, inspired by Prometheus’s PromQL. LogQL queries have two stages:\nLog stream selection: Filter streams using label matchers Log pipeline: Parse, filter, and transform selected log lines # Select streams, then filter log content {service=\"game-server\"} |= \"error\" | json | level=\"ERROR\" # Aggregate metrics from logs rate({service=\"game-server\"} |= \"battle completed\" [5m]) # Multi-stage pipeline with parsing and filtering {namespace=\"battlebots\"} | json | line_format \"{{.message}}\" | pattern `\u003c_\u003e level=\u003clevel\u003e \u003c_\u003e` | level = \"error\" LogQL supports metric queries, allowing you to generate time-series data from logs (e.g., error rates, request counts).\nArchitecture Components Loki uses a microservices architecture where each component can be scaled independently or combined into larger deployment targets.\ngraph TB A[Log Clients\u003cbr/\u003ePromtail/Alloy/Fluentd] --\u003e|Push logs| B[Distributor] B --\u003e|Replicate| C1[Ingester 1] B --\u003e|Replicate| C2[Ingester 2] B --\u003e|Replicate| C3[Ingester N] C1 \u0026 C2 \u0026 C3 --\u003e|Flush chunks| D[Object Storage\u003cbr/\u003eS3/GCS/MinIO] C1 \u0026 C2 \u0026 C3 --\u003e|Update index| E[Index Store\u003cbr/\u003eBoltDB/TSDB] F[Grafana/API] --\u003e|Query| G[Query Frontend] G --\u003e|Split queries| H[Querier] H --\u003e|Read recent| C1 \u0026 C2 \u0026 C3 H --\u003e|Read historical| D H --\u003e|Read index| E I[Compactor] --\u003e|Compact| D I --\u003e|Update| E J[Ruler] --\u003e|Evaluate rules| H J --\u003e|Store| D style B fill:#fff4e6 style C1 fill:#fff4e6 style C2 fill:#fff4e6 style C3 fill:#fff4e6 style G fill:#e1f5ff style H fill:#e1f5ff style I fill:#f3e5f5 style J fill:#e8f5e9 style D fill:#fce4ec style E fill:#fce4ec Distributor The distributor is the entry point for log ingestion. It receives log streams from clients (Promtail, Alloy, OTLP endpoints) and routes them to ingesters.\nResponsibilities:\nValidate incoming log streams for correctness and tenant limits Apply rate limiting per tenant Hash log streams by labels to determine target ingesters Replicate each stream to multiple ingesters (default: 3 replicas) Load balance across available ingesters Distributors are stateless and can be horizontally scaled to handle high ingestion rates.\nIngester The ingester receives log streams from distributors and is responsible for:\nBuffering logs in memory for each stream Building compressed chunks Flushing chunks to object storage periodically Writing index entries Serving queries for recent (unflushed) data Ingesters maintain an in-memory index of recent logs and use a write-ahead log (WAL) for crash recovery. Upon graceful shutdown, ingesters flush all buffered data to storage.\nIngesters are stateful and require careful scaling considerations. They use consistent hashing to distribute streams evenly across instances.\nQuerier The querier executes LogQL queries by:\nFetching index data to identify relevant chunks Retrieving chunks from object storage Querying ingesters for recent unflushed data Merging results from multiple sources Applying log pipeline operations (parsing, filtering) Returning results to the query frontend Queriers are stateless and can be scaled horizontally. They cache chunk data and index lookups to improve performance.\nQuery Frontend The query frontend sits in front of queriers and provides:\nQuery splitting: Breaks large time-range queries into smaller sub-queries Query queuing: Prevents overwhelming queriers during traffic spikes Caching: Stores query results to avoid redundant computation Fair scheduling: Ensures multiple tenants share query resources equitably The frontend is optional but highly recommended for production deployments. It significantly improves query performance and protects backend components from overload.\nCompactor The compactor is a background service that:\nMerges small chunks into larger ones to improve query performance Removes duplicate data from replicated writes Applies retention policies by deleting old data Updates index to reflect compacted chunks Only one compactor should run per tenant to avoid conflicts. The compactor is critical for long-term storage efficiency.\nRuler The ruler evaluates recording rules and alerting rules against stored logs:\nRuns LogQL queries on a schedule Generates derived metrics from log data Triggers alerts based on log patterns Stores rule evaluation results The ruler is optional and typically used for log-based alerting scenarios.\nIndex Gateway The index gateway (available in recent versions) centralizes index access:\nProvides a single point for index queries Reduces load on the index store Enables better caching of index data Simplifies index backend scaling This component is particularly useful with BoltDB index backends to avoid direct file access from multiple queriers.\nDeployment Modes Loki supports three deployment modes, each balancing simplicity against scalability and operational flexibility.\nMonolithic Mode In monolithic mode, all Loki components run in a single process. This is the simplest deployment option.\nConfiguration:\nloki -target=all -config.file=loki-config.yaml Characteristics:\nSingle binary or container All components share memory and resources Minimal operational complexity Limited horizontal scalability Suitable for development and small deployments When to use:\nDevelopment and testing environments Proof-of-concept deployments Small-scale production (\u003c100GB/day log ingestion) Single-server deployments Limitations:\nCannot scale components independently Single point of failure Resource contention between components Limited to vertical scaling (bigger instances) Simple Scalable Deployment (SSD) Simple scalable deployment groups components into three logical targets: read, write, and backend.\nTargets:\nRead (-target=read): Query Frontend, Querier Write (-target=write): Distributor, Ingester Backend (-target=backend): Compactor, Ruler, Index Gateway Configuration example:\n# Write path (3 replicas for high availability) loki -target=write -config.file=loki-config.yaml # Read path (scale based on query load) loki -target=read -config.file=loki-config.yaml # Backend (single instance) loki -target=backend -config.file=loki-config.yaml Characteristics:\nSeparates read and write paths Independent scaling of ingestion vs queries Easier to operate than full microservices Supports ~1TB/day log ingestion When to use:\nMedium-scale production deployments When you need to scale reads and writes independently Kubernetes environments using Helm charts Teams wanting operational simplicity with scalability This is the recommended starting point for most production deployments.\nMicroservices Mode Microservices mode runs each Loki component as a separate deployment, providing maximum flexibility.\nComponents:\nDistributor (multiple instances) Ingester (multiple instances, stateful) Querier (multiple instances) Query Frontend (multiple instances) Compactor (single instance per tenant) Ruler (multiple instances) Index Gateway (multiple instances) Characteristics:\nEach component scaled independently Fine-grained resource allocation Most complex to deploy and maintain Supports enterprise-scale deployments (multi-TB/day) When to use:\nVery large Loki clusters (\u003e1TB/day) Organizations requiring precise control over scaling Multi-tenant SaaS deployments Teams with dedicated Loki operations expertise Considerations:\nSignificantly higher operational complexity More components to monitor and maintain Requires sophisticated orchestration (Kubernetes) Network communication overhead between components How to Run Loki This section provides practical guidance for running Loki, focusing on Docker/Podman Compose for POC and development.\nQuick Start with Docker Compose The simplest way to evaluate Loki is using the official Docker Compose example.\nStep 1: Create directory and download configurations\nmkdir loki-poc \u0026\u0026 cd loki-poc # Download Loki configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/loki-config.yaml # Download Alloy (log shipper) configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/alloy-local-config.yaml # Download Docker Compose file wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/docker-compose.yaml Step 2: Review the Docker Compose file\nThe compose file includes:\nLoki (monolithic mode) Grafana (for visualization) Grafana Alloy (log collection agent) flog (log generator for testing) Step 3: Start the stack\ndocker compose up -d Step 4: Verify deployment\n# Check Loki readiness curl http://localhost:3100/ready # Check Loki metrics curl http://localhost:3100/metrics # Access Grafana # URL: http://localhost:3000 # Default credentials: admin / admin Step 5: Query logs\nIn Grafana, navigate to Explore and select the Loki datasource to query logs using LogQL.\nBasic Configuration File Structure A minimal Loki configuration for local development:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /tmp/loki storage: filesystem: chunks_directory: /tmp/loki/chunks rules_directory: /tmp/loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24h limits_config: reject_old_samples: true reject_old_samples_max_age: 168h max_cache_freshness_per_query: 10m split_queries_by_interval: 15m query_range: align_queries_with_step: true cache_results: true ruler: alertmanager_url: http://localhost:9093 This configuration uses local filesystem storage and is suitable for development only.\nProduction Configuration with MinIO For a more production-like setup using MinIO as object storage:\nDocker Compose with MinIO:\nversion: \"3.8\" services: minio: image: minio/minio:latest entrypoint: - sh - -euc - | mkdir -p /data/loki-data minio server /data --console-address :9001 environment: - MINIO_ROOT_USER=loki - MINIO_ROOT_PASSWORD=supersecret - MINIO_PROMETHEUS_AUTH_TYPE=public ports: - \"9000:9000\" - \"9001:9001\" volumes: - minio-data:/data loki: image: grafana/loki:3.0.0 ports: - \"3100:3100\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml command: -config.file=/etc/loki/config.yaml depends_on: - minio grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin volumes: - grafana-data:/var/lib/grafana volumes: minio-data: grafana-data: Loki configuration with MinIO:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /loki storage: s3: endpoint: minio:9000 bucketnames: loki-data access_key_id: loki secret_access_key: supersecret s3forcepathstyle: true insecure: true replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 schema: v13 index: prefix: index_ period: 24h limits_config: ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 max_global_streams_per_user: 10000 max_query_length: 721h max_query_parallelism: 16 max_streams_per_user: 0 max_cache_freshness_per_query: 10m query_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 100 frontend: encoding: protobuf compress_responses: true max_outstanding_per_tenant: 2048 chunk_store_config: max_look_back_period: 0s table_manager: retention_deletes_enabled: true retention_period: 336h Resource Requirements Minimum requirements for development/POC:\nCPU: 2 cores Memory: 4 GB RAM Storage: 20 GB (filesystem) or object storage bucket Recommended production requirements (simple scalable mode):\nWrite path (per instance):\nCPU: 4-8 cores Memory: 8-16 GB RAM (for buffering chunks) Network: High bandwidth for ingestion Read path (per instance):\nCPU: 4-8 cores Memory: 16-32 GB RAM (for query caching) Network: High bandwidth for chunk retrieval Backend:\nCPU: 2-4 cores Memory: 4-8 GB RAM Storage: Object storage (S3, GCS, MinIO) with sufficient capacity for retention period Storage sizing:\nEstimate: ~5-10 GB/day per 1 million log lines (varies by compression ratio) Retention: storage_size = daily_volume * retention_days Index: ~1-2% of total chunk storage Getting Started Steps Choose deployment mode: Start with monolithic for POC, plan for simple scalable in production Set up object storage: MinIO for local dev, S3/GCS for production Configure Loki: Use appropriate schema version (v13 recommended) Deploy Loki: Docker Compose for POC, Helm for Kubernetes Configure log shippers: Alloy, Promtail, or OTLP endpoints Verify ingestion: Check /ready endpoint and metrics Set up Grafana: Add Loki datasource and create dashboards Test queries: Use LogQL to validate data retrieval Monitor Loki: Set up self-monitoring (metrics, logs, traces) Optimize configuration: Tune based on ingestion rate and query patterns Best Practices for Running Loki Successful Loki deployments depend on following these operational best practices.\nLabel Strategy 1. Keep labels low cardinality\nAim for 10-15 labels maximum across all streams. Each unique label combination creates a new stream.\n# Good: Low cardinality (bounded values) { service=\"game-server\", environment=\"production\", region=\"us-east-1\", cluster=\"battlebot-cluster-01\" } # Bad: High cardinality (unbounded values) { service=\"game-server\", trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", user_id=\"user_12345\", session_id=\"sess_67890\" } 2. Use static labels that describe log sources\nLabels should represent where logs come from, not what’s in them:\nnamespace, pod, container (Kubernetes) host, instance (infrastructure) service, application, component (application) environment, region, cluster (deployment context) 3. Avoid pod names and instance IDs as labels\nPod names and container IDs change frequently, creating stream churn:\n# Avoid {pod=\"game-server-abc123-xyz456\"} # Instead use {service=\"game-server\", namespace=\"battlebots\"} 4. Use filter expressions for high-cardinality data\nSearch for user IDs, trace IDs, or other high-cardinality values using LogQL filters:\n# Query for specific trace ID {service=\"game-server\"} |= \"trace_id=7a3f8c2d\" # Query for specific user {service=\"game-server\"} | json | user_id=\"12345\" 5. Use structured metadata for supplemental high-cardinality data\nLoki v2.9+ supports structured metadata, which stores high-cardinality data without indexing it:\n# Structured metadata (not indexed, not creating streams) labels: {service=\"game-server\"} structured_metadata: trace_id: \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" user_id: \"user_12345\" 6. Monitor stream count\nUse Loki metrics to track stream cardinality:\n# Total active streams loki_ingester_memory_streams # Streams per tenant sum by (tenant) (loki_ingester_memory_streams) Keep total streams under 10,000 for small deployments, under 100,000 for larger deployments.\nConfiguration Tips 1. Set appropriate limits\nlimits_config: # Rate limiting ingestion_rate_mb: 10 # MB/s per tenant ingestion_burst_size_mb: 20 # Burst allowance # Stream limits max_global_streams_per_user: 10000 # Total active streams max_line_size: 256000 # Bytes per log line max_entries_limit_per_query: 5000 # Max returned entries # Query limits max_query_length: 721h # 30 days max_query_parallelism: 16 # Concurrent query threads # Retention reject_old_samples: true reject_old_samples_max_age: 168h # 7 days 2. Configure retention\nlimits_config: retention_period: 744h # 31 days table_manager: retention_deletes_enabled: true retention_period: 744h Note: Retention requires compactor to be running.\n3. Enable caching\nquery_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 500 chunk_store_config: chunk_cache_config: embedded_cache: enabled: true max_size_mb: 1000 4. Use TSDB index (v13 schema)\nThe TSDB index (schema v13) offers better performance than BoltDB:\nschema_config: configs: - from: 2024-01-01 store: tsdb # Use TSDB object_store: s3 schema: v13 # Latest schema index: prefix: index_ period: 24h 5. Configure appropriate chunk settings\ningester: chunk_idle_period: 30m # Flush idle chunks after 30 min chunk_block_size: 262144 # 256 KB blocks chunk_encoding: snappy # Compression algorithm chunk_retain_period: 15m # Retain flushed chunks in memory max_chunk_age: 1h # Max time before forced flush wal: enabled: true dir: /loki/wal Storage Considerations 1. Choose appropriate object storage\nDevelopment: Filesystem or MinIO Production AWS: S3 with lifecycle policies Production GCP: GCS with object versioning Production Azure: Azure Blob Storage On-premises: MinIO cluster or compatible S3 service 2. Configure object storage lifecycle\nReduce storage costs by transitioning older data to cheaper tiers:\n# AWS S3 lifecycle example - Id: TransitionOldChunks Status: Enabled Transitions: - Days: 30 StorageClass: STANDARD_IA - Days: 90 StorageClass: GLACIER 3. Separate index and chunk storage\nFor better performance, use different backends:\nschema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 # Chunks in S3 schema: v13 index: prefix: index_ period: 24h storage_config: tsdb_shipper: active_index_directory: /loki/index cache_location: /loki/index_cache shared_store: s3 # Index in S3 aws: s3: s3://us-east-1/loki-chunks bucketnames: loki-chunks 4. Monitor storage usage\nTrack storage metrics to plan capacity:\n# Chunk storage rate rate(loki_ingester_chunk_stored_bytes_total[5m]) # Index entries created rate(loki_ingester_index_entries_total[5m]) Performance Tuning 1. Optimize ingestion\nUse batching in log shippers (Promtail, Alloy) Enable compression for network transport Scale distributors horizontally for high write load Scale ingesters based on stream count and retention 2. Optimize queries\nUse specific label matchers to reduce streams searched Limit query time ranges Use query frontend for splitting and caching Add parallelism for large queries # Good: Specific label selector, limited time range {service=\"game-server\", environment=\"prod\"} |= \"error\" [5m] # Suboptimal: Broad selector, large time range {environment=\"prod\"} [24h] 3. Use bloom filters (experimental)\nBloom filters can speed up log line filtering:\nbloom_compactor: enabled: true bloom_gateway: enabled: true 4. Tune querier parallelism\nquerier: max_concurrent: 10 # Concurrent queries per querier query_timeout: 1m # Per-query timeout limits_config: max_query_parallelism: 16 # Parallel workers per query Common Pitfalls to Avoid 1. High-cardinality labels\nProblem: Using trace IDs, user IDs, or timestamps as labels creates millions of streams.\nSolution: Use structured metadata or filter expressions instead.\n2. Not monitoring stream count\nProblem: Stream count grows unbounded, degrading performance.\nSolution: Monitor loki_ingester_memory_streams and set alerts at thresholds.\n3. Insufficient ingester memory\nProblem: Ingesters crash or flush chunks too frequently.\nSolution: Allocate 8-16 GB RAM per ingester, adjust max_chunk_age and chunk_idle_period.\n4. No retention policy\nProblem: Storage costs grow unbounded.\nSolution: Configure retention_period and enable compactor.\n5. Querying too much data\nProblem: Queries time out or overload queriers.\nSolution: Use query frontend, limit time ranges, add specific label selectors.\n6. Single ingester (no replication)\nProblem: Data loss during ingester failure.\nSolution: Set replication_factor: 3 in production.\n7. Using filesystem storage in production\nProblem: Data loss, no scalability, no durability.\nSolution: Always use object storage (S3, GCS, MinIO) for production.\n8. Not using WAL\nProblem: In-memory data lost on ingester crash.\nSolution: Enable write-ahead log:\ningester: wal: enabled: true dir: /loki/wal When to Use Loki Ideal Use Cases 1. Cloud-native and Kubernetes environments\nLoki excels in containerized environments with:\nAutomatic label extraction from Kubernetes metadata Efficient handling of ephemeral infrastructure Native Prometheus integration for unified observability 2. Cost-sensitive deployments\nLoki’s index-free architecture dramatically reduces:\nStorage costs (5-10x cheaper than Elasticsearch) Memory requirements Operational overhead 3. Integration with existing Prometheus/Grafana stacks\nIf you already use Prometheus and Grafana:\nUnified visualization across logs, metrics, and traces Similar query language (LogQL ~ PromQL) Consistent operational model 4. High-volume log aggregation with simple queries\nLoki handles massive log volumes efficiently when:\nQueries primarily filter by labels and time ranges Full-text search is limited to known patterns Aggregation and metrics-from-logs are common use cases 5. Correlation between logs, metrics, and traces\nLoki enables:\nLog-trace correlation via TraceID/SpanID Metrics extraction from logs Unified observability workflows in Grafana 6. Multi-tenant logging platforms\nLoki’s built-in multi-tenancy supports:\nIsolated log streams per tenant Per-tenant rate limiting and retention Shared infrastructure with tenant isolation Anti-Patterns 1. Complex full-text search requirements\nLoki is not a replacement for Elasticsearch when you need:\nAdvanced full-text search across all log content Complex query DSL with scoring and relevance Frequent regex searches without label filtering Ad-hoc exploratory searches without known labels 2. Frequent high-cardinality queries\nAvoid Loki if you regularly need to:\nSearch by unique identifiers (user IDs, session IDs) as primary access pattern Query without label-based filtering Perform analytics on unbounded dimensions 3. Long-term analytics and data warehouse use cases\nLoki is optimized for recent data access, not:\nHistorical data mining over years of logs Complex joins between log datasets Business intelligence and reporting workflows 4. Transactional workloads\nLoki does not provide:\nACID guarantees Immediate consistency for queries Strong durability guarantees (eventual consistency model) Loki vs. Elasticsearch Comparison Aspect Loki Elasticsearch Indexing Labels only (metadata) Full-text indexing Storage cost Low (index-free) High (full indexes) Memory usage Low High Query performance Fast for label-based queries Fast for full-text search Setup complexity Low Medium-high Operational overhead Low High Search capabilities Label filtering + grep-style Advanced full-text, DSL Best for Cloud-native, Kubernetes, cost-sensitive Enterprise search, analytics Scalability Horizontal (microservices) Horizontal (cluster) Multi-tenancy Built-in Via indexes/namespaces Integration Grafana, Prometheus, Tempo Kibana, Elastic ecosystem Decision Factors Choose Loki when:\nCost efficiency is a priority You have well-defined label taxonomy Logs are primarily time-series access patterns You use Kubernetes and Prometheus Queries filter by known dimensions (service, environment) Integration with Grafana is important Choose Elasticsearch when:\nComplex full-text search is required Ad-hoc exploratory queries are common Advanced analytics and aggregations are needed You need enterprise search capabilities Budget allows for higher infrastructure costs Team has existing ELK expertise BattleBots Integration Points For the BattleBots platform, Loki would serve as the centralized log storage backend in the observability stack.\nHow Loki Fits in the Observability Stack graph TB A[Game Servers\u003cbr/\u003eGo Services] --\u003e|Logs| B[OTel Collector] C[Infrastructure\u003cbr/\u003eContainers] --\u003e|Logs| B D[Application\u003cbr/\u003eLibraries] --\u003e|Logs| B B --\u003e|OTLP/HTTP| E[Loki\u003cbr/\u003eDistributor] E --\u003e F[Loki\u003cbr/\u003eIngesters] F --\u003e G[Object Storage\u003cbr/\u003eS3/MinIO] H[Grafana] --\u003e|LogQL| I[Loki\u003cbr/\u003eQuery Frontend] I --\u003e J[Loki\u003cbr/\u003eQueriers] J --\u003e F J --\u003e G B --\u003e|Metrics| K[Prometheus/Mimir] B --\u003e|Traces| L[Tempo] H -.Unified View.-\u003e K H -.Unified View.-\u003e L style B fill:#fff4e6 style E fill:#e1f5ff style F fill:#e1f5ff style H fill:#e8f5e9 style K fill:#f3e5f5 style L fill:#fce4ec Game Event Logging Use Cases 1. Battle event timeline\nLog all significant battle events with consistent labels:\n{service=\"battle-server\", battle_id=\"battle_123\"} | json | line_format \"{{.timestamp}} [{{.event_type}}] {{.description}}\" Example logs:\n2025-12-03T10:00:00Z [battle_start] Battle battle_123 initialized 2025-12-03T10:00:05Z [bot_action] Bot bot_456 executed attack on bot_789 2025-12-03T10:00:06Z [damage_calc] Bot bot_789 took 15 damage 2025-12-03T10:00:10Z [victory] Bot bot_456 won battle battle_123 2. Game state transitions\nTrack game state changes with structured logging:\n{ \"timestamp\": \"2025-12-03T10:00:00Z\", \"service\": \"game-server\", \"level\": \"info\", \"message\": \"Game state transition\", \"game_id\": \"game_123\", \"previous_state\": \"waiting_for_players\", \"new_state\": \"in_progress\", \"player_count\": 4 } Query pattern:\n{service=\"game-server\"} | json | message=\"Game state transition\" | game_id=\"game_123\" 3. Error tracking and debugging\nCapture errors with correlation to traces:\n{ \"timestamp\": \"2025-12-03T10:00:15Z\", \"service\": \"matchmaker\", \"level\": \"error\", \"message\": \"Failed to assign player to match\", \"error\": \"queue timeout exceeded\", \"trace_id\": \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", \"span_id\": \"8b4g9d3e-5f6g-22fd-92e4-1353bd241114\", \"player_id\": \"player_456\", \"queue_wait_time_ms\": 30000 } Query errors for a trace:\n{service=\"matchmaker\"} | json | trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" | level=\"error\" Server Log Aggregation Patterns 1. Label strategy for BattleBots\n# Standard labels for all services { service=\"battle-server\", # Service name environment=\"production\", # Deployment environment namespace=\"battlebots\", # Kubernetes namespace region=\"us-east-1\", # Deployment region version=\"v1.2.3\" # Application version } # Container-level labels (auto-discovered) { container=\"battle-server\", pod=\"battle-server-abc123-xyz456\", node=\"node-01\" } 2. Aggregating logs from multiple sources\n# All battle-related services {namespace=\"battlebots\"} | json | level=\"error\" # Specific service across all environments {service=\"matchmaker\"} | json # All production services in a region {environment=\"production\", region=\"us-east-1\"} | json 3. Metrics extraction from logs\nGenerate metrics from log data:\n# Battle completion rate rate({service=\"battle-server\"} |= \"battle completed\" [5m]) # Error rate by service sum by (service) (rate({namespace=\"battlebots\"} | json | level=\"error\" [5m])) # Average match duration avg_over_time({service=\"matchmaker\"} | json | unwrap duration_ms [5m]) Integration with OTel Collector Loki integrates with the OpenTelemetry Collector via native OTLP endpoints or exporters.\nOTel Collector configuration (brief example):\nexporters: otlphttp/loki: endpoint: http://loki:3100/otlp service: pipelines: logs: receivers: [otlp] processors: [batch, resourcedetection] exporters: [otlphttp/loki] For comprehensive OTel Collector integration details, see the dedicated Loki OTLP Integration document.\nFurther Reading Official Documentation Grafana Loki Documentation - Official documentation home Loki Architecture - Detailed architecture overview Loki Components - Component reference Loki Deployment Modes - Deployment mode comparison LogQL Language - Query language reference Label Best Practices - Label design guidelines Cardinality Management - Cardinality considerations Configuration Reference - Full configuration documentation Installation and Setup Install Loki with Docker - Docker and Docker Compose setup Quick Start Guide - Getting started tutorial Helm Installation - Kubernetes Helm charts Configure Storage - Storage backend configuration Best Practices and Guides How Labels Work in Loki - Label design deep dive The Concise Guide to Grafana Loki Labels - Comprehensive label guide Loki 2.4 Simple Scalable Deployment - Simple scalable mode introduction Grafana Loki Architecture Guide - Architecture deep dive Storage and Integration Using MinIO with Loki - MinIO integration guide Loki and MinIO Configuration - MinIO setup tutorial Storage Configuration - Storage backend options Comparisons and Decision Making Loki vs Elasticsearch - Detailed comparison Grafana Loki vs ELK Stack - Use case comparison Loki vs ELK: A Light Alternative - Lightweight alternative perspective Community and Source Code Loki GitHub Repository - Source code and issues Loki Examples - Configuration examples Grafana Community Forums - Community discussions Related BattleBots Documentation OpenTelemetry Collector Overview - OTel Collector architecture OTel Collector Logs - Log handling in OTel Collector Loki OTLP Integration - Detailed OTLP integration guide User Journey 0001: POC - Observability requirements ","categories":"","description":"Comprehensive overview of Grafana Loki log aggregation system, covering architecture, deployment modes, and operational best practices.\n","excerpt":"Comprehensive overview of Grafana Loki log aggregation system, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/loki-overview/","tags":"","title":"Loki: Overview"},{"body":"Overview This section contains research and analysis of observability solutions for the BattleBots platform. Observability is critical for:\nMonitoring real-time battle events and game state Tracking bot performance and system health Debugging issues in distributed game architecture Analyzing player behavior and system usage patterns Ensuring reliable service operation Components OpenTelemetry Collector Analysis of the OpenTelemetry Collector, a vendor-agnostic telemetry data pipeline that can receive, process, and export logs, metrics, and traces to multiple backends.\nThe OpenTelemetry Collector serves as a centralized telemetry hub, providing:\nVendor neutrality for any observability backend Protocol translation between Prometheus, Jaeger, Zipkin, and OTLP Unified collection pipeline for logs, metrics, and traces Flexible deployment in agent, gateway, or hybrid modes Signal correlation linking traces, metrics, and logs Includes detailed analysis of:\nArchitecture and core components Logs, metrics, and traces support Self-monitoring and operational considerations BattleBots platform integration patterns Log Storage Analysis of log storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nLog storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Grafana Loki Research on Grafana Loki, a horizontally scalable, multi-tenant log aggregation system designed for cost-effective log storage and querying.\nLoki uses an index-free approach that indexes only metadata labels rather than full log content, providing:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and operational best practices OTLP compatibility and OTel Collector integration Label strategy and performance considerations Future ADR Dependencies This analysis will inform:\nADR-NNNN: Observability Stack Selection - Which backends to use (Loki, Prometheus, Jaeger, etc.) ADR-NNNN: Telemetry Collection Strategy - Agent vs. gateway deployment, sampling policies ADR-NNNN: Telemetry Data Retention - Storage duration and cost management Related Documentation R\u0026D Documentation User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of observability solutions for the BattleBots platform.\n","excerpt":"Research and analysis of observability solutions for the BattleBots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/","tags":"","title":"Observability Analysis"},{"body":"Overview The OpenTelemetry Collector serves as a centralized telemetry hub, removing the need to run multiple agents or collectors for different formats and backends. It provides:\nVendor neutrality: Works with any observability backend Protocol translation: Converts between Prometheus, Jaeger, Zipkin, and OTLP formats Unified collection: Single pipeline for logs, metrics, and traces Flexible deployment: Agent mode, gateway mode, or hybrid Signal correlation: Links traces, metrics, and logs through shared context Document Structure The analysis is organized into the following documents:\nOpenTelemetry Collector Overview High-level architectural overview covering:\nCore components (receivers, processors, exporters, extensions) Pipeline-based architecture and data flow Deployment patterns (agent, gateway, hybrid) Configuration fundamentals When to use the Collector vs. direct exports Audience: Everyone—provides foundational understanding for all subsequent documents.\nLogs Support Deep dive into log data handling:\nOTLP logs data model and structure Log receivers (filelog, syslog, OTLP) Log processors (attributes, filter, transform) Log exporters (Loki, Elasticsearch, OTLP) Log correlation with traces and metrics Configuration patterns for log collection Audience: Developers implementing log collection, operations teams configuring log pipelines.\nMetrics Support Deep dive into metrics data handling:\nOpenTelemetry metrics data model Metric types (counters, gauges, histograms, summaries) Temporality (delta vs. cumulative) Metrics receivers (Prometheus, hostmetrics, OTLP) Metrics processors and exporters Performance and cardinality considerations Audience: Developers instrumenting applications, SREs monitoring infrastructure.\nTraces Support Deep dive into distributed tracing:\nTrace and span data model Context propagation mechanisms Trace receivers (OTLP, Jaeger, Zipkin) Sampling strategies (head vs. tail sampling) Trace processors and exporters Multi-backend routing Audience: Developers implementing distributed tracing, architects designing observability strategy.\nSelf-Monitoring How to observe the Collector itself:\nInternal metrics and telemetry Extensions (health_check, zpages, pprof) Debugging and troubleshooting techniques Performance monitoring and optimization Production monitoring best practices Audience: Operations teams, SREs responsible for Collector reliability.\nBattleBots Platform Context For the BattleBots platform, the OpenTelemetry Collector would support:\nGame Event Observability Logs: Battle events, bot actions, game state transitions, error conditions Metrics: Match duration, action rates, player counts, system resource usage Traces: Request flows from player action to state update to broadcast Infrastructure Monitoring Host metrics: Server CPU, memory, disk, network utilization Application metrics: Go runtime metrics, HTTP latency, WebSocket connections Container metrics: Resource limits, restart counts, health status Cross-Signal Correlation The Collector enables powerful debugging workflows:\nAlert fires on high error rate (metrics) Drill down to traces showing failing requests View logs associated with failing trace spans Identify root cause with full context This unified observability is particularly valuable during live battles when quick diagnosis is essential.\nImplementation Considerations Deployment Architecture For BattleBots, a recommended deployment would include:\nAgent Mode:\nCollectors running alongside each game server Local log file collection with filelog receiver Host metrics collection for server monitoring OTLP receiver for application telemetry Gateway Mode:\nCentralized collectors receiving data from agents Tail sampling for intelligent trace retention Multi-backend routing (analytics, debugging, long-term storage) Buffering and retry for backend resilience Signal-Specific Patterns Logs:\nCollect structured JSON logs from game servers Parse and enrich with resource attributes Filter debug logs in production Route to Loki or Elasticsearch Metrics:\nScrape Prometheus metrics from Go services Collect host metrics from servers Aggregate and downsample for cost efficiency Export to Prometheus or cloud backends Traces:\nInstrument Go services with OpenTelemetry SDK Use head sampling for baseline reduction (10%) Apply tail sampling to always capture errors Export to Jaeger or Grafana Tempo External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of the OpenTelemetry Collector for logs, metrics, and traces collection and processing.\n","excerpt":"Research and analysis of the OpenTelemetry Collector for logs, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/","tags":"","title":"OpenTelemetry Collector"},{"body":"Overview The OpenTelemetry Collector is a vendor-agnostic application that receives, processes, and exports telemetry data (traces, metrics, and logs). It serves as a centralized component in observability architectures, removing the need to run multiple agents or collectors for different telemetry formats and backends.\nThe Collector supports open-source observability data formats including Jaeger, Prometheus, Fluent Bit, and others, while providing a unified approach to telemetry handling. It enables services to offload telemetry data quickly while the Collector handles retries, batching, encryption, and sensitive data filtering.\nKey benefits include vendor independence, reduced operational complexity, and the ability to route telemetry data to multiple backends simultaneously without modifying application code.\nKey Concepts The Collector is built around five guiding principles:\nUsability: Provides functional defaults with support for popular protocols out-of-the-box Performance: Maintains stability under varying loads with predictable resource usage Observability: Designed as an observable service itself, exposing its own metrics and health status Extensibility: Allows customization through plugins without requiring modifications to core code Unification: Single codebase supporting all three telemetry signals (traces, metrics, logs) Core Architecture The Collector uses a pipeline-based architecture where data flows through three primary component types, orchestrated by a configuration file.\ngraph LR A[Telemetry Sources] --\u003e B[Receivers] B --\u003e C[Processors] C --\u003e D[Exporters] D --\u003e E[Observability Backends] F[Extensions] -.Optional.-\u003e B F -.Optional.-\u003e C F -.Optional.-\u003e D style A fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style C fill:#f3e5f5 style D fill:#e8f5e9 style F fill:#fce4ec Data Flow Telemetry data flows unidirectionally through the Collector:\nReceivers accept incoming telemetry in various formats (OTLP, Jaeger, Prometheus) Processors transform, filter, or enrich the data in a sequential chain Exporters send processed data to one or more backend destinations This pipeline architecture allows the same data to be simultaneously:\nSampled differently for different backends Enriched with environment-specific attributes Routed to multiple observability platforms Components Receivers Receivers gather telemetry data through two mechanisms:\nPush-based: Listen on network endpoints for incoming data Pull-based: Actively scrape metrics from instrumented services Multiple receivers can feed into a single pipeline, with their outputs merged before reaching processors. Common receivers include:\notlp - OpenTelemetry Protocol (gRPC/HTTP) prometheus - Prometheus metrics scraping jaeger - Jaeger trace data filelog - Log file collection For comprehensive receiver documentation, see the OpenTelemetry Collector Receivers.\nProcessors Processors form a sequential chain that transforms data between receivers and exporters. Each processor in the chain operates on the data before passing it to the next processor.\nCommon operations include:\nAdding or removing attributes Sampling (probabilistic, tail-based) Batching for efficient transmission Filtering unwanted telemetry Resource detection (cloud provider, Kubernetes metadata) Important: Each pipeline maintains independent processor instances, even when referencing the same configuration name across multiple pipelines. This prevents unintended state sharing between pipelines.\nFor comprehensive processor documentation, see the OpenTelemetry Collector Processors.\nExporters Exporters forward processed data to external destinations, typically by sending to network endpoints or writing to logging systems. Multiple exporters can receive identical data copies through a fan-out mechanism, enabling simultaneous transmission to different backends.\nCommon exporters include:\notlp - OpenTelemetry Protocol destinations prometheus - Prometheus remote write jaeger - Jaeger backend logging - Standard output for debugging file - Local file storage For comprehensive exporter documentation, see the OpenTelemetry Collector Exporters.\nConnectors Connectors enable inter-pipeline communication and data flow between different telemetry signal types. A connector acts as both an exporter (for one pipeline) and a receiver (for another pipeline).\nUse cases include:\nGenerating metrics from trace spans Creating logs from metric anomalies Correlating signals for enhanced observability For more information, see the OpenTelemetry Collector Connectors.\nExtensions Extensions provide auxiliary functionality that doesn’t directly process telemetry data but supports Collector operations:\nhealth_check - HTTP endpoint for health monitoring pprof - Go profiling endpoint for performance analysis zpages - In-process debugging pages ballast - Memory ballast for GC optimization Extensions are optional but highly recommended for production deployments. See self-monitoring documentation for more details.\nConfiguration The Collector uses YAML configuration files (default: /etc/otelcol/config.yaml) with four main sections:\nreceivers: # Define how to collect telemetry processors: # Define how to transform telemetry exporters: # Define where to send telemetry service: pipelines: # Connect receivers → processors → exporters Pipeline Types Three pipeline types handle different telemetry signals:\ntraces: Distributed tracing data metrics: Time-series measurements logs: Event records Each pipeline independently connects receivers to exporters through an optional processor chain. The same receiver can feed multiple pipelines simultaneously, though this creates a potential bottleneck if one processor blocks.\nConfiguration Best Practices Validate configurations before deployment: otelcol validate --config=file.yaml Use environment variables for sensitive values: ${env:API_KEY} Bind endpoints to localhost for local-only access Apply TLS certificates for production environments Use the type/name syntax for multiple instances of the same component type For detailed configuration guidance, see the Configuration Documentation.\nDeployment Patterns The Collector supports multiple deployment models based on operational requirements and scale.\nAgent Mode In agent mode, lightweight Collector instances run as daemons alongside applications (on the same host, container, or Kubernetes pod). This pattern:\nCollects telemetry from co-located services Performs local sampling and aggregation Reduces network overhead by local processing Simplifies application configuration (default OTLP exporters assume localhost:4317) Use when: You need local collection with per-host resource limits and want to offload telemetry handling from application processes.\nGateway Mode In gateway mode, centralized Collector instances receive data from distributed agents and application libraries, then route to backend systems. This pattern:\nProvides centralized control over data transformation Enables consistent sampling decisions across services Supports complex routing logic to multiple backends Allows for sensitive data filtering before egress Use when: You need centralized policy enforcement, multi-backend routing, or want to isolate backend credentials from application environments.\nHybrid Mode Many production deployments use both agent and gateway patterns:\nAgents perform local collection and basic processing Gateways handle aggregation, complex transformations, and backend routing This hybrid approach balances local efficiency with centralized control.\nNo Collector For development environments or initial experimentation, services can export directly to backends using OTLP. However, this approach loses the benefits of buffering, retries, and transformation capabilities.\nWhen to Use the Collector Recommended Use Cases Deploy a Collector when you need to:\nSupport multiple telemetry formats (Jaeger, Prometheus, custom formats) Route telemetry to multiple backends simultaneously Perform data transformation or enrichment before export Sample or filter telemetry based on configurable rules Isolate backend credentials from application deployments Buffer telemetry during backend outages Offload retry and batching logic from applications Direct Export Scenarios Direct service-to-backend export may be sufficient for:\nDevelopment and testing environments Single-backend deployments with no transformation needs Prototyping and proof-of-concept work Very small-scale deployments Integration Points BattleBots Observability Requirements For the BattleBots platform, the Collector would support:\nBattle Event Tracking: Collecting logs of bot actions and game state changes Performance Metrics: Gathering metrics on bot response times and system resource usage Distributed Tracing: Tracking request flows across client/server or P2P architectures See User Journey 0001: POC for observability requirements context.\nMulti-Signal Correlation The Collector enables correlation between:\nTrace spans and related logs (via trace context) Metrics and traces (via exemplars) Resource attributes across all signals This correlation is particularly valuable for debugging battle scenarios where you need to understand both the timeline of events (traces), the quantitative measurements (metrics), and the detailed context (logs).\nFurther Reading Official Documentation OpenTelemetry Collector Main Documentation Collector Architecture Configuration Reference Component Registry Specifications OTLP Specification OpenTelemetry Specification Implementation Resources Collector GitHub Repository Collector Contrib Repository (additional components) Related Analysis Documents Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"High-level architectural overview of the OpenTelemetry Collector, covering core components, deployment patterns, and use cases.\n","excerpt":"High-level architectural overview of the OpenTelemetry Collector, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/opentelemetry-collector-overview/","tags":"","title":"OpenTelemetry Collector Overview"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting log data from various sources to multiple backends. Unlike traditional logging agents that focus on specific formats or destinations, the Collector treats logs as first-class observability signals alongside metrics and traces.\nThe Collector’s log support enables correlation between logs and traces through shared execution context (TraceId and SpanId), allowing unified observability across all three signal types. This correlation is particularly valuable for debugging complex distributed systems where understanding both the quantitative measurements and the detailed event context is essential.\nKey capabilities include parsing structured and unstructured log formats, enriching logs with resource attributes, filtering and transforming log content, and routing logs to multiple backends simultaneously.\nKey Concepts OTLP Logs Data Model OpenTelemetry defines a standardized log data model to establish a common understanding of what a LogRecord is and what data needs to be recorded, transferred, stored, and interpreted by logging systems.\nLogRecord Structure:\nA LogRecord contains several key components:\nTimestamp: The moment in time when the event occurred TraceId and SpanId: Execution context identifiers enabling correlation between logs and traces Resource: Describes the origin of the log (host name, container name, pod name, etc.) Instrumentation Scope: Identifies the library or component that generated the log Severity: Indicates the importance or criticality of the log entry Body: The actual log message content (string, structured data, or binary) Attributes: Structured key-value pairs providing additional context This standardized model allows logs from different sources to be processed uniformly while preserving correlation capabilities.\nLog Correlation The logs data model enables correlation across three dimensions:\nTemporal correlation: Based on timestamp alignment Execution context correlation: Using TraceId and SpanId to link logs to specific trace spans Origin correlation: Through Resource context describing the source infrastructure and application This unified approach allows observability backends to perform exact and unambiguous correlation between logs, metrics, and traces.\nLog Receivers Receivers collect log data from various sources and convert it into the OpenTelemetry logs data model.\nFilelog Receiver The filelog receiver tails and parses logs from files, making it ideal for collecting logs from applications that write to local disk.\nKey Capabilities:\nFile monitoring: Tracks multiple log files using glob patterns for inclusion and exclusion Automatic rotation handling: Detects and follows rotated log files and symlinks Compression support: Reads gzip-compressed files with auto-detection Multiline support: Combines log entries spanning multiple lines using custom patterns Format parsing: Built-in parsers for JSON, regex patterns, and structured text Metadata extraction: Parses timestamps, severity levels, and custom fields Persistent offsets: Maintains file positions across collector restarts Example use cases:\nCollecting application logs written to /var/log/ Parsing container logs from Kubernetes Reading structured JSON logs from microservices Configuration note: The filelog receiver uses a pipeline of operators that transform raw file content into structured LogRecords. Each operator performs a specific transformation (parsing, extraction, modification) before passing data to the next operator.\nOTLP Receiver The OTLP receiver accepts log data transmitted using the OpenTelemetry Protocol.\nSupported transports:\ngRPC (default port 4317): Uses unary requests with ExportLogsServiceRequest messages HTTP (default port 4318): POST requests to /v1/logs endpoint Encoding formats:\nBinary Protobuf (application/x-protobuf) JSON Protobuf (proto3 JSON mapping) Use when: Collecting logs directly from applications instrumented with OpenTelemetry SDKs or from upstream OpenTelemetry Collectors in a multi-tier deployment.\nSyslog Receiver The syslog receiver listens for syslog messages over TCP or UDP, supporting RFC 3164 and RFC 5424 formats.\nUse cases:\nCollecting system logs from Linux/Unix hosts Receiving logs from network devices (routers, switches, firewalls) Integrating with legacy applications that use syslog Other Log Receivers The OpenTelemetry Collector ecosystem includes receivers for:\njournald: Reads logs from systemd journal tcplog/udplog: Generic TCP/UDP log receivers windowseventlog: Collects Windows Event Logs kafka: Consumes logs from Kafka topics For a comprehensive list, see the Receiver Components documentation.\nLog Processors Processors transform, filter, and enrich log data as it flows through the pipeline.\nAttributes Processor The attributes processor modifies attributes of log records.\nCapabilities:\nInsert new attributes Update existing attributes Delete attributes Hash attribute values for privacy Extract values from one attribute to another Common use cases:\nAdding environment labels (e.g., environment=production) Removing sensitive data from log attributes Normalizing attribute names across different sources For more details, see the Mastering the OpenTelemetry Attributes Processor guide.\nFilter Processor The filter processor drops log records that match specified conditions using the OpenTelemetry Transformation Language (OTTL).\nCapabilities:\nFilter by log severity level Drop logs matching specific patterns Exclude logs from certain resources Reduce log volume by dropping debug logs in production Example scenarios:\nDropping health check logs to reduce noise Filtering out logs below a certain severity threshold Excluding logs from specific namespaces or services See also the Filter Processor for OpenTelemetry Collector documentation.\nTransform Processor The transform processor modifies log records using OTTL statements.\nCapabilities:\nParse log body content into structured attributes Modify log severity based on content Extract values using regex or JSON path Compute new attributes from existing ones Normalize timestamps Use cases:\nConverting unstructured log messages to structured attributes Extracting user IDs or request IDs from log text Standardizing log formats from multiple sources For transformation guidance, see Transforming telemetry.\nResource Detection Processor The resource detection processor enriches logs with metadata about their execution environment:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information (Docker, containerd) Host information (hostname, OS, architecture) This automatic enrichment enables filtering and grouping logs by infrastructure context without manual configuration.\nBatch Processor The batch processor groups log records before sending to exporters, improving throughput and reducing network overhead.\nConfiguration considerations:\nTimeout: Maximum time to wait before sending a batch Batch size: Number of log records per batch Memory limits: Prevents excessive memory usage Batching is recommended for production deployments to optimize resource usage.\nLog Exporters Exporters send processed log data to observability backends and storage systems.\nOTLP HTTP Exporter The OTLP HTTP exporter is the recommended exporter for modern observability backends that support native OTLP ingestion.\nSupported destinations:\nGrafana Loki (v3+): Send logs to Loki’s native OTLP endpoint at http://loki:3100/otlp Elasticsearch: Use OTLP-compatible endpoints Commercial backends: Datadog, New Relic, Honeycomb, Dynatrace Important: The Loki-specific exporter is deprecated. Use the standard otlphttp/logs exporter for Loki v3+ which supports native OTLP ingestion.\nFor setup guidance, see Getting started with the OpenTelemetry Collector and Loki tutorial.\nElasticsearch Exporter The Elasticsearch exporter sends logs, metrics, traces, and profiles directly to Elasticsearch.\nSupported versions:\nElasticsearch 7.17.x Elasticsearch 8.x Elasticsearch 9.x Features:\nIndex routing based on log attributes Dynamic index naming with time-based patterns Bulk API for efficient ingestion File Exporter The file exporter writes logs to local files, useful for:\nDebugging collector pipelines Creating log archives Forwarding to systems that process files Note: Not recommended for production logging backends—use OTLP or dedicated exporters instead.\nLogging Exporter The logging (debug) exporter writes logs to the collector’s standard output. Use this for:\nDevelopment and testing Troubleshooting pipeline configuration Verifying data transformation For additional exporters, see the Exporter Components documentation.\nLog Pipeline Flow A typical log pipeline in the Collector follows this pattern:\ngraph LR A[Log Files] --\u003e B[Filelog Receiver] C[OTLP SDK] --\u003e D[OTLP Receiver] B --\u003e E[Resource Detection] D --\u003e E E --\u003e F[Transform Processor] F --\u003e G[Filter Processor] G --\u003e H[Attributes Processor] H --\u003e I[Batch Processor] I --\u003e J[OTLP HTTP Exporter] J --\u003e K[Loki] I --\u003e L[Elasticsearch Exporter] L --\u003e M[Elasticsearch] style A fill:#e1f5ff style C fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#e8f5e9 style L fill:#e8f5e9 style K fill:#e1f5ff style M fill:#e1f5ff Configuration Considerations Multiline Log Handling Many applications emit logs spanning multiple lines (e.g., stack traces, JSON objects). The filelog receiver supports multiline patterns to combine these entries:\nreceivers: filelog: include: [/var/log/app/*.log] multiline: line_start_pattern: '^\\d{4}-\\d{2}-\\d{2}' Parsing Structured Logs For JSON-formatted logs, configure the filelog receiver with JSON parsing:\nreceivers: filelog: include: [/var/log/app/*.log] operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' Log Volume Management High log volumes can overwhelm collectors and backends. Strategies include:\nSampling: Use the probabilistic sampler processor to keep a percentage of logs Filtering: Drop debug/trace logs in production using the filter processor Batching: Configure appropriate batch sizes to balance latency and throughput Tail sampling: Keep only logs associated with interesting traces Performance Tuning For high-throughput log collection:\nIncrease the number of concurrent file readers in filelog receiver Tune batch processor settings (size, timeout) Use multiple collector instances with load balancing Consider gateway deployment to centralize processing Integration Points BattleBots Log Collection For the BattleBots platform, log collection would capture:\nGame events: Bot actions, state transitions, match outcomes System logs: Server startup, configuration changes, errors Client logs: User actions, connection events, performance issues The filelog receiver can parse structured JSON logs from the game server while the OTLP receiver collects logs directly from instrumented Go services.\nLog-Trace Correlation When both logs and traces are collected, correlation enables:\nFinding all logs for a specific trace (query by TraceId) Jumping from a log entry to its parent trace span Identifying logs that occurred during slow requests This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically.\nCross-Signal Analysis Logs complement metrics and traces:\nMetrics show aggregate patterns (error rate spike) Traces show request flow (which service failed) Logs show detailed context (exception message, variable values) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Logs Specification OTLP Specification Receiver Components Processor Components Exporter Components Transforming Telemetry Component-Specific Resources Filelog Receiver Attributes Processor Guide Filter Processor Guide Transform Processor Elasticsearch Exporter Integration Guides Ingesting logs to Loki using OpenTelemetry Collector Getting started with the OpenTelemetry Collector and Loki tutorial Honeycomb Filter Processor Documentation Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles log data, including receivers, processors, exporters, and the OTLP logs data model.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles log data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-logs/","tags":"","title":"OpenTelemetry Collector: Logs Support"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting metrics data from diverse sources to multiple backends. It serves as a bridge between different metrics ecosystems, enabling seamless integration of Prometheus metrics, host system metrics, and custom application metrics within a unified observability platform.\nThe Collector’s metrics support emphasizes signal correlation—connecting metrics to traces through exemplars and enriching attributes via Baggage and Context. This enables powerful observability patterns such as jumping from a metric anomaly to related traces or finding metrics that explain slow trace spans.\nKey capabilities include scraping Prometheus endpoints, collecting host system metrics, transforming metric formats, aggregating data points, and routing metrics to multiple backends simultaneously while handling different temporality preferences.\nKey Concepts OpenTelemetry Metrics Data Model The OpenTelemetry Metrics data model defines how metrics are represented and processed. The data model serves to:\nCapture raw measurements efficiently and simultaneously Decouple instrumentation from the SDK implementation Enable correlation with traces and logs Support migration from OpenCensus and Prometheus Architecture layers:\nMeterProvider \u0026 Instruments: Applications collect measurements through Meters and their associated Instruments In-Memory Aggregation: Measurements aggregate into an intermediate representation MetricReader: Processes aggregated metrics for export to backends Metric Types OpenTelemetry supports four primary metric types, each suited for different measurement scenarios.\nCounter (Sum) Counters represent cumulative or delta measurements that can only increase over time (or be reset to zero). Common examples include:\nRequest count Error count Bytes transmitted Items processed Characteristics:\nMonotonically increasing Supports both delta and cumulative temporality Can be aggregated across instances Typically visualized as rate-of-change For detailed specifications, see OTLP Metrics Types.\nGauge Gauges represent sampled values that can arbitrarily increase or decrease over time. Unlike counters, gauges are not cumulative—they reflect the current value at the time of measurement.\nCommon examples include:\nCPU usage percentage Memory utilization Queue depth Active connection count Temperature readings Characteristics:\nNon-monotonic (can increase or decrease) No aggregation temporality (uses “last sample value”) Represents point-in-time state Cannot be meaningfully aggregated across instances without additional context Histogram Histograms convey a population of recorded measurements in a compressed format by grouping measurements into configurable buckets. This enables statistical analysis without storing individual data points.\nCommon examples include:\nRequest latency distribution Response size distribution Query execution time Message size distribution Characteristics:\nProvides count, sum, and bucket distributions Supports both delta and cumulative temporality Enables percentile calculations (p50, p95, p99) More efficient than storing individual measurements Histograms are particularly valuable for understanding the distribution of latency or size measurements, revealing whether most requests are fast with occasional slow outliers, or if performance degrades uniformly.\nSummary Summaries provide pre-calculated quantile values (percentiles) over a time window. Unlike histograms, which send bucket distributions for backend calculation, summaries compute quantiles client-side.\nImportant: Summary points cannot always be merged meaningfully. This point type is not recommended for new applications and exists primarily for compatibility with other formats like Prometheus summaries.\nFor comprehensive metric type details, see OpenTelemetry Metrics.\nTemporality Temporality defines how metric values are accumulated and reported over time. OpenTelemetry supports two aggregation temporality modes:\nDelta Temporality Delta temporality reports the change since the last collection period. Each data point represents only new measurements since the previous export.\nCharacteristics:\nNon-overlapping time windows Measures rate of change Preferred by some backends (StatsD, Carbon) Requires stateless aggregation Example: A request counter shows +100 requests in period 1, then +150 requests in period 2.\nCumulative Temporality Cumulative temporality reports the total value since process start (or a fixed start point). Each data point includes all measurements from the beginning.\nCharacteristics:\nOverlapping time windows from fixed start Accumulates over application lifetime Preferred by Prometheus Resilient to collection gaps Example: A request counter shows 100 total requests in period 1, then 250 total requests in period 2.\nImportant note: Set the temporality preference to DELTA when possible, as setting it to CUMULATIVE may discard some data points during application or collector startup. However, Prometheus backends require cumulative temporality.\nFor more details, see OpenTelemetry Metrics Aggregation.\nMetrics Receivers Receivers collect metrics data from various sources and convert it into the OpenTelemetry metrics data model.\nPrometheus Receiver The Prometheus receiver enables the OpenTelemetry Collector to act as a Prometheus server by scraping Prometheus-compatible endpoints, then converting the metrics into OTLP format.\nKey capabilities:\nScrapes any Prometheus /metrics endpoint Supports service discovery mechanisms Converts Prometheus metrics to OpenTelemetry format Handles metric relabeling and filtering Scales with Target Allocator for large deployments Configuration example:\nreceivers: prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 15s static_configs: - targets: ['localhost:8888'] Scaling with Target Allocator:\nThe Target Allocator decouples service discovery and metric collection, allowing independent scaling. Each Collector pod registers with the Target Allocator, which uses consistent hashing to distribute discovered targets evenly among active Collectors, ensuring each target is scraped exactly once without overlap.\nFor comprehensive guidance, see Prometheus and OpenTelemetry Collector Integration.\nHost Metrics Receiver The hostmetrics receiver collects comprehensive system-level metrics from host machines, providing visibility into infrastructure health.\nAvailable scrapers:\ncpu: CPU utilization, time, and frequency disk: Disk I/O operations and throughput filesystem: Filesystem usage and available space memory: Memory utilization and swap usage network: Network interface statistics and errors load: System load averages paging: Paging and swapping activity processes: Process count and resource usage Configuration example:\nreceivers: hostmetrics: collection_interval: 30s scrapers: cpu: disk: filesystem: memory: network: The hostmetrics receiver is essential for infrastructure monitoring and provides context for application-level metrics. When deployed in Kubernetes, appropriate volumes and volumeMounts are automatically configured when the hostMetrics preset is enabled.\nOTLP Receiver The OTLP receiver accepts metrics data transmitted using the OpenTelemetry Protocol from instrumented applications or upstream collectors.\nSupported transports:\ngRPC (default port 4317) HTTP (default port 4318, endpoint /v1/metrics) Use cases:\nCollecting metrics directly from OpenTelemetry SDKs Multi-tier collector deployments (agent → gateway) Receiving metrics from serverless functions Other Metrics Receivers The ecosystem includes receivers for diverse metrics sources:\nstatsd: Receives StatsD protocol metrics kafka: Consumes metrics from Kafka topics influxdb: Receives InfluxDB line protocol carbon: Receives Graphite carbon metrics collectd: Receives collectd metrics postgresql: Scrapes PostgreSQL metrics redis: Scrapes Redis metrics mongodb: Scrapes MongoDB metrics For a complete list, see the Receiver Components documentation.\nMetrics Processors Processors transform and enrich metrics data as it flows through pipelines.\nMetrics Transform Processor The metrics transform processor modifies metric names, types, and attributes using transformation rules.\nCommon operations:\nRename metrics for consistency Change metric types (e.g., gauge to counter) Add or modify resource attributes Aggregate metrics across dimensions Filter Processor The filter processor drops metrics matching specified conditions, reducing data volume and costs.\nUse cases:\nDropping debug metrics in production Filtering metrics from test environments Excluding high-cardinality metrics Removing specific metric names or attribute values Cumulative to Delta Processor This processor converts cumulative temporality metrics to delta temporality, useful when backends prefer delta metrics.\nAttributes Processor Adds, updates, or deletes metric attributes and resource attributes, enabling:\nEnvironment labeling (environment=production) Team ownership tags (team=platform) Cost allocation labels Normalization across different metric sources Batch Processor Groups metrics before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nConfiguration considerations:\nBatch size: Number of metric data points per batch Timeout: Maximum wait before sending partial batch Memory limits: Prevents unbounded memory growth Metrics Exporters Exporters send processed metrics to observability backends and time-series databases.\nOTLP Exporter The OTLP exporter sends metrics using the OpenTelemetry Protocol to OTLP-compatible backends.\nSupported destinations:\nOpenTelemetry-native backends Cloud vendor endpoints (AWS CloudWatch, Google Cloud Monitoring, Azure Monitor) Commercial observability platforms (Datadog, New Relic, Honeycomb) Important: OTLP is now the recommended protocol for sending metrics to modern backends. For example, Prometheus can now accept OTLP directly:\nexport OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://localhost:9090/api/v1/otlp/v1/metrics Prometheus Remote Write Exporter The Prometheus Remote Write exporter sends OpenTelemetry metrics to Prometheus remote write compatible backends such as:\nCortex Grafana Mimir Thanos Amazon Managed Service for Prometheus Google Cloud Managed Prometheus Capabilities:\nTLS support (required by default) Queued retry mechanisms Authentication options (basic auth, bearer token, OAuth2) Important limitation: Non-cumulative monotonic, histogram summary, and exponential histogram OTLP metrics are dropped by this exporter.\nFor Grafana Mimir specifically, it’s recommended to use OTLP rather than Prometheus remote write.\nPrometheus Exporter The Prometheus exporter exposes metrics in Prometheus format on an HTTP endpoint for Prometheus servers to scrape.\nUse cases:\nExisting Prometheus deployments Push-based collection converted to pull-based Multi-backend export (push to one, expose for scraping by another) File Exporter Writes metrics to local files for debugging, archival, or processing by batch systems.\nFor additional exporters, see the Exporter Components documentation.\nMetrics Pipeline Flow A typical metrics pipeline demonstrates collection, processing, and export to multiple backends:\ngraph LR A[Prometheus Endpoints] --\u003e B[Prometheus Receiver] C[Host System] --\u003e D[Host Metrics Receiver] E[OTLP SDK] --\u003e F[OTLP Receiver] B --\u003e G[Attributes Processor] D --\u003e G F --\u003e G G --\u003e H[Filter Processor] H --\u003e I[Transform Processor] I --\u003e J[Batch Processor] J --\u003e K[OTLP Exporter] K --\u003e L[Prometheus Server] J --\u003e M[Prometheus Remote Write] M --\u003e N[Grafana Mimir] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#e8f5e9 style M fill:#e8f5e9 style L fill:#e1f5ff style N fill:#e1f5ff Configuration Considerations Temporality Management Different backends have different temporality preferences:\nPrometheus: Requires cumulative temporality StatsD-like systems: Prefer delta temporality Cloud vendors: Often accept both Configure the Collector to convert between temporalities based on backend requirements using the cumulative-to-delta processor.\nCardinality Control High cardinality metrics (many unique label combinations) can overwhelm backends and increase costs. Strategies include:\nFiltering high-cardinality dimensions Aggregating metrics before export Dropping rarely-used labels Using metric relabeling to reduce dimensions Scrape Interval Tuning Balance data freshness with resource consumption:\nShort intervals (5-15s): Real-time monitoring, higher costs Medium intervals (30-60s): Standard monitoring Long intervals (5m+): Capacity planning, cost optimization Exemplar Support Exemplars link metrics to traces by attaching trace IDs to specific metric data points. This enables:\nJumping from a high-latency histogram bucket to example slow traces Finding traces that contributed to an error rate spike Correlating metrics anomalies with detailed trace analysis Enable exemplar support in the Prometheus receiver and ensure trace context propagation in applications.\nIntegration Points BattleBots Metrics Collection For the BattleBots platform, metrics collection would capture:\nGame Metrics:\nMatch duration and outcome distribution Bot action rates (attacks, defenses, moves) Game state transition frequency Performance Metrics:\nRequest latency percentiles WebSocket connection counts Message throughput rates Server CPU and memory usage Business Metrics:\nActive user count Matches per hour Bot creation rate The combination of Prometheus receiver (for Go runtime metrics), hostmetrics receiver (for infrastructure), and OTLP receiver (for custom metrics) provides comprehensive visibility.\nMetrics-Trace Correlation Connecting metrics and traces enables powerful workflows:\nAlerting on metrics: High error rate triggers investigation Drill-down to traces: Click exemplar to see example failing requests Root cause analysis: Examine detailed trace spans to identify cause Fix validation: Monitor metrics to confirm fix effectiveness This requires:\nApplications emit both metrics and traces Exemplars enabled in metric collection Unified storage backend (or cross-backend linking) Cross-Signal Analysis Metrics complement logs and traces:\nMetrics identify anomalies at scale (response time spike) Traces show affected request flows (which service is slow) Logs provide detailed context (exception messages, stack traces) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Metrics Specification Metrics Data Model Receiver Components Processor Components Exporter Components Integration Guides Collecting Prometheus Metrics with the OpenTelemetry Collector Prometheus and OpenTelemetry Collector Integration OpenTelemetry Host Metrics receiver Using Prometheus as your OpenTelemetry backend Configure the OpenTelemetry Collector to write metrics into Mimir How to collect Prometheus metrics with the OpenTelemetry Collector and Grafana Component-Specific Resources Prometheus Remote Write Exporter OpenTelemetry Collector Chart (Kubernetes) Prometheus and OpenTelemetry - Better Together Analysis and Best Practices OTLP Metrics Types Understanding OpenTelemetry Metrics OpenTelemetry Metrics Aggregation How Prometheus Exporters Work With OpenTelemetry Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles metrics data, including the metrics data model, receivers, processors, exporters, and temporality concepts.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles metrics data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-metrics/","tags":"","title":"OpenTelemetry Collector: Metrics Support"},{"body":"Overview The OpenTelemetry Collector is designed as an observable service itself, following the principle that observability infrastructure must be observable. The Collector exposes its own telemetry (metrics, logs, and optionally traces) to enable monitoring health, diagnosing issues, and optimizing performance.\nSelf-monitoring is critical for production deployments—without visibility into the Collector’s operation, data loss or performance degradation can go undetected. The Collector provides built-in telemetry, diagnostic extensions, and debugging capabilities to ensure reliable operation at scale.\nKey capabilities include internal metrics exposed via Prometheus endpoints, health check endpoints for liveness/readiness probes, diagnostic extensions for real-time inspection, and structured logging for troubleshooting pipeline issues.\nKey Concepts Internal Telemetry The OpenTelemetry Collector generates internal telemetry by default to expose its operational state. This self-generated observability data helps operators monitor Collector health and performance.\nTelemetry types:\nMetrics: Quantitative measurements of Collector operation (default: Prometheus format on port 8888) Logs: Structured event records emitted to stderr by default Traces: Optional internal tracing of data flow through pipelines Internal telemetry enables:\nReal-time health monitoring Capacity planning and resource optimization Troubleshooting data loss or pipeline issues Performance profiling and bottleneck identification Observability vs. Debugging The Collector provides two complementary approaches:\nObservability (production):\nContinuous metrics collection Health check endpoints Structured logging External monitoring integration Debugging (development/troubleshooting):\nzPages for live data inspection pprof for performance profiling Debug exporters for pipeline validation Verbose logging modes Internal Metrics The Collector exposes comprehensive metrics about its operation through a Prometheus-compatible endpoint.\nMetrics Endpoint By default, the Collector exposes metrics at http://localhost:8888/metrics in Prometheus format. This endpoint can be scraped by Prometheus or any compatible metrics collector.\nConfiguration:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed # Options: none, basic, normal, detailed Key Metrics Categories Data Ingress Metrics Monitor data received by receivers:\notelcol_receiver_accepted_log_records - Log records accepted otelcol_receiver_accepted_spans - Spans accepted otelcol_receiver_accepted_metric_points - Metric points accepted otelcol_receiver_refused_* - Refused data (errors) These metrics help identify:\nData ingestion rates Receiver errors or rejections Source-specific throughput patterns Data Egress Metrics Monitor data sent by exporters:\notelcol_exporter_sent_log_records - Log records sent otelcol_exporter_sent_spans - Spans sent otelcol_exporter_sent_metric_points - Metric points sent otelcol_exporter_send_failed_* - Failed export attempts These metrics reveal:\nExport success rates Backend connectivity issues Data loss from failed exports Queue Metrics Monitor internal buffer state:\notelcol_exporter_queue_capacity - Queue capacity in batches otelcol_exporter_queue_size - Current queue utilization otelcol_exporter_enqueue_failed_* - Failed enqueues (buffer full) Alert on: Queue size approaching capacity indicates backpressure from slow exporters or high ingestion rates.\nProcessor Metrics Monitor processor operation:\notelcol_processor_batch_batch_send_size - Batch sizes sent otelcol_processor_batch_timeout_trigger_send - Timeouts triggering sends Processor-specific metrics (sampling rates, dropped data, etc.) Resource Metrics Monitor Collector resource usage:\nprocess_runtime_* - Go runtime metrics (memory, goroutines) process_cpu_seconds_total - CPU time consumed process_resident_memory_bytes - Memory usage For comprehensive metric details, see Internal telemetry and How to Monitor Open Telemetry Collector Performance.\nSelf-Monitoring Dashboards Several platforms provide pre-built dashboards for Collector monitoring:\nDynatrace OpenTelemetry Collector Self-Monitoring (June 2025 release) Grafana dashboards from the community Vendor-specific monitoring integrations Extensions for Observability Extensions provide auxiliary functionality that supports Collector operation and debugging.\nHealth Check Extension The health_check extension enables an HTTP endpoint that can be probed to check the Collector’s status.\nConfiguration:\nextensions: health_check: endpoint: 0.0.0.0:13133 path: /health/status check_collector_pipeline: enabled: false # Not recommended—use at own risk service: extensions: [health_check] Endpoints:\n/health/status - Returns 200 OK if the Collector is running Important note: The check_collector_pipeline feature is not working as expected and should not be used. Use metrics-based monitoring instead for pipeline health.\nUse cases:\nKubernetes liveness probes Load balancer health checks Container orchestration health monitoring Service mesh integration For more details, see Health Check Monitoring With OpenTelemetry.\nzPages Extension The zpages extension serves HTTP endpoints that provide live data for debugging different components without depending on external backends.\nConfiguration:\nextensions: zpages: endpoint: 0.0.0.0:55679 service: extensions: [zpages] Available pages:\n/debug/servicez - Service summary and version information /debug/pipelinez - Pipeline configuration and status /debug/extensionz - Loaded extensions /debug/tracez - Sample trace data (if internal tracing enabled) Use cases:\nInspecting live data flowing through pipelines Validating configuration changes Debugging data transformation issues In-process diagnostics during development zPages are particularly useful for answering questions like “Is the Collector receiving data?” and “What does the data look like after processing?”\nFor detailed usage, see Monitoring and Debugging the OpenTelemetry Collector.\npprof Extension The pprof extension enables the Go net/http/pprof endpoint for performance profiling.\nConfiguration:\nextensions: pprof: endpoint: 0.0.0.0:1777 service: extensions: [pprof] Available profiles:\n/debug/pprof/profile - CPU profile /debug/pprof/heap - Memory allocation profile /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/block - Blocking profile /debug/pprof/mutex - Mutex contention profile Use cases:\nInvestigating CPU hotspots Analyzing memory leaks Identifying goroutine leaks Profiling lock contention Collecting profiles:\n# CPU profile (30 seconds) curl http://localhost:1777/debug/pprof/profile?seconds=30 \u003e cpu.prof # Heap profile curl http://localhost:1777/debug/pprof/heap \u003e heap.prof # Analyze with pprof go tool pprof cpu.prof Security note: pprof endpoints should only be exposed internally, never to the public internet, as they can reveal sensitive information and consume resources.\nLogging and Debugging Structured Logging The Collector emits structured logs to stderr by default, which can be redirected to files or collected by log aggregation systems.\nLog levels:\ndebug - Verbose debugging information info - General operational messages (default) warn - Warning conditions error - Error conditions Configuration:\nservice: telemetry: logs: level: info encoding: json # Options: json, console Common log patterns:\nReceiver connection failures Exporter send failures Processor errors Configuration validation warnings Debug Exporter The debug (logging) exporter writes telemetry data to the Collector’s standard output, useful for confirming that data is being received, processed, and exported correctly.\nConfiguration:\nexporters: debug: verbosity: detailed # Options: basic, normal, detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, otlp] # Add debug alongside production exporters Use cases:\nValidating pipeline configuration Inspecting data transformations Troubleshooting receiver issues Confirming data format Warning: Debug exporters should be removed or disabled in production due to performance impact and log volume.\nFor troubleshooting guidance, see the Troubleshooting documentation.\nCommon Issues and Troubleshooting Data Loss Symptoms:\notelcol_exporter_send_failed_* metrics increasing Queue size approaching capacity Export errors in logs Common causes:\nExporter destination unavailable or slow Collector under-provisioned (insufficient CPU/memory) Network connectivity issues Backend rate limiting Solutions:\nIncrease queue size and retry parameters Scale Collector instances horizontally Add buffering through load balancers Implement backpressure handling High Memory Usage Symptoms:\nprocess_resident_memory_bytes continuously increasing OOM kills in container environments Slow garbage collection Common causes:\nLarge batch sizes Tail sampling buffer accumulation Queue size too large Memory leaks in processors Solutions:\nReduce batch size and timeout Tune tail sampling buffer limits Enable memory limiting in batch processor Update to latest Collector version (bug fixes) Use pprof to identify memory leaks Note: Memory usage increases in steps due to Go’s garbage collection characteristics, which is normal.\nCPU Spikes Symptoms:\nprocess_cpu_seconds_total rate spikes Request latency increases Throttled container CPU Common causes:\nBatch processing overhead Complex processor logic High ingestion rates Inefficient regex patterns in processors Solutions:\nOptimize processor configuration Distribute load across multiple instances Use simpler transformation patterns Profile with pprof to identify hotspots For detailed debugging workflows, see Guide — How to Debug OpenTelemetry Pipelines.\nSelf-Monitoring Architecture A production self-monitoring setup exports Collector telemetry to external systems:\ngraph TB A[OTel Collector Instance] --\u003e|Internal Metrics| B[Prometheus Exporter :8888] A --\u003e|Internal Logs| C[Stderr Logs] A --\u003e|Health Checks| D[Health Check Extension :13133] A --\u003e|Debugging| E[zPages Extension :55679] A --\u003e|Profiling| F[pprof Extension :1777] B --\u003e|Scrape| G[Prometheus/Metrics Backend] C --\u003e|Collect| H[Log Aggregation System] D --\u003e|Probe| I[Kubernetes/Load Balancer] G --\u003e J[Alerting \u0026 Dashboards] H --\u003e J K[Monitoring Collector] --\u003e|Scrape :8888| B K --\u003e|Send to Backends| L[External Observability Platform] style A fill:#e1f5ff style B fill:#fff4e6 style C fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#e8f5e9 style H fill:#e8f5e9 style I fill:#e8f5e9 style J fill:#ffe6e6 style K fill:#e1f5ff style L fill:#e8f5e9 Configuration Best Practices Production Monitoring Setup 1. Enable comprehensive internal metrics:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed logs: level: info encoding: json 2. Deploy monitoring Collector:\nCreate a dedicated Collector instance to scrape other Collectors:\nreceivers: prometheus: config: scrape_configs: - job_name: otel-collector scrape_interval: 15s static_configs: - targets: ['collector-1:8888', 'collector-2:8888'] exporters: otlp: endpoint: monitoring-backend:4317 service: pipelines: metrics: receivers: [prometheus] exporters: [otlp] 3. Configure health checks:\nextensions: health_check: endpoint: 0.0.0.0:13133 service: extensions: [health_check] 4. Set up alerts:\nKey alerts to configure:\nQueue size \u003e 80% capacity Export failure rate \u003e 1% Memory usage \u003e 80% limit CPU throttling detected Receiver refused rate \u003e 0 Development/Debugging Setup Enable all diagnostic extensions:\nextensions: health_check: endpoint: 0.0.0.0:13133 zpages: endpoint: 0.0.0.0:55679 pprof: endpoint: 0.0.0.0:1777 service: extensions: [health_check, zpages, pprof] telemetry: logs: level: debug Add debug exporters:\nexporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, jaeger] # Debug alongside production Security Considerations Restrict extension endpoints to internal networks only Never expose pprof to the internet Use TLS for metrics endpoints in production Implement authentication for sensitive endpoints Rate limit health check endpoints to prevent DoS Integration Points BattleBots Collector Monitoring For the BattleBots platform, Collector self-monitoring would track:\nOperational metrics:\nGame event ingestion rate (log records/second) Battle trace throughput (spans/second) Bot performance metric collection (metric points/second) Health indicators:\nExport success rate to observability backends Queue utilization during peak match activity Resource usage (CPU, memory) per Collector instance Alerting scenarios:\nQueue capacity exceeded during tournament events Export failures to game analytics backend High latency in telemetry pipeline affecting real-time dashboards Kubernetes Integration In Kubernetes deployments:\nLiveness probe:\nlivenessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 30 periodSeconds: 10 Readiness probe:\nreadinessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 5 periodSeconds: 5 Metrics scraping:\nannotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8888\" prometheus.io/path: \"/metrics\" Further Reading Official Documentation Internal telemetry Troubleshooting Configuration Extensions README Extension Documentation Health Check Extension zPages Extension Collector Observability Documentation Guides and Best Practices Monitoring and Debugging the OpenTelemetry Collector How to Monitor Open Telemetry Collector Performance Guide — How to Debug OpenTelemetry Pipelines Health Check Monitoring With OpenTelemetry Vendor Resources Dynatrace: Introducing OpenTelemetry Collector Self-Monitoring Dashboards Dynatrace: OpenTelemetry Collector self-monitoring OpenTelemetry Collector from A to Z: A Production-Ready Guide Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces ","categories":"","description":"How to observe and debug the OpenTelemetry Collector itself through internal telemetry, extensions, and monitoring strategies.\n","excerpt":"How to observe and debug the OpenTelemetry Collector itself through …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-self-monitoring/","tags":"","title":"OpenTelemetry Collector: Self-Monitoring"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for distributed tracing, enabling collection, processing, and export of trace data from multiple sources to various backend systems. Distributed tracing tracks requests as they flow through distributed systems, providing visibility into service interactions, latency bottlenecks, and error propagation paths.\nThe Collector acts as a central hub for trace data, accepting traces in multiple formats (OTLP, Jaeger, Zipkin), performing intelligent sampling decisions, and routing to multiple tracing backends simultaneously. This unified approach simplifies observability infrastructure while preserving the ability to use best-of-breed tools for different use cases.\nKey capabilities include protocol translation between trace formats, sophisticated sampling strategies (head-based and tail-based), trace enrichment with resource and span attributes, and correlation with metrics and logs through shared context identifiers.\nKey Concepts Traces and Spans A trace represents the full journey of one request or transaction across services, while a span is a timed unit of work inside that journey such as a function call, database query, or external API call.\nTrace structure:\nA trace consists of one or more spans organized in a tree structure Each span represents an operation with a start time and duration Spans have parent-child relationships forming the call graph The root span represents the initial request entry point Span characteristics:\nName: Describes the operation (e.g., “GET /api/battles”) Start time and duration: Timing information Status: Success, error, or unset Span kind: Client, server, internal, producer, or consumer Span Context and Propagation Span context is the portion of a span that must be serialized and propagated between services to maintain trace continuity.\nContext components:\nTraceId: Unique identifier for the entire trace (shared across all spans) SpanId: Unique identifier for the specific span TraceFlags: Sampling and other flags TraceState: System-specific trace state values Propagation mechanism:\nContext propagation transmits context between services via protocols such as HTTP headers, gRPC metadata, or message queues. The default propagator uses the W3C TraceContext specification with traceparent and tracestate headers.\nExample HTTP headers:\ntraceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01 tracestate: vendor1=value1,vendor2=value2 For detailed propagation concepts, see An overview of Context Propagation in OpenTelemetry.\nSpan Attributes Attributes provide additional context about the operation represented by a span. They are key-value pairs that describe request parameters, database queries, HTTP methods, status codes, and other relevant details.\nCommon attribute categories:\nHTTP attributes: http.method, http.status_code, http.route Database attributes: db.system, db.statement, db.name RPC attributes: rpc.service, rpc.method Network attributes: net.peer.name, net.peer.port Best practice: Set attributes at span creation rather than later, since samplers can only consider information present during span creation.\nSpan Events Span events are structured log messages or annotations on a span, typically used to denote meaningful singular points in time during the span’s duration.\nUse cases:\nException events (including stack traces) Checkpoint markers in long operations State transitions Cache hits/misses Retry attempts Events include a name, timestamp, and optional attributes, providing detailed debugging context without creating separate spans for every sub-operation.\nSpan Links Span links establish relationships between spans in different traces or between causally-related but non-parent-child spans. Common scenarios include:\nBatch processing where one span processes multiple input messages Following redirects across multiple traces Async operations spawned from a parent request Trace Receivers Receivers collect trace data from various sources and convert it into the OpenTelemetry traces data model.\nOTLP Receiver The OTLP receiver accepts trace data transmitted using the OpenTelemetry Protocol, the native and recommended format for OpenTelemetry traces.\nSupported transports:\ngRPC (default port 4317): High-performance binary protocol HTTP (default port 4318): RESTful endpoint at /v1/traces Configuration example:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 Use cases:\nCollecting traces from OpenTelemetry-instrumented applications Multi-tier collector deployments (agent → gateway) Modern observability architectures Important: Jaeger V2 natively supports OTLP, making OTLP the recommended protocol for Jaeger backends.\nJaeger Receiver The Jaeger receiver receives trace data in Jaeger format and translates it to OpenTelemetry format. This enables migration from Jaeger-instrumented applications without requiring code changes.\nSupported protocols:\ngRPC (default port 14250): Binary Jaeger protocol thrift_compact (default port 6831): UDP-based compact Thrift thrift_http (default port 14268): HTTP-based Thrift thrift_binary: TCP-based binary Thrift Configuration example:\nreceivers: jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_http: endpoint: 0.0.0.0:14268 thrift_compact: endpoint: 0.0.0.0:6831 Use cases:\nMigrating from Jaeger agent/collector infrastructure Supporting legacy applications instrumented with Jaeger SDKs Gradual transition to OpenTelemetry Zipkin Receiver The Zipkin receiver receives spans in Zipkin V1 and V2 formats and translates them to OpenTelemetry format.\nConfiguration example:\nreceivers: zipkin: endpoint: 0.0.0.0:9411 Use cases:\nMigrating from Zipkin instrumentation Supporting applications instrumented with Zipkin libraries Integration with Zipkin-compatible systems Protocol Translation The Collector acts as a protocol translator, accepting traces in one format and exporting in another. This enables:\nJaeger-instrumented apps → OTLP export to modern backends OpenTelemetry apps → Zipkin export for legacy systems Unified collection from heterogeneous instrumentation Sampling Strategies Sampling controls which traces are retained for analysis, balancing observability value with storage costs and performance impact.\nHead Sampling Head sampling makes sampling decisions at trace creation time, before seeing the complete trace. The decision applies to the entire trace and propagates to downstream services.\nCommon algorithms:\nAlways On: Sample 100% of traces (development/debugging) Always Off: Sample 0% of traces (disable tracing) TraceID Ratio: Sample a percentage based on TraceId hash (e.g., 10%) Rate Limiting: Sample at most N traces per second Advantages:\nLow latency decision (immediate) Low memory overhead (no buffering) Consistent across distributed services Limitations:\nCannot make decisions based on complete trace data Cannot guarantee capturing all error traces Cannot sample based on span attributes or duration For consistent probability sampling details, see OpenTelemetry Sampling.\nTail Sampling Tail sampling makes sampling decisions after seeing all or most spans in a trace, enabling more intelligent sampling based on trace characteristics.\nAvailable policies:\nLatency: Sample traces exceeding duration threshold Status code: Always sample traces with errors Numeric attribute: Sample based on attribute values (min/max thresholds) Probabilistic: Sample a percentage of traces String attribute: Sample traces matching string attributes Rate limiting: Limit traces per second per policy Composite: Combine multiple policies (AND/OR logic) Configuration example:\nprocessors: tail_sampling: policies: - name: errors-policy type: status_code status_code: status_codes: [ERROR] - name: slow-requests type: latency latency: threshold_ms: 1000 - name: sample-10-percent type: probabilistic probabilistic: sampling_percentage: 10 Architecture requirements:\nAll spans for a given trace MUST be received by the same collector instance for effective sampling decisions. This requires:\nLoad balancing exporter: Routes spans by TraceId to consistent collectors Two-tier architecture: Agent collectors → tail sampling gateway collectors For implementation guidance, see Tail Sampling with OpenTelemetry and New Relic and Sampling at scale with OpenTelemetry.\nAdvantages:\nSample all error traces regardless of volume Capture slow requests while dropping fast ones Make sampling decisions based on complete trace data Challenges:\nHigher memory overhead (buffering complete traces) Increased latency (waiting for trace completion) Requires stateful, coordinated collectors Sampling Best Practices For production deployments:\nUse head sampling for baseline traffic reduction (e.g., 10% sampling) Add tail sampling to always capture errors and slow traces Implement two-tier architecture for tail sampling at scale Monitor sampled vs. unsampled trace ratios Adjust policies based on traffic patterns and costs For recent sampling updates, see OpenTelemetry Sampling update.\nTrace Processors Processors transform and enrich trace data as it flows through pipelines.\nSpan Processor The span processor modifies span names, attributes, and other properties.\nCommon operations:\nRename spans for consistency Add/remove span attributes Set span status Modify span kind Attributes Processor Adds, updates, or deletes span and resource attributes, enabling:\nEnvironment labeling Team ownership tags PII removal Attribute normalization Resource Detection Processor Enriches traces with environment metadata:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information Host details This automatic enrichment enables filtering and grouping traces by infrastructure context.\nBatch Processor Groups spans before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nService Graph Processor Generates metrics representing service call relationships from trace data, creating:\nRequest rate between services Error rate between services Latency between services These derived metrics enable service dependency visualization without query-time trace aggregation.\nTrace Exporters Exporters send processed trace data to observability backends and storage systems.\nOTLP Exporter (Recommended) The OTLP exporter sends traces using the OpenTelemetry Protocol to OTLP-compatible backends. OTLP is the recommended choice for new deployments as it’s designed with the OpenTelemetry data model in mind, emitting trace data without loss of information.\nSupported destinations:\nJaeger V2 (native OTLP support) Commercial platforms (Datadog, New Relic, Honeycomb, Dynatrace) Cloud vendor endpoints (AWS X-Ray, Google Cloud Trace, Azure Monitor) Open source backends (Uptrace, Grafana Tempo) Configuration example:\nexporters: otlp: endpoint: jaeger:4317 tls: insecure: false For Jaeger V2 integration, see Using OpenTelemetry to send traces to Jaeger V2.\nJaeger Exporter The Jaeger exporter sends traces to Jaeger backends using the Jaeger gRPC protocol.\nNote: For Jaeger V2, use the OTLP exporter instead. The dedicated Jaeger exporter is maintained for backward compatibility with Jaeger V1 deployments.\nZipkin Exporter The Zipkin exporter sends traces to Zipkin-compatible backends.\nUse cases:\nLegacy Zipkin deployments Systems expecting Zipkin format Gradual migration scenarios Logging Exporter Writes traces to collector standard output for debugging and development.\nFor comprehensive exporter documentation, see OpenTelemetry Collector Exporters.\nTrace Pipeline Flow A sophisticated trace pipeline with multiple receivers, sampling, and multi-backend export:\ngraph LR A[OTLP SDK] --\u003e B[OTLP Receiver] C[Jaeger SDK] --\u003e D[Jaeger Receiver] E[Zipkin SDK] --\u003e F[Zipkin Receiver] B --\u003e G[Resource Detection] D --\u003e G F --\u003e G G --\u003e H[Attributes Processor] H --\u003e I{Load Balancer} I --\u003e|By TraceId| J[Tail Sampling Processor] J --\u003e K[Batch Processor] K --\u003e L[OTLP Exporter] L --\u003e M[Jaeger V2] K --\u003e N[OTLP Exporter] N --\u003e O[Cloud Backend] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#f3e5f5 style L fill:#e8f5e9 style N fill:#e8f5e9 style M fill:#e1f5ff style O fill:#e1f5ff Configuration Considerations Context Propagation Ensure consistent propagation across all services:\nConfigure the same propagators in all SDKs Use W3C TraceContext (standard default) Include Baggage propagation if using cross-cutting concerns Test propagation across language boundaries Sampling Trade-offs Balance observability and cost:\nHigh sampling (50-100%): Development, debugging, low-traffic systems Medium sampling (10-30%): Production with moderate traffic Low sampling (1-10%): High-traffic production systems Tail sampling: Always capture errors regardless of base rate Performance Tuning For high-throughput trace collection:\nEnable batching with appropriate size/timeout Use multiple collector instances with load balancing Configure adequate memory for tail sampling buffers Monitor collector CPU and memory usage Consider two-tier architecture (agent + gateway) Storage Optimization Manage trace storage costs:\nImplement retention policies in backends Use sampling to reduce volume Drop high-cardinality attributes if needed Compress trace data before export Integration Points BattleBots Trace Collection For the BattleBots platform, distributed tracing would track:\nRequest flows:\nClient WebSocket connection → authentication → game state sync Player action → validation → state update → broadcast Match creation → bot pairing → game initialization Service interactions:\nAPI gateway → game service → persistence layer Event publisher → message broker → subscriber services Load balancer → multiple game server instances Timing analysis:\nEnd-to-end battle action latency Database query performance WebSocket message propagation time The OTLP receiver collects traces from Go services instrumented with the OpenTelemetry Go SDK, while Jaeger/Zipkin receivers support any legacy instrumentation.\nTrace-Log Correlation Connecting traces and logs enables powerful debugging workflows:\nStart with trace: Identify slow or failing request Find associated logs: Query logs by TraceId and SpanId Examine context: Read detailed log messages and exceptions Understand causation: See timeline of events leading to issue This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically when both signals are instrumented.\nTrace-Metric Correlation Link traces and metrics through exemplars:\nHistogram buckets contain sample trace IDs Click from high-latency metric to example slow trace Correlate error rate spike with specific failing traces Validate fixes by monitoring metrics and inspecting traces Further Reading Official Documentation OpenTelemetry Traces Specification Context Propagation Sampling Receiver Components Processor Components Exporter Components Sampling Resources Tail Sampling Processor OpenTelemetry Sampling Tail Sampling with OpenTelemetry and New Relic Sampling at scale with OpenTelemetry OpenTelemetry Sampling update Integration Guides Getting Started with the Jaeger and Zipkin Receivers Using OpenTelemetry to send traces to Jaeger V2 OpenTelemetry Collector Exporters Context Propagation Deep Dives An overview of Context Propagation in OpenTelemetry OpenTelemetry Context Propagation Explained Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles distributed tracing data, including the span model, receivers, sampling strategies, and exporters.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles distributed …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-traces/","tags":"","title":"OpenTelemetry Collector: Traces Support"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/tags/","tags":"","title":"Tags"}]