[{"body":"Idea This proof of concept demonstrates containerized bots battling each other in a 1v1 match within a 2-dimensional space. To evaluate the optimal architecture for the final BattleBots platform, both a client/server implementation and a peer-to-peer implementation will be completed and compared.\nRequirements Client/Server Architecture Implement a client/server architecture to evaluate its suitability for the final BattleBots platform.\nPeer-to-Peer Architecture Implement a peer-to-peer architecture to compare against the client/server approach.\n1v1 Battle Bots compete in a 1v1 battle within a 2-dimensional space.\nContainerized Bots Each bot should be a container to ensure isolation and portability.\nLanguage-Agnostic Bot Implementation The game logic should be independent of each bot so that any programming language can be used to implement a bot.\nObservability Observability signals should be captured so the battle can be monitored in real-time.\nBattle Visualization A battle visualization should be implemented to display the battle state and actions.\nPending ADRs ADR-NNNN: Client/Server Architecture This ADR will document the design decisions for the client/server implementation, including the server’s responsibilities for game state management, turn coordination, and validation of bot actions.\nADR-NNNN: Peer-to-Peer Architecture This ADR will document the design decisions for the peer-to-peer implementation, including consensus mechanisms for game state, conflict resolution, and how bots communicate directly with each other.\nADR-NNNN: Game Runtime Architecture This ADR will define the “game loop” and core game mechanics, including turn-based vs. real-time gameplay, tick rates, state updates, and the overall flow of battle execution.\nADR-NNNN: Bot to Battle Server Interface This ADR will define the communication protocol between bots and the battle server or between bots in P2P mode, evaluating options such as gRPC, HTTP, or custom TCP/UDP packets.\nADR-NNNN: Observability Stack This ADR will document the observability architecture, including metrics collection, logging, tracing, and how battle telemetry is captured and exposed for monitoring and analysis.\nADR-NNNN: Battle Visualization This ADR will document the design of the battle visualization system, including the rendering approach, real-time updates, and how observability data is translated into visual representations.\n","categories":"","description":"Documents the proof of concept for running a 1v1 battle between two containerized bots using podman-compose\n","excerpt":"Documents the proof of concept for running a 1v1 battle between two …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/0001-poc/","tags":"","title":"[0001] Proof of Concept - 1v1 Battle"},{"body":" Context and Problem Statement As the project grows, architectural decisions are made that have long-term impacts on the system’s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.\nHow should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?\nDecision Drivers Need for clear documentation of architectural decisions and their rationale Easy accessibility and searchability of past decisions Low barrier to entry for creating and maintaining decision records Integration with existing documentation workflow Version control friendly format Industry-standard approach that team members may already be familiar with Considered Options MADR (Markdown Architectural Decision Records) ADR using custom format Wiki-based documentation No formal ADR process Decision Outcome Chosen option: “MADR (Markdown Architectural Decision Records)”, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.\nConsequences Good, because MADR is a widely adopted standard with clear documentation and examples Good, because markdown files are easy to create, edit, and review through pull requests Good, because ADRs will be version-controlled alongside code, maintaining historical context Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions Good, because team members can easily search and reference past decisions Neutral, because requires discipline to maintain and update ADR status as decisions evolve Bad, because team members need to learn and follow the MADR format conventions Confirmation Compliance will be confirmed through:\nCode reviews ensuring new architectural decisions are documented as ADRs ADRs are stored in docs/content/r\u0026d/adrs/ following the naming convention NNNN-title-with-dashes.md Regular reviews during architecture discussions to reference and update existing ADRs Pros and Cons of the Options MADR (Markdown Architectural Decision Records) MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.\nGood, because it’s a well-established standard with extensive documentation Good, because markdown is simple, portable, and version-control friendly Good, because it provides a clear structure while remaining flexible Good, because it integrates with static site generators and documentation tools Good, because it’s lightweight and doesn’t require special tools Neutral, because it requires some initial learning of the format Neutral, because maintaining consistency requires discipline ADR using custom format Create our own custom format for architectural decision records.\nGood, because we can tailor it exactly to our needs Bad, because it requires defining and maintaining our own standard Bad, because new team members won’t be familiar with the format Bad, because we lose the benefits of community knowledge and tooling Bad, because it may evolve inconsistently over time Wiki-based documentation Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.\nGood, because wikis provide easy editing and hyperlinking Good, because some team members may be familiar with wiki tools Neutral, because it may or may not integrate with version control Bad, because content may not be version-controlled alongside code Bad, because it creates a separate system to maintain Bad, because it’s harder to review changes through standard PR process Bad, because portability and long-term accessibility may be concerns No formal ADR process Continue without a structured approach to documenting architectural decisions.\nGood, because it requires no additional overhead Bad, because context and rationale for decisions are lost over time Bad, because new team members struggle to understand why decisions were made Bad, because it leads to repeated discussions of previously settled questions Bad, because it makes it difficult to track when decisions should be revisited More Information MADR 4.0.0 specification: https://adr.github.io/madr/ ADRs will be categorized as: strategic, user-journey, or api-design ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX All ADRs are stored in docs/content/r\u0026d/adrs/ directory ","categories":"","description":"Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.\n","excerpt":"Adopt Markdown Architectural Decision Records (MADR) as the standard …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/0001-use-madr-for-architecture-decision-records/","tags":"","title":"[0001] Use MADR for Architecture Decision Records"},{"body":"User Journeys This section contains detailed user journey documentation that defines how users interact with the Battlebots platform. Each journey document includes:\nUser personas and their goals Step-by-step flow diagrams Technical requirements (access control, analytics, etc.) Success metrics These documents serve as the foundation for feature development and help ensure a consistent, user-centered experience.\n","categories":"","description":"Documentation of user flows and experiences for the Battlebots platform\n","excerpt":"Documentation of user flows and experiences for the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/user-journeys/","tags":"","title":"User Journeys"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/","tags":"","title":"Analysis"},{"body":"Architecture Decision Records (ADRs) This section contains architectural decision records that document the key design choices made for the Battlebots platform. Each ADR follows the MADR 4.0.0 format and includes:\nContext and problem statement Decision drivers and constraints Considered options with pros and cons Decision outcome and rationale Consequences (positive and negative) Confirmation methods ADR Categories ADRs are classified into three categories:\nStrategic - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices. User Journey - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features. API Design - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation. Status Values Each ADR has a status that reflects its current state:\nproposed - Decision is under consideration accepted - Decision has been approved and should be implemented rejected - Decision was considered but not approved deprecated - Decision is no longer relevant or has been superseded superseded by ADR-XXXX - Decision has been replaced by a newer ADR These records provide historical context for architectural decisions and help ensure consistency across the platform.\n","categories":"","description":"Documentation of architectural decisions made in the Battlebots platform using MADR 4.0.0 standard\n","excerpt":"Documentation of architectural decisions made in the Battlebots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/adrs/","tags":"","title":"Architecture Decision Records"},{"body":"Welcome! Battle Bots is a game in which you, the human, implement an autonomous “bot” to do battle with “bots” implemented by other humans.\nWhat is a Bot? A bot is a independent piece of software which is programmed to battle other bots by reacting to state updates (e.g. bot B moved to point A) and performing its own actions (e.g. fire missile at point A).\n","categories":"","description":"Battle Bots is a PVP game for autonomous players","excerpt":"Battle Bots is a PVP game for autonomous players","ref":"/battlebots/pr-preview/pr-150/","tags":"","title":"Battle Bots"},{"body":"R\u0026D Process The Research \u0026 Design process follows a structured workflow to ensure comprehensive analysis and documentation of user experiences, technical solutions, and implementation details.\nProcess Steps Document the User Journey\nCreate a user journey document for the specific user experience Include flow diagrams using Mermaid to visualize user interactions Define prioritized technical requirements (P0/P1/P2) Use the /new-user-journey command to create standardized documentation Design the Solution\nCreate an ADR that designs a solution to implement the user journey Identify and document: Additional ADRs needed for specific components APIs that need to be defined User interface flows (mobile, web, etc.) Data flow from user to end systems (database, notification system, etc.) Capture the complete system architecture and integration points Document Component ADRs\nCreate ADRs for specific technical components identified in the solution design Examples: authentication strategy, session management, account linking, data storage Use the /new-adr command to create standardized MADR 4.0.0 format documents Document technical decisions with context, considered options, and consequences Document Required APIs\nFor each API endpoint identified in the solution, create comprehensive API documentation Use the /new-api-doc command to create standardized documentation Include: Request/response schemas Authentication requirements Business logic flows (Mermaid diagrams) Error responses and status codes Example curl requests Document API Implementation\nFor each documented API, create an ADR describing the implementation approach Document technical decisions including: Programming language selection Framework and libraries Architecture patterns Testing strategy Example: ADR-0006 documents the tech stack for API development (z5labs/humus framework) Design User Interface\nCreate UI/UX designs for the user journey Ensure designs align with the documented user flows and API contracts Consider platform-specific requirements (mobile, web, desktop) Documentation Structure The R\u0026D documentation is organized into the following sections:\nUser Journeys - User experience flows with technical requirements ADRs - Architectural Decision Records documenting technical decisions APIs - REST API endpoint documentation with schemas and examples Analysis - Research and analysis of technologies and solutions ","categories":"","description":"","excerpt":"R\u0026D Process The Research \u0026 Design process follows a structured …","ref":"/battlebots/pr-preview/pr-150/research_and_development/","tags":"","title":"Research \u0026 Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/categories/","tags":"","title":"Categories"},{"body":"Overview Grafana Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. Unlike other log aggregation systems, Loki is designed to be cost-effective and easy to operate by not indexing the contents of logs, but rather a set of labels for each log stream.\nThis analysis explores Loki as a potential log storage backend for the BattleBots platform, focusing on its architecture, deployment options, OTLP compatibility, and integration with the OpenTelemetry Collector.\nWhy Research Loki? For the BattleBots observability stack, Loki offers several compelling advantages:\nNative OTLP Support: Loki v3+ provides native OTLP ingestion endpoints, enabling seamless integration with the OpenTelemetry Collector Cost-Effective Storage: Index-free approach dramatically reduces storage costs compared to full-text indexing systems Horizontal Scalability: Microservices architecture supports scaling from development to production workloads Grafana Integration: Tight integration with Grafana provides unified visualization of logs, metrics, and traces Cloud-Native Design: Built for containerized environments with Kubernetes-first deployment patterns Document Structure The Loki analysis is organized into the following documents:\nLoki Overview Comprehensive overview covering architecture, deployment, and operational best practices.\nTopics covered:\nWhat is Loki and its design philosophy (index-free, label-based querying) Core concepts: streams, labels, chunks, index, LogQL Architecture components: distributor, ingester, querier, compactor Deployment modes: monolithic, simple scalable, microservices How to run Loki with Docker/Podman Compose for POC Best practices for label strategy and configuration When to use Loki vs. alternatives like Elasticsearch Audience: Everyone—provides foundational understanding for evaluating Loki as a log backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration.\nTopics covered:\nNative OTLP support in Loki v3+ (endpoints, configuration) OTel Collector otlphttp exporter setup Resource attribute mapping to Loki labels Log-trace correlation via TraceID/SpanID Complete working configuration examples Authentication and multi-tenancy Troubleshooting common integration issues Audience: Developers and operators implementing the OTel Collector to Loki pipeline.\nBattleBots Integration Context For the BattleBots platform, Loki would serve as the centralized log storage backend, receiving logs from the OpenTelemetry Collector via OTLP. This enables:\nGame Event Logging Battle events (bot actions, damage calculations, victory conditions) Game state transitions and timing information Player actions and command processing Error conditions and system anomalies Infrastructure Logging Container logs from game servers System logs from host infrastructure Application logs from Go services Network and security logs Unified Observability Log-trace correlation through shared TraceID/SpanID Linking log events to metrics and distributed traces Grafana dashboards combining logs, metrics, and traces Streamlined debugging workflows across all telemetry signals Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the log storage backend for BattleBots. Key decision factors include:\nFunctional fit: Does Loki meet log storage, query, and correlation requirements? Operational complexity: How difficult is it to deploy, monitor, and maintain Loki? Cost: What are the infrastructure and operational costs at POC and production scale? Integration: How well does it integrate with OTel Collector, Grafana, and the broader observability stack? External Resources Grafana Loki Official Documentation Loki GitHub Repository Grafana Labs Blog CNCF Loki Project ","categories":"","description":"Research and analysis of Grafana Loki log aggregation system for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Loki log aggregation system for the …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/","tags":"","title":"Grafana Loki"},{"body":"Overview Grafana Mimir is a horizontally scalable, highly available, multi-tenant, long-term storage solution for Prometheus metrics. It transforms Prometheus’s single-server architecture into a distributed microservices platform capable of handling over 1 billion active time series with unlimited retention.\nThis analysis explores Mimir as the metrics storage backend for the BattleBots platform, focusing on its architecture, native OTLP support, OpenTelemetry Collector integration, and operational characteristics.\nWhy Research Mimir? For the BattleBots observability stack, Mimir offers several compelling advantages over standalone Prometheus:\nNative OTLP Support: Direct ingestion via /otlp/v1/metrics endpoint since version 2.3.0, enabling seamless OpenTelemetry Collector integration without protocol translation Massive Scalability: Proven at 1 billion active time series (Grafana Labs internal testing) and 500 million series in production customer deployments Long-Term Storage: Object storage backend (S3, GCS, MinIO) enables months to years of retention at minimal cost compared to local disk storage Multi-Tenancy: Built-in tenant isolation with per-tenant limits, enabling future use cases like per-player metrics or per-battle analytics High Availability: 3x replication by default, distributed architecture, and automatic failover eliminate single points of failure Prometheus Compatibility: Full PromQL support ensures existing Prometheus queries, dashboards, and alerts work unchanged Document Structure The Mimir analysis is organized into the following documents:\nMimir Overview Comprehensive overview covering architecture, deployment modes, storage backends, and operational best practices.\nTopics covered:\nWhat is Mimir and its design philosophy (distributed Prometheus with object storage) Core concepts: blocks storage, time series, multi-tenancy, cardinality management Architecture components: distributor, ingester, querier, query-frontend, store-gateway, compactor, ruler Deployment modes: monolithic, read-write, microservices (with comparison table) How to run Mimir with Docker Compose and MinIO for POC environments Production deployment patterns with Kubernetes and Helm Best practices for label strategy, configuration, storage selection, and performance tuning When to use Mimir vs. Prometheus vs. Thanos (decision criteria matrix) Resource requirements and capacity planning guidance Complete configuration examples for all deployment modes Audience: Everyone—provides foundational understanding for evaluating Mimir as the metrics backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration (addresses critical user requirements).\nTopics covered:\nNative OTLP support in Mimir (status, version requirements, endpoint configuration) OTLP vs. Prometheus remote write comparison and recommendations OpenTelemetry Collector otlphttp exporter configuration for Mimir Alternative: OpenTelemetry Collector prometheusremotewrite exporter setup Batch processor, retry policies, and queue management best practices Resource attribute mapping from OTel to Mimir labels Label strategy and cardinality control for OTel-generated metrics Authentication and multi-tenancy setup with X-Scope-OrgID headers Complete working configuration examples (OTel Collector + Mimir + Grafana) Troubleshooting common integration issues (connection errors, cardinality, performance) BattleBots-specific integration patterns and example queries Audience: Developers and operators implementing the OpenTelemetry Collector to Mimir pipeline.\nBattleBots Integration Context For the BattleBots platform, Mimir would serve as the centralized metrics storage backend, receiving metrics from the OpenTelemetry Collector via native OTLP ingestion. This enables:\nGame Metrics Storage Bot Performance: Action latency, damage calculations, movement speed, resource utilization per bot Battle Events: Start/end times, player actions, victory conditions, matchmaking metrics Game State: Active battles count, queued players, concurrent users, session durations Quality Metrics: Frame rates, tick rates, network latency, synchronization quality Infrastructure Metrics Container Metrics: CPU/memory usage, restart counts, health checks for bot containers and game servers Kubernetes Metrics: Pod status, node utilization, deployment health, scaling events Network Metrics: Request rates, latency distributions, error rates, bandwidth consumption Host Metrics: System-level CPU, memory, disk I/O, network traffic across infrastructure Observability Stack Metrics OpenTelemetry Collector: Pipeline throughput, batch sizes, queue depths, export success/failure rates Loki (Log Storage): Log ingestion rates, query latency, storage utilization Tempo (Trace Storage): Span ingestion, trace completeness, sampling rates Mimir Self-Monitoring: Ingester series counts, query performance, compaction status, object storage health Long-Term Analytics Capacity Planning: Historical resource usage trends to predict scaling needs Cost Optimization: Identify underutilized resources and optimize allocation Performance Baselines: Establish normal behavior patterns for anomaly detection Business Intelligence: Player engagement metrics, battle frequency, peak usage times Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the metrics storage backend for BattleBots. Key decision factors specific to Mimir include:\nScalability Requirements: Can Mimir handle expected growth from POC (thousands of series) to production (millions+ of series)? OTLP Integration: Does native OTLP support simplify the OpenTelemetry Collector integration compared to alternatives? Operational Complexity: Is the team prepared to operate a distributed metrics system, or should we start with standalone Prometheus? Cost vs. Value: Do Mimir’s features (long-term storage, scalability, multi-tenancy) justify the increased infrastructure cost and complexity? Migration Path: If starting with Prometheus, how difficult is migration to Mimir when scale demands it? The ADR will also consider alternative approaches:\nStandalone Prometheus: Simpler but limited to ~10M series and short retention Thanos: Similar capabilities to Mimir with different architectural trade-offs Managed Services: Grafana Cloud or other hosted Prometheus-compatible solutions External Resources Grafana Mimir Official Documentation Mimir GitHub Repository Grafana Mimir Blog Mimir Capacity Calculator OpenTelemetry Metrics to Mimir Guide Prometheus Remote Write Specification ","categories":"","description":"Research and analysis of Grafana Mimir metrics storage system for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Mimir metrics storage system for the …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/metrics/mimir/","tags":"","title":"Grafana Mimir"},{"body":"Overview This section contains research and analysis of log storage solutions for the BattleBots platform. Effective log storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Log Backend Options Grafana Loki Analysis of Grafana Loki, a horizontally scalable, multi-tenant log aggregation system optimized for storing and querying log data.\nLoki uses a unique index-free approach that indexes only metadata labels rather than full log content, significantly reducing storage and operational costs compared to traditional log aggregation systems.\nKey features include:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and how to run Loki OTLP compatibility and OTel Collector integration Best practices for running and operating Loki Future Analysis Additional log backend options may be researched based on BattleBots requirements:\nElasticsearch/OpenSearch - Full-text search capabilities Cloud-native options - AWS CloudWatch Logs, Google Cloud Logging Self-hosted alternatives - ClickHouse, Vector Related Documentation R\u0026D Documentation OpenTelemetry Collector Analysis - Log collection and processing Observability Analysis - Overall observability strategy Future ADR on observability stack selection External Resources Grafana Loki Documentation OpenTelemetry Documentation CNCF Observability Projects ","categories":"","description":"Research and analysis of log storage backends for the BattleBots observability stack.\n","excerpt":"Research and analysis of log storage backends for the BattleBots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/","tags":"","title":"Log Storage Analysis"},{"body":"Overview Grafana Loki introduced native OpenTelemetry Protocol (OTLP) support in version 3.0, marking a significant advancement in how logs can be ingested into Loki. This native integration allows applications instrumented with OpenTelemetry to send logs directly to Loki using the standardized OTLP format, eliminating the need for format transformations and simplifying the observability pipeline.\nThe native OTLP endpoint provides a fully OpenTelemetry-compliant ingestion path where logs sent in OTLP format are stored directly in Loki without requiring conversion to JSON or logfmt blobs. This approach leverages Loki’s structured metadata feature, which stores log attributes and other OpenTelemetry LogRecord fields separately from the log body itself. The result is a more intuitive query experience and better performance, as queries no longer need to parse JSON at runtime to access fields.\nFor the BattleBots observability stack, native OTLP integration offers several advantages: unified telemetry collection across logs, metrics, and traces through the OpenTelemetry Collector; simplified configuration compared to legacy exporters; better correlation between logs and traces through preserved TraceId and SpanId fields; and vendor portability, making it easier to migrate between observability backends without changing instrumentation.\nOTLP Support in Loki Answer: YES - Loki versions 3.0 and later natively support the OpenTelemetry Protocol (OTLP) for log ingestion.\nNative OTLP Endpoint Loki exposes an OTLP-compliant endpoint at /otlp that accepts OpenTelemetry log data. When clients send logs to this endpoint, the collector automatically appends the appropriate path suffix (/v1/logs), resulting in requests to /otlp/v1/logs.\nSupported Protocols:\nHTTP: POST requests using HTTP/1.1 or HTTP/2 gRPC: Unary RPC calls using the OTLP service definition Default Port:\nLoki typically runs on port 3100 for all HTTP endpoints, including OTLP Version Requirements Minimum Loki Version: 3.0 or later\nSchema Requirements:\nSchema version: v13 or higher (required for structured metadata) Index type: tsdb (Time Series Database index required) Structured metadata is essential for OTLP ingestion because it stores the OpenTelemetry LogRecord fields (resource attributes, instrumentation scope, log attributes) separately from the log body. Without schema v13 and tsdb, Loki cannot properly handle OTLP data.\nBenefits of Native OTLP vs Legacy Loki Exporter The legacy lokiexporter component in the OpenTelemetry Collector encoded logs as JSON or logfmt blobs with Loki-specific label conventions. The native OTLP endpoint provides several improvements:\nSimplified Querying: No JSON parsing required at query time. Instead of {job=\"dev/auth\"} | json | severity=\"INFO\", you can query directly: {service_name=\"auth\"} | severity_text=\"INFO\"\nCleaner Log Bodies: The log message is stored as-is rather than wrapped in a JSON structure. A log “user logged in” is stored exactly as that string, with metadata in structured fields.\nStandard Resource Labels: Uses OpenTelemetry semantic conventions (service_name, service_namespace) instead of custom labels (job=service.namespace/service.name)\nBetter Performance: Structured metadata allows efficient filtering without parsing the entire log body\nVendor Portability: Standard OTLP configuration works across multiple backends without Loki-specific hints\nFuture-Proof: The native endpoint represents Grafana’s strategic direction for log ingestion\nOTLP Endpoint Configuration To enable OTLP ingestion in Loki, you must configure structured metadata support and optionally customize which resource attributes become index labels.\nEnabling Structured Metadata Structured metadata is enabled by default in Loki 3.0+, but you should explicitly configure it in your limits:\nlimits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 # Maximum metadata entries per log record Schema Configuration Your Loki schema must use version 13 or higher with the tsdb index:\nschema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: s3 # or filesystem, gcs, azure, etc. schema: v13 index: prefix: loki_index_ period: 24h Complete Loki Configuration Example Here’s a complete Loki configuration with OTLP support enabled:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 per_stream_rate_limit: 5MB per_stream_rate_limit_burst: 15MB # OTLP-specific configuration otlp_config: resource_attributes: # Configure which resource attributes become index labels attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.cluster.name - k8s.namespace.name - cloud.region - cloud.provider # Convert high-cardinality attributes to structured metadata - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 OTel Collector Export Configuration Answer: YES - The OpenTelemetry Collector can export logs to Loki using the otlphttp exporter.\nBasic OTLP HTTP Exporter The recommended exporter for Loki is otlphttp/logs:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true # For local development without TLS Important: Do not append /v1/logs to the endpoint URL. The OTLP exporter automatically adds the appropriate path suffix.\nComplete Exporter Configuration Here’s a production-ready configuration with retry, timeout, and queue settings:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" # TLS Configuration tls: insecure: false ca_file: /etc/ssl/certs/loki-ca.crt cert_file: /etc/ssl/certs/client.crt key_file: /etc/ssl/private/client.key min_version: \"1.2\" # Timeout for individual requests (default: 30s recommended) timeout: 30s # Retry configuration retry_on_failure: enabled: true initial_interval: 5s # Time to wait before first retry max_interval: 30s # Maximum backoff interval max_elapsed_time: 300s # Give up after 5 minutes # Queue configuration for reliability sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage # Reference to file_storage extension for persistence # Compression (gzip recommended for production) compression: gzip # Headers (for authentication, multi-tenancy) headers: X-Scope-OrgID: \"battlebots\" File Storage Extension for Persistence To persist queued logs across collector restarts, configure the file storage extension:\nextensions: file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s service: extensions: [file_storage] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Batch Processor Configuration Always include a batch processor before the exporter to optimize throughput:\nprocessors: batch: timeout: 10s # Send batch after this duration send_batch_size: 8192 # Send when batch reaches this size send_batch_max_size: 16384 # Never exceed this size Complete Service Pipeline service: extensions: [file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [resource_detection, batch, attributes] exporters: [otlphttp/logs] Resource Attribute Mapping When logs arrive via OTLP, resource attributes from the OpenTelemetry SDK map to either index labels or structured metadata in Loki.\nDefault Resource Attributes as Index Labels By default, Loki converts these 17 resource attributes to index labels:\nservice.name → service_name service.namespace → service_namespace service.instance.id → service_instance_id deployment.environment → deployment_environment cloud.region → cloud_region cloud.availability_zone → cloud_availability_zone cloud.platform → cloud_platform k8s.cluster.name → k8s_cluster_name k8s.namespace.name → k8s_namespace_name k8s.pod.name → k8s_pod_name k8s.container.name → k8s_container_name container.name → container_name k8s.replicaset.name → k8s_replicaset_name k8s.deployment.name → k8s_deployment_name k8s.statefulset.name → k8s_statefulset_name k8s.daemonset.name → k8s_daemonset_name k8s.cronjob.name → k8s_cronjob_name Note: Attribute names with dots (.) are converted to underscores (_) for Loki label compatibility.\nAttribute Transformation in Collector To add or modify resource attributes before sending to Loki:\nprocessors: resource: attributes: # Add static attributes - key: deployment.environment value: production action: insert # Copy attributes with character transformations - key: service_name from_attribute: service.name action: insert # Delete attributes you don't want - key: telemetry.sdk.version action: delete attributes: actions: # Add log-level attributes - key: environment value: production action: insert # Extract correlation IDs from log body - key: correlation_id pattern: \"correlation_id=([a-z0-9-]+)\" action: extract Example Attribute to Label Mapping Input (OpenTelemetry SDK):\n{ \"resource\": { \"attributes\": { \"service.name\": \"game-server\", \"service.namespace\": \"battlebots\", \"deployment.environment\": \"production\", \"k8s.pod.name\": \"game-server-5d7c8f9b-xq2wr\", \"k8s.namespace.name\": \"battlebots-prod\" } } } Output (Loki Labels):\n{ service_name=\"game-server\", service_namespace=\"battlebots\", deployment_environment=\"production\", k8s_namespace_name=\"battlebots-prod\" } Note: k8s.pod.name should be converted to structured metadata (see Label Strategy section).\nLabel Strategy and Best Practices Loki’s performance depends heavily on proper label cardinality management. Every unique combination of label values creates a new stream, and too many streams degrade performance significantly.\nAvoiding High Cardinality Issues Problem: High cardinality causes Loki to build a huge index and flush thousands of tiny chunks to object storage, resulting in poor performance and high costs.\nHigh-Cardinality Attributes (Avoid as Labels):\nk8s.pod.name - Each pod instance creates a new label value service.instance.id - Each service instance is unique process.pid - Changes on every process restart User IDs, request IDs, transaction IDs Timestamps, UUIDs, hashes Which Attributes to Use as Labels Good Label Candidates (Low Cardinality):\nservice.name - Limited number of services service.namespace - Few namespaces (dev, staging, prod) deployment.environment - Usually 3-5 values k8s.cluster.name - Fixed cluster names k8s.namespace.name - Limited namespaces per cluster cloud.region - Fixed set of regions log.severity or severity_text - Limited severity levels Cardinality Rule of Thumb: Keep total stream count under 10,000. With 5 labels averaging 10 values each, you get 10^5 = 100,000 streams (too many). Reduce to 3-4 labels with controlled values.\nWhich Attributes to Keep as Structured Metadata Configure high-cardinality attributes as structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid - process.command_line - host.id Structured metadata remains queryable but doesn’t create new streams:\n{service_name=\"game-server\"} | k8s_pod_name=\"game-server-5d7c8f9b-xq2wr\" Recommended Label Strategy for BattleBots limits_config: otlp_config: resource_attributes: attributes_config: # Index labels (low cardinality) - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region # Structured metadata (high cardinality or optional) - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name - process.pid Expected Cardinality:\nservice.name: ~10 services (game-server, matchmaker, auth, etc.) service.namespace: 1 value (battlebots) deployment.environment: 3 values (dev, staging, production) k8s.namespace.name: ~5 namespaces cloud.region: ~3 regions Total streams: 10 × 1 × 3 × 5 × 3 = 450 streams (excellent)\nLog-Trace Correlation OpenTelemetry’s unified data model enables seamless correlation between logs and traces through TraceId and SpanId fields embedded in log records.\nTraceID and SpanID Storage When applications instrumented with OpenTelemetry SDKs emit logs within a trace context, the SDK automatically includes:\nTraceId: Unique identifier for the entire trace SpanId: Unique identifier for the current span TraceFlags: Sampling and other flags Loki stores these fields as structured metadata, making them queryable without parsing the log body.\nQuerying Logs by Trace ID Find all logs for a specific trace:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Find logs with any trace context:\n{service_name=\"game-server\"} | trace_id != \"\" Find logs for a specific span:\n{service_name=\"game-server\"} | span_id=\"00f067aa0ba902b7\" Grafana Integration for Correlation Configure Grafana data source correlations to link Loki and Tempo:\nLoki Data Source Configuration:\n{ \"derivedFields\": [ { \"datasourceUid\": \"tempo-uid\", \"matcherRegex\": \"trace_id=(\\\\w+)\", \"name\": \"TraceID\", \"url\": \"${__value.raw}\" } ] } This creates clickable trace ID links in the Grafana Explore view, allowing you to:\nView a log entry in Loki Click the trace ID link Jump directly to the full trace in Tempo See the complete request flow with timing information Example Correlation Workflow Scenario: Investigating a slow game server response\nStart with metrics: Notice elevated response times in Prometheus metrics Query slow traces: Find traces with duration \u003e 1s in Tempo Jump to logs: Click trace ID to see all logs for that request Identify root cause: Read detailed error messages and debug logs Correlate with resources: Use k8s_pod_name metadata to check pod health Query pattern:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" | severity_text=\"ERROR\" Authentication \u0026 Multi-tenancy Loki supports both basic authentication and multi-tenant deployments for OTLP ingestion.\nBasic Authentication Setup Collector Configuration with Basic Auth:\nextensions: basicauth/otlp: client_auth: username: \"battlebots-collector\" password: \"${LOKI_PASSWORD}\" # Use environment variable exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" auth: authenticator: basicauth/otlp service: extensions: [basicauth/otlp] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Loki Configuration with Basic Auth:\nConfigure authentication in your reverse proxy (nginx, Envoy) or API gateway rather than directly in Loki:\nlocation /otlp { auth_basic \"Loki OTLP Endpoint\"; auth_basic_user_file /etc/nginx/.htpasswd; proxy_pass http://loki:3100; } Multi-tenant Headers For multi-tenant Loki deployments, use the X-Scope-OrgID header to specify the tenant:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"battlebots-production\" Dynamic Tenant Selection:\nRoute different services to different tenants using the resource processor:\nprocessors: resource: attributes: - key: loki.tenant from_attribute: deployment.environment action: insert exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"${LOKI_TENANT}\" Loki Multi-tenancy Configuration:\nauth_enabled: true limits_config: # Per-tenant rate limits ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 # Per-tenant OTLP configuration per_tenant_override_config: /etc/loki/overrides.yaml Per-tenant overrides (/etc/loki/overrides.yaml):\noverrides: battlebots-production: ingestion_rate_mb: 50 retention_period: 720h # 30 days battlebots-staging: ingestion_rate_mb: 20 retention_period: 168h # 7 days TLS Configuration Collector with TLS:\nexporters: otlphttp/logs: endpoint: \"https://loki.battlebots.example.com/otlp\" tls: insecure: false ca_file: /etc/ssl/certs/ca-bundle.crt cert_file: /etc/ssl/certs/collector-client.crt key_file: /etc/ssl/private/collector-client.key min_version: \"1.3\" server_name_override: loki.battlebots.example.com Loki TLS Configuration:\nserver: http_listen_port: 3100 grpc_listen_port: 9096 http_tls_config: cert_file: /etc/loki/tls/server.crt key_file: /etc/loki/tls/server.key client_auth_type: RequireAndVerifyClientCert client_ca_file: /etc/loki/tls/ca.crt Complete Configuration Example This section provides a full, working configuration for integrating OpenTelemetry Collector with Loki OTLP.\nFull OpenTelemetry Collector Configuration # /etc/otelcol/config.yaml extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 filelog: include: - /var/log/battlebots/*.log include_file_path: true include_file_name: false operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' processors: resourcedetection: detectors: [env, system, docker, kubernetes] timeout: 5s resource: attributes: - key: deployment.environment value: production action: insert - key: service.namespace value: battlebots action: insert attributes: actions: - key: loki.attribute.labels value: severity_text, service_name action: insert batch: timeout: 10s send_batch_size: 8192 send_batch_max_size: 16384 memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true timeout: 30s retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage compression: gzip headers: X-Scope-OrgID: \"battlebots\" debug: verbosity: detailed sampling_initial: 5 sampling_thereafter: 200 service: extensions: [health_check, pprof, zpages, file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [memory_limiter, resourcedetection, resource, attributes, batch] exporters: [otlphttp/logs] telemetry: logs: level: info metrics: address: 0.0.0.0:8888 Full Loki Configuration with OTLP # /etc/loki/config.yaml auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 50 ingestion_burst_size_mb: 100 per_stream_rate_limit: 10MB per_stream_rate_limit_burst: 20MB max_label_names_per_series: 15 otlp_config: resource_attributes: attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 Docker Compose Example version: '3.8' services: loki: image: grafana/loki:3.0.0 container_name: loki ports: - \"3100:3100\" - \"9096:9096\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml - loki-data:/loki command: -config.file=/etc/loki/config.yaml networks: - battlebots-observability otel-collector: image: otel/opentelemetry-collector-contrib:0.96.0 container_name: otel-collector ports: - \"4317:4317\" # OTLP gRPC receiver - \"4318:4318\" # OTLP HTTP receiver - \"8888:8888\" # Metrics endpoint - \"13133:13133\" # Health check volumes: - ./otel-config.yaml:/etc/otelcol/config.yaml - /var/lib/otelcol:/var/lib/otelcol command: [\"--config=/etc/otelcol/config.yaml\"] depends_on: - loki networks: - battlebots-observability grafana: image: grafana/grafana:10.4.0 container_name: grafana ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin - GF_USERS_ALLOW_SIGN_UP=false volumes: - grafana-data:/var/lib/grafana - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml depends_on: - loki networks: - battlebots-observability volumes: loki-data: grafana-data: networks: battlebots-observability: driver: bridge Grafana Data Source Configuration # grafana-datasources.yaml apiVersion: 1 datasources: - name: Loki type: loki access: proxy url: http://loki:3100 jsonData: derivedFields: - datasourceUid: tempo matcherRegex: \"trace_id=(\\\\w+)\" name: TraceID url: \"$${__value.raw}\" Step-by-Step Setup Create configuration files:\nmkdir -p battlebots-observability cd battlebots-observability # Copy the configurations above into: # - loki-config.yaml # - otel-config.yaml # - grafana-datasources.yaml # - docker-compose.yaml Start the stack:\ndocker-compose up -d Verify Loki is running:\ncurl http://localhost:3100/ready # Expected: ready Verify OTel Collector is running:\ncurl http://localhost:13133 # Expected: {\"status\":\"Server available\"} Send test logs:\ncurl -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d '{ \"resourceLogs\": [{ \"resource\": { \"attributes\": [{ \"key\": \"service.name\", \"value\": {\"stringValue\": \"test-service\"} }] }, \"scopeLogs\": [{ \"logRecords\": [{ \"timeUnixNano\": \"1640000000000000000\", \"severityText\": \"INFO\", \"body\": {\"stringValue\": \"Test log message\"} }] }] }] }' Query logs in Grafana:\nOpen http://localhost:3000 Login with admin/admin Navigate to Explore Select Loki data source Query: {service_name=\"test-service\"} Troubleshooting Connection Errors Problem: Collector cannot connect to Loki OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: Post \"http://loki:3100/otlp/v1/logs\": dial tcp: lookup loki: no such host Solutions:\nVerify Loki is running: curl http://loki:3100/ready Check DNS resolution: nslookup loki (or use IP address) Verify network connectivity: telnet loki 3100 Check Docker network configuration if using containers Verify endpoint URL doesn’t include /v1/logs suffix Problem: 404 Not Found on OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: HTTP 404 Not Found Solutions:\nVerify Loki version is 3.0 or later: curl http://loki:3100/loki/api/v1/status/buildinfo Check schema version is v13 in Loki config Verify allow_structured_metadata: true in limits_config Restart Loki after configuration changes Label Cardinality Problems Problem: Too many streams causing performance degradation\nSymptoms:\nlevel=warn msg=\"stream limit exceeded\" limit=10000 streams=15234 Solutions:\nReview current label cardinality:\ncurl http://localhost:3100/loki/api/v1/label Check value distribution per label:\ncurl http://localhost:3100/loki/api/v1/label/service_name/values Move high-cardinality attributes to structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id Set cardinality limits:\nlimits_config: max_label_names_per_series: 15 max_label_value_length: 2048 max_label_name_length: 1024 Structured Metadata Issues Problem: Structured metadata not appearing in queries\nSymptoms: Attributes missing from log entries in Grafana\nSolutions:\nVerify schema version 13 or higher Check allow_structured_metadata: true in limits Verify attributes aren’t being dropped by processors Query with explicit structured metadata filter: {service_name=\"game-server\"} | k8s_pod_name=\"pod-123\" Performance Issues Problem: Slow queries or high memory usage\nSymptoms: Grafana queries timeout or Loki OOM errors\nSolutions:\nEnable query limits:\nlimits_config: max_query_series: 500 max_query_lookback: 720h max_entries_limit_per_query: 5000 Optimize batch processor:\nprocessors: batch: timeout: 5s # Reduce for faster flushing send_batch_size: 4096 # Smaller batches Add memory limiter:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 Use query acceleration:\nlimits_config: bloom_gateway_enable_filtering: true Debugging Techniques Enable debug logging in Collector:\nexporters: debug: verbosity: detailed service: pipelines: logs: receivers: [otlp] processors: [batch] exporters: [debug, otlphttp/logs] # Add debug exporter telemetry: logs: level: debug # Collector internal logs Enable debug logging in Loki:\nserver: log_level: debug Check Loki metrics:\ncurl http://localhost:3100/metrics | grep loki_distributor Verify OTLP data structure:\n# Send test log and capture response curl -v -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d @test-log.json BattleBots Integration Points Observability Stack Architecture The Loki OTLP integration fits into the BattleBots observability stack as follows:\nGame Servers (Go) └─\u003e OpenTelemetry SDK └─\u003e OTLP/gRPC (4317) └─\u003e OTel Collector ├─\u003e Loki (Logs via OTLP) ├─\u003e Tempo (Traces via OTLP) └─\u003e Prometheus (Metrics via OTLP) └─\u003e Grafana (Visualization \u0026 Correlation) Collector → Loki Pipeline for Game Servers Recommended pipeline configuration:\nReceive logs from game servers via OTLP Detect resource attributes (Kubernetes, cloud provider) Add BattleBots-specific labels (environment, service namespace) Filter out verbose debug logs in production Batch and compress for efficiency Export to Loki via OTLP HTTP Log Types and Labeling Strategy Game Event Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" event_type: \"game_event\" # Custom label Structured Metadata: match_id: \"uuid\" player_count: 4 game_mode: \"elimination\" System Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" log_type: \"system\" Structured Metadata: k8s_pod_name: \"game-server-abc123\" severity_text: \"ERROR\" Client Connection Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" Structured Metadata: client_id: \"uuid\" connection_state: \"connected\" trace_id: \"trace-id\" # For correlation Example Queries for BattleBots Find all errors in production:\n{service_namespace=\"battlebots\", deployment_environment=\"production\"} | severity_text=\"ERROR\" Find logs for a specific match:\n{service_name=\"game-server\"} | match_id=\"550e8400-e29b-41d4-a716-446655440000\" Find all logs related to a slow trace:\n{service_namespace=\"battlebots\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Count game events by type:\nsum by (event_type) ( count_over_time({service_name=\"game-server\"}[5m]) ) Further Reading OTLP Specification OpenTelemetry Protocol Specification - Complete OTLP specification OTLP Logs Data Model - OpenTelemetry logs data model OTLP Exporter Configuration - SDK configuration guide Grafana Loki OTLP Documentation Ingesting logs to Loki using OpenTelemetry Collector - Official Loki OTLP guide Getting started with OTel Collector and Loki - Step-by-step tutorial Native OTLP vs Loki Exporter - Migration guide Loki 3.0 Release Notes - Features announcement Migration to Native OTLP Format - Cloud migration guide OpenTelemetry Collector Documentation Collector Configuration - General configuration guide Collector Resiliency - Retry and queue configuration Configuration Best Practices - Security and performance Exporter Helper Documentation - Retry and queue details Loki Configuration and Operations Understanding Labels - Label strategy guide Structured Metadata - Structured metadata overview Loki Schema Configuration - Schema v13 setup Upgrade to Loki 3.0 - Migration instructions Grafana Integration Trace Integration in Explore - Log-trace correlation Trace Correlations - Setting up correlations Configure Loki Data Source - Data source configuration Community Guides and Tutorials Grafana Loki 101: Ingesting with OTel Collector - Official blog tutorial Building a Logging Pipeline with OTel, Loki, and Grafana - Docker Compose guide Logging with OpenTelemetry and Loki - Practical implementation Efficient Application Log Collection - Analysis guide Troubleshooting and Best Practices OTel Batching Best Practices - Batch configuration Collector Persistence and Retry - Deep dive Label Cardinality Issues - Common problems and solutions Related BattleBots Documentation OpenTelemetry Collector: Logs Support - Collector log processing OpenTelemetry Collector: Overview - Collector architecture ","categories":"","description":"Detailed guide for integrating Grafana Loki with OpenTelemetry Collector via native OTLP support.\n","excerpt":"Detailed guide for integrating Grafana Loki with OpenTelemetry …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/loki-otlp-integration/","tags":"","title":"Loki: OTLP Integration"},{"body":"Overview Grafana Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system designed to be cost-effective and easy to operate. Inspired by Prometheus, Loki takes a fundamentally different approach to log storage compared to traditional systems like Elasticsearch.\nThe core innovation of Loki is its index-free architecture: instead of indexing the full contents of log lines, Loki only indexes metadata labels for each log stream. This design dramatically reduces storage costs, memory requirements, and operational complexity while still enabling fast queries through label-based filtering and grep-style text search.\nLoki integrates seamlessly with Grafana for visualization, supports native OTLP ingestion (v3+), and works alongside Prometheus and Tempo to provide a complete observability stack. The system is built for cloud-native environments with first-class support for Kubernetes, containerized workloads, and distributed architectures.\nKey Concepts Understanding Loki’s data model is essential for effective deployment and usage.\nStreams A stream is the fundamental data structure in Loki, representing a sequence of log entries that share the same set of labels. Each unique combination of labels creates a new stream. For example:\n{app=\"battleserver\", env=\"prod\", region=\"us-east\"} {app=\"battleserver\", env=\"prod\", region=\"us-west\"} {app=\"matchmaker\", env=\"prod\", region=\"us-east\"} These three label sets create three distinct streams. All log entries with identical labels are appended to the same stream in chronological order.\nLabels Labels are key-value pairs that categorize and identify log streams. Labels are the only metadata indexed by Loki, making label design the most critical aspect of Loki deployment.\nLabels should be:\nLow cardinality: Use labels that have bounded, predictable values (e.g., environment, service, host) Descriptive: Represent the source or context of logs (e.g., namespace, pod, container) Static: Avoid labels that change frequently or have unique values per log entry Anti-pattern: Using high-cardinality values like user IDs, trace IDs, or timestamps as labels will severely degrade performance.\n# Good label design (low cardinality) {service=\"game-server\", environment=\"production\", region=\"us-east-1\"} # Bad label design (high cardinality) {trace_id=\"abc123\", user_id=\"user456\", timestamp=\"2025-12-03T10:00:00Z\"} Loki recommends keeping total stream count below 10,000 for small deployments and under 100,000 active streams for larger deployments.\nChunks Chunks are compressed blocks of log data from a single stream. As logs arrive for a stream, the ingester accumulates them in memory, then periodically flushes completed chunks to object storage.\nKey chunk characteristics:\nContain only data from a single stream Compressed using LZ4 or Snappy Stored in object storage (S3, GCS, MinIO, filesystem) Typically span minutes to hours of log data Subject to configurable size and time limits The chunk format optimizes for sequential reads, making time-range queries efficient.\nIndex The index stores the mapping between label sets and their corresponding chunks. Unlike full-text indexes, Loki’s index only contains:\nLabel names and values Chunk references (location, time range) Stream metadata This minimal indexing approach is what makes Loki cost-effective. The index is stored separately from chunks, typically in a different backend (BoltDB, TSDB).\nLogQL LogQL is Loki’s query language, inspired by Prometheus’s PromQL. LogQL queries have two stages:\nLog stream selection: Filter streams using label matchers Log pipeline: Parse, filter, and transform selected log lines # Select streams, then filter log content {service=\"game-server\"} |= \"error\" | json | level=\"ERROR\" # Aggregate metrics from logs rate({service=\"game-server\"} |= \"battle completed\" [5m]) # Multi-stage pipeline with parsing and filtering {namespace=\"battlebots\"} | json | line_format \"{{.message}}\" | pattern `\u003c_\u003e level=\u003clevel\u003e \u003c_\u003e` | level = \"error\" LogQL supports metric queries, allowing you to generate time-series data from logs (e.g., error rates, request counts).\nArchitecture Components Loki uses a microservices architecture where each component can be scaled independently or combined into larger deployment targets.\ngraph TB A[Log Clients\u003cbr/\u003ePromtail/Alloy/Fluentd] --\u003e|Push logs| B[Distributor] B --\u003e|Replicate| C1[Ingester 1] B --\u003e|Replicate| C2[Ingester 2] B --\u003e|Replicate| C3[Ingester N] C1 \u0026 C2 \u0026 C3 --\u003e|Flush chunks| D[Object Storage\u003cbr/\u003eS3/GCS/MinIO] C1 \u0026 C2 \u0026 C3 --\u003e|Update index| E[Index Store\u003cbr/\u003eBoltDB/TSDB] F[Grafana/API] --\u003e|Query| G[Query Frontend] G --\u003e|Split queries| H[Querier] H --\u003e|Read recent| C1 \u0026 C2 \u0026 C3 H --\u003e|Read historical| D H --\u003e|Read index| E I[Compactor] --\u003e|Compact| D I --\u003e|Update| E J[Ruler] --\u003e|Evaluate rules| H J --\u003e|Store| D style B fill:#fff4e6 style C1 fill:#fff4e6 style C2 fill:#fff4e6 style C3 fill:#fff4e6 style G fill:#e1f5ff style H fill:#e1f5ff style I fill:#f3e5f5 style J fill:#e8f5e9 style D fill:#fce4ec style E fill:#fce4ec Distributor The distributor is the entry point for log ingestion. It receives log streams from clients (Promtail, Alloy, OTLP endpoints) and routes them to ingesters.\nResponsibilities:\nValidate incoming log streams for correctness and tenant limits Apply rate limiting per tenant Hash log streams by labels to determine target ingesters Replicate each stream to multiple ingesters (default: 3 replicas) Load balance across available ingesters Distributors are stateless and can be horizontally scaled to handle high ingestion rates.\nIngester The ingester receives log streams from distributors and is responsible for:\nBuffering logs in memory for each stream Building compressed chunks Flushing chunks to object storage periodically Writing index entries Serving queries for recent (unflushed) data Ingesters maintain an in-memory index of recent logs and use a write-ahead log (WAL) for crash recovery. Upon graceful shutdown, ingesters flush all buffered data to storage.\nIngesters are stateful and require careful scaling considerations. They use consistent hashing to distribute streams evenly across instances.\nQuerier The querier executes LogQL queries by:\nFetching index data to identify relevant chunks Retrieving chunks from object storage Querying ingesters for recent unflushed data Merging results from multiple sources Applying log pipeline operations (parsing, filtering) Returning results to the query frontend Queriers are stateless and can be scaled horizontally. They cache chunk data and index lookups to improve performance.\nQuery Frontend The query frontend sits in front of queriers and provides:\nQuery splitting: Breaks large time-range queries into smaller sub-queries Query queuing: Prevents overwhelming queriers during traffic spikes Caching: Stores query results to avoid redundant computation Fair scheduling: Ensures multiple tenants share query resources equitably The frontend is optional but highly recommended for production deployments. It significantly improves query performance and protects backend components from overload.\nCompactor The compactor is a background service that:\nMerges small chunks into larger ones to improve query performance Removes duplicate data from replicated writes Applies retention policies by deleting old data Updates index to reflect compacted chunks Only one compactor should run per tenant to avoid conflicts. The compactor is critical for long-term storage efficiency.\nRuler The ruler evaluates recording rules and alerting rules against stored logs:\nRuns LogQL queries on a schedule Generates derived metrics from log data Triggers alerts based on log patterns Stores rule evaluation results The ruler is optional and typically used for log-based alerting scenarios.\nIndex Gateway The index gateway (available in recent versions) centralizes index access:\nProvides a single point for index queries Reduces load on the index store Enables better caching of index data Simplifies index backend scaling This component is particularly useful with BoltDB index backends to avoid direct file access from multiple queriers.\nDeployment Modes Loki supports three deployment modes, each balancing simplicity against scalability and operational flexibility.\nMonolithic Mode In monolithic mode, all Loki components run in a single process. This is the simplest deployment option.\nConfiguration:\nloki -target=all -config.file=loki-config.yaml Characteristics:\nSingle binary or container All components share memory and resources Minimal operational complexity Limited horizontal scalability Suitable for development and small deployments When to use:\nDevelopment and testing environments Proof-of-concept deployments Small-scale production (\u003c100GB/day log ingestion) Single-server deployments Limitations:\nCannot scale components independently Single point of failure Resource contention between components Limited to vertical scaling (bigger instances) Simple Scalable Deployment (SSD) Simple scalable deployment groups components into three logical targets: read, write, and backend.\nTargets:\nRead (-target=read): Query Frontend, Querier Write (-target=write): Distributor, Ingester Backend (-target=backend): Compactor, Ruler, Index Gateway Configuration example:\n# Write path (3 replicas for high availability) loki -target=write -config.file=loki-config.yaml # Read path (scale based on query load) loki -target=read -config.file=loki-config.yaml # Backend (single instance) loki -target=backend -config.file=loki-config.yaml Characteristics:\nSeparates read and write paths Independent scaling of ingestion vs queries Easier to operate than full microservices Supports ~1TB/day log ingestion When to use:\nMedium-scale production deployments When you need to scale reads and writes independently Kubernetes environments using Helm charts Teams wanting operational simplicity with scalability This is the recommended starting point for most production deployments.\nMicroservices Mode Microservices mode runs each Loki component as a separate deployment, providing maximum flexibility.\nComponents:\nDistributor (multiple instances) Ingester (multiple instances, stateful) Querier (multiple instances) Query Frontend (multiple instances) Compactor (single instance per tenant) Ruler (multiple instances) Index Gateway (multiple instances) Characteristics:\nEach component scaled independently Fine-grained resource allocation Most complex to deploy and maintain Supports enterprise-scale deployments (multi-TB/day) When to use:\nVery large Loki clusters (\u003e1TB/day) Organizations requiring precise control over scaling Multi-tenant SaaS deployments Teams with dedicated Loki operations expertise Considerations:\nSignificantly higher operational complexity More components to monitor and maintain Requires sophisticated orchestration (Kubernetes) Network communication overhead between components How to Run Loki This section provides practical guidance for running Loki, focusing on Docker/Podman Compose for POC and development.\nQuick Start with Docker Compose The simplest way to evaluate Loki is using the official Docker Compose example.\nStep 1: Create directory and download configurations\nmkdir loki-poc \u0026\u0026 cd loki-poc # Download Loki configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/loki-config.yaml # Download Alloy (log shipper) configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/alloy-local-config.yaml # Download Docker Compose file wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/docker-compose.yaml Step 2: Review the Docker Compose file\nThe compose file includes:\nLoki (monolithic mode) Grafana (for visualization) Grafana Alloy (log collection agent) flog (log generator for testing) Step 3: Start the stack\ndocker compose up -d Step 4: Verify deployment\n# Check Loki readiness curl http://localhost:3100/ready # Check Loki metrics curl http://localhost:3100/metrics # Access Grafana # URL: http://localhost:3000 # Default credentials: admin / admin Step 5: Query logs\nIn Grafana, navigate to Explore and select the Loki datasource to query logs using LogQL.\nBasic Configuration File Structure A minimal Loki configuration for local development:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /tmp/loki storage: filesystem: chunks_directory: /tmp/loki/chunks rules_directory: /tmp/loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24h limits_config: reject_old_samples: true reject_old_samples_max_age: 168h max_cache_freshness_per_query: 10m split_queries_by_interval: 15m query_range: align_queries_with_step: true cache_results: true ruler: alertmanager_url: http://localhost:9093 This configuration uses local filesystem storage and is suitable for development only.\nProduction Configuration with MinIO For a more production-like setup using MinIO as object storage:\nDocker Compose with MinIO:\nversion: \"3.8\" services: minio: image: minio/minio:latest entrypoint: - sh - -euc - | mkdir -p /data/loki-data minio server /data --console-address :9001 environment: - MINIO_ROOT_USER=loki - MINIO_ROOT_PASSWORD=supersecret - MINIO_PROMETHEUS_AUTH_TYPE=public ports: - \"9000:9000\" - \"9001:9001\" volumes: - minio-data:/data loki: image: grafana/loki:3.0.0 ports: - \"3100:3100\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml command: -config.file=/etc/loki/config.yaml depends_on: - minio grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin volumes: - grafana-data:/var/lib/grafana volumes: minio-data: grafana-data: Loki configuration with MinIO:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /loki storage: s3: endpoint: minio:9000 bucketnames: loki-data access_key_id: loki secret_access_key: supersecret s3forcepathstyle: true insecure: true replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 schema: v13 index: prefix: index_ period: 24h limits_config: ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 max_global_streams_per_user: 10000 max_query_length: 721h max_query_parallelism: 16 max_streams_per_user: 0 max_cache_freshness_per_query: 10m query_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 100 frontend: encoding: protobuf compress_responses: true max_outstanding_per_tenant: 2048 chunk_store_config: max_look_back_period: 0s table_manager: retention_deletes_enabled: true retention_period: 336h Resource Requirements Minimum requirements for development/POC:\nCPU: 2 cores Memory: 4 GB RAM Storage: 20 GB (filesystem) or object storage bucket Recommended production requirements (simple scalable mode):\nWrite path (per instance):\nCPU: 4-8 cores Memory: 8-16 GB RAM (for buffering chunks) Network: High bandwidth for ingestion Read path (per instance):\nCPU: 4-8 cores Memory: 16-32 GB RAM (for query caching) Network: High bandwidth for chunk retrieval Backend:\nCPU: 2-4 cores Memory: 4-8 GB RAM Storage: Object storage (S3, GCS, MinIO) with sufficient capacity for retention period Storage sizing:\nEstimate: ~5-10 GB/day per 1 million log lines (varies by compression ratio) Retention: storage_size = daily_volume * retention_days Index: ~1-2% of total chunk storage Getting Started Steps Choose deployment mode: Start with monolithic for POC, plan for simple scalable in production Set up object storage: MinIO for local dev, S3/GCS for production Configure Loki: Use appropriate schema version (v13 recommended) Deploy Loki: Docker Compose for POC, Helm for Kubernetes Configure log shippers: Alloy, Promtail, or OTLP endpoints Verify ingestion: Check /ready endpoint and metrics Set up Grafana: Add Loki datasource and create dashboards Test queries: Use LogQL to validate data retrieval Monitor Loki: Set up self-monitoring (metrics, logs, traces) Optimize configuration: Tune based on ingestion rate and query patterns Best Practices for Running Loki Successful Loki deployments depend on following these operational best practices.\nLabel Strategy 1. Keep labels low cardinality\nAim for 10-15 labels maximum across all streams. Each unique label combination creates a new stream.\n# Good: Low cardinality (bounded values) { service=\"game-server\", environment=\"production\", region=\"us-east-1\", cluster=\"battlebot-cluster-01\" } # Bad: High cardinality (unbounded values) { service=\"game-server\", trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", user_id=\"user_12345\", session_id=\"sess_67890\" } 2. Use static labels that describe log sources\nLabels should represent where logs come from, not what’s in them:\nnamespace, pod, container (Kubernetes) host, instance (infrastructure) service, application, component (application) environment, region, cluster (deployment context) 3. Avoid pod names and instance IDs as labels\nPod names and container IDs change frequently, creating stream churn:\n# Avoid {pod=\"game-server-abc123-xyz456\"} # Instead use {service=\"game-server\", namespace=\"battlebots\"} 4. Use filter expressions for high-cardinality data\nSearch for user IDs, trace IDs, or other high-cardinality values using LogQL filters:\n# Query for specific trace ID {service=\"game-server\"} |= \"trace_id=7a3f8c2d\" # Query for specific user {service=\"game-server\"} | json | user_id=\"12345\" 5. Use structured metadata for supplemental high-cardinality data\nLoki v2.9+ supports structured metadata, which stores high-cardinality data without indexing it:\n# Structured metadata (not indexed, not creating streams) labels: {service=\"game-server\"} structured_metadata: trace_id: \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" user_id: \"user_12345\" 6. Monitor stream count\nUse Loki metrics to track stream cardinality:\n# Total active streams loki_ingester_memory_streams # Streams per tenant sum by (tenant) (loki_ingester_memory_streams) Keep total streams under 10,000 for small deployments, under 100,000 for larger deployments.\nConfiguration Tips 1. Set appropriate limits\nlimits_config: # Rate limiting ingestion_rate_mb: 10 # MB/s per tenant ingestion_burst_size_mb: 20 # Burst allowance # Stream limits max_global_streams_per_user: 10000 # Total active streams max_line_size: 256000 # Bytes per log line max_entries_limit_per_query: 5000 # Max returned entries # Query limits max_query_length: 721h # 30 days max_query_parallelism: 16 # Concurrent query threads # Retention reject_old_samples: true reject_old_samples_max_age: 168h # 7 days 2. Configure retention\nlimits_config: retention_period: 744h # 31 days table_manager: retention_deletes_enabled: true retention_period: 744h Note: Retention requires compactor to be running.\n3. Enable caching\nquery_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 500 chunk_store_config: chunk_cache_config: embedded_cache: enabled: true max_size_mb: 1000 4. Use TSDB index (v13 schema)\nThe TSDB index (schema v13) offers better performance than BoltDB:\nschema_config: configs: - from: 2024-01-01 store: tsdb # Use TSDB object_store: s3 schema: v13 # Latest schema index: prefix: index_ period: 24h 5. Configure appropriate chunk settings\ningester: chunk_idle_period: 30m # Flush idle chunks after 30 min chunk_block_size: 262144 # 256 KB blocks chunk_encoding: snappy # Compression algorithm chunk_retain_period: 15m # Retain flushed chunks in memory max_chunk_age: 1h # Max time before forced flush wal: enabled: true dir: /loki/wal Storage Considerations 1. Choose appropriate object storage\nDevelopment: Filesystem or MinIO Production AWS: S3 with lifecycle policies Production GCP: GCS with object versioning Production Azure: Azure Blob Storage On-premises: MinIO cluster or compatible S3 service 2. Configure object storage lifecycle\nReduce storage costs by transitioning older data to cheaper tiers:\n# AWS S3 lifecycle example - Id: TransitionOldChunks Status: Enabled Transitions: - Days: 30 StorageClass: STANDARD_IA - Days: 90 StorageClass: GLACIER 3. Separate index and chunk storage\nFor better performance, use different backends:\nschema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 # Chunks in S3 schema: v13 index: prefix: index_ period: 24h storage_config: tsdb_shipper: active_index_directory: /loki/index cache_location: /loki/index_cache shared_store: s3 # Index in S3 aws: s3: s3://us-east-1/loki-chunks bucketnames: loki-chunks 4. Monitor storage usage\nTrack storage metrics to plan capacity:\n# Chunk storage rate rate(loki_ingester_chunk_stored_bytes_total[5m]) # Index entries created rate(loki_ingester_index_entries_total[5m]) Performance Tuning 1. Optimize ingestion\nUse batching in log shippers (Promtail, Alloy) Enable compression for network transport Scale distributors horizontally for high write load Scale ingesters based on stream count and retention 2. Optimize queries\nUse specific label matchers to reduce streams searched Limit query time ranges Use query frontend for splitting and caching Add parallelism for large queries # Good: Specific label selector, limited time range {service=\"game-server\", environment=\"prod\"} |= \"error\" [5m] # Suboptimal: Broad selector, large time range {environment=\"prod\"} [24h] 3. Use bloom filters (experimental)\nBloom filters can speed up log line filtering:\nbloom_compactor: enabled: true bloom_gateway: enabled: true 4. Tune querier parallelism\nquerier: max_concurrent: 10 # Concurrent queries per querier query_timeout: 1m # Per-query timeout limits_config: max_query_parallelism: 16 # Parallel workers per query Common Pitfalls to Avoid 1. High-cardinality labels\nProblem: Using trace IDs, user IDs, or timestamps as labels creates millions of streams.\nSolution: Use structured metadata or filter expressions instead.\n2. Not monitoring stream count\nProblem: Stream count grows unbounded, degrading performance.\nSolution: Monitor loki_ingester_memory_streams and set alerts at thresholds.\n3. Insufficient ingester memory\nProblem: Ingesters crash or flush chunks too frequently.\nSolution: Allocate 8-16 GB RAM per ingester, adjust max_chunk_age and chunk_idle_period.\n4. No retention policy\nProblem: Storage costs grow unbounded.\nSolution: Configure retention_period and enable compactor.\n5. Querying too much data\nProblem: Queries time out or overload queriers.\nSolution: Use query frontend, limit time ranges, add specific label selectors.\n6. Single ingester (no replication)\nProblem: Data loss during ingester failure.\nSolution: Set replication_factor: 3 in production.\n7. Using filesystem storage in production\nProblem: Data loss, no scalability, no durability.\nSolution: Always use object storage (S3, GCS, MinIO) for production.\n8. Not using WAL\nProblem: In-memory data lost on ingester crash.\nSolution: Enable write-ahead log:\ningester: wal: enabled: true dir: /loki/wal When to Use Loki Ideal Use Cases 1. Cloud-native and Kubernetes environments\nLoki excels in containerized environments with:\nAutomatic label extraction from Kubernetes metadata Efficient handling of ephemeral infrastructure Native Prometheus integration for unified observability 2. Cost-sensitive deployments\nLoki’s index-free architecture dramatically reduces:\nStorage costs (5-10x cheaper than Elasticsearch) Memory requirements Operational overhead 3. Integration with existing Prometheus/Grafana stacks\nIf you already use Prometheus and Grafana:\nUnified visualization across logs, metrics, and traces Similar query language (LogQL ~ PromQL) Consistent operational model 4. High-volume log aggregation with simple queries\nLoki handles massive log volumes efficiently when:\nQueries primarily filter by labels and time ranges Full-text search is limited to known patterns Aggregation and metrics-from-logs are common use cases 5. Correlation between logs, metrics, and traces\nLoki enables:\nLog-trace correlation via TraceID/SpanID Metrics extraction from logs Unified observability workflows in Grafana 6. Multi-tenant logging platforms\nLoki’s built-in multi-tenancy supports:\nIsolated log streams per tenant Per-tenant rate limiting and retention Shared infrastructure with tenant isolation Anti-Patterns 1. Complex full-text search requirements\nLoki is not a replacement for Elasticsearch when you need:\nAdvanced full-text search across all log content Complex query DSL with scoring and relevance Frequent regex searches without label filtering Ad-hoc exploratory searches without known labels 2. Frequent high-cardinality queries\nAvoid Loki if you regularly need to:\nSearch by unique identifiers (user IDs, session IDs) as primary access pattern Query without label-based filtering Perform analytics on unbounded dimensions 3. Long-term analytics and data warehouse use cases\nLoki is optimized for recent data access, not:\nHistorical data mining over years of logs Complex joins between log datasets Business intelligence and reporting workflows 4. Transactional workloads\nLoki does not provide:\nACID guarantees Immediate consistency for queries Strong durability guarantees (eventual consistency model) Loki vs. Elasticsearch Comparison Aspect Loki Elasticsearch Indexing Labels only (metadata) Full-text indexing Storage cost Low (index-free) High (full indexes) Memory usage Low High Query performance Fast for label-based queries Fast for full-text search Setup complexity Low Medium-high Operational overhead Low High Search capabilities Label filtering + grep-style Advanced full-text, DSL Best for Cloud-native, Kubernetes, cost-sensitive Enterprise search, analytics Scalability Horizontal (microservices) Horizontal (cluster) Multi-tenancy Built-in Via indexes/namespaces Integration Grafana, Prometheus, Tempo Kibana, Elastic ecosystem Decision Factors Choose Loki when:\nCost efficiency is a priority You have well-defined label taxonomy Logs are primarily time-series access patterns You use Kubernetes and Prometheus Queries filter by known dimensions (service, environment) Integration with Grafana is important Choose Elasticsearch when:\nComplex full-text search is required Ad-hoc exploratory queries are common Advanced analytics and aggregations are needed You need enterprise search capabilities Budget allows for higher infrastructure costs Team has existing ELK expertise BattleBots Integration Points For the BattleBots platform, Loki would serve as the centralized log storage backend in the observability stack.\nHow Loki Fits in the Observability Stack graph TB A[Game Servers\u003cbr/\u003eGo Services] --\u003e|Logs| B[OTel Collector] C[Infrastructure\u003cbr/\u003eContainers] --\u003e|Logs| B D[Application\u003cbr/\u003eLibraries] --\u003e|Logs| B B --\u003e|OTLP/HTTP| E[Loki\u003cbr/\u003eDistributor] E --\u003e F[Loki\u003cbr/\u003eIngesters] F --\u003e G[Object Storage\u003cbr/\u003eS3/MinIO] H[Grafana] --\u003e|LogQL| I[Loki\u003cbr/\u003eQuery Frontend] I --\u003e J[Loki\u003cbr/\u003eQueriers] J --\u003e F J --\u003e G B --\u003e|Metrics| K[Prometheus/Mimir] B --\u003e|Traces| L[Tempo] H -.Unified View.-\u003e K H -.Unified View.-\u003e L style B fill:#fff4e6 style E fill:#e1f5ff style F fill:#e1f5ff style H fill:#e8f5e9 style K fill:#f3e5f5 style L fill:#fce4ec Game Event Logging Use Cases 1. Battle event timeline\nLog all significant battle events with consistent labels:\n{service=\"battle-server\", battle_id=\"battle_123\"} | json | line_format \"{{.timestamp}} [{{.event_type}}] {{.description}}\" Example logs:\n2025-12-03T10:00:00Z [battle_start] Battle battle_123 initialized 2025-12-03T10:00:05Z [bot_action] Bot bot_456 executed attack on bot_789 2025-12-03T10:00:06Z [damage_calc] Bot bot_789 took 15 damage 2025-12-03T10:00:10Z [victory] Bot bot_456 won battle battle_123 2. Game state transitions\nTrack game state changes with structured logging:\n{ \"timestamp\": \"2025-12-03T10:00:00Z\", \"service\": \"game-server\", \"level\": \"info\", \"message\": \"Game state transition\", \"game_id\": \"game_123\", \"previous_state\": \"waiting_for_players\", \"new_state\": \"in_progress\", \"player_count\": 4 } Query pattern:\n{service=\"game-server\"} | json | message=\"Game state transition\" | game_id=\"game_123\" 3. Error tracking and debugging\nCapture errors with correlation to traces:\n{ \"timestamp\": \"2025-12-03T10:00:15Z\", \"service\": \"matchmaker\", \"level\": \"error\", \"message\": \"Failed to assign player to match\", \"error\": \"queue timeout exceeded\", \"trace_id\": \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", \"span_id\": \"8b4g9d3e-5f6g-22fd-92e4-1353bd241114\", \"player_id\": \"player_456\", \"queue_wait_time_ms\": 30000 } Query errors for a trace:\n{service=\"matchmaker\"} | json | trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" | level=\"error\" Server Log Aggregation Patterns 1. Label strategy for BattleBots\n# Standard labels for all services { service=\"battle-server\", # Service name environment=\"production\", # Deployment environment namespace=\"battlebots\", # Kubernetes namespace region=\"us-east-1\", # Deployment region version=\"v1.2.3\" # Application version } # Container-level labels (auto-discovered) { container=\"battle-server\", pod=\"battle-server-abc123-xyz456\", node=\"node-01\" } 2. Aggregating logs from multiple sources\n# All battle-related services {namespace=\"battlebots\"} | json | level=\"error\" # Specific service across all environments {service=\"matchmaker\"} | json # All production services in a region {environment=\"production\", region=\"us-east-1\"} | json 3. Metrics extraction from logs\nGenerate metrics from log data:\n# Battle completion rate rate({service=\"battle-server\"} |= \"battle completed\" [5m]) # Error rate by service sum by (service) (rate({namespace=\"battlebots\"} | json | level=\"error\" [5m])) # Average match duration avg_over_time({service=\"matchmaker\"} | json | unwrap duration_ms [5m]) Integration with OTel Collector Loki integrates with the OpenTelemetry Collector via native OTLP endpoints or exporters.\nOTel Collector configuration (brief example):\nexporters: otlphttp/loki: endpoint: http://loki:3100/otlp service: pipelines: logs: receivers: [otlp] processors: [batch, resourcedetection] exporters: [otlphttp/loki] For comprehensive OTel Collector integration details, see the dedicated Loki OTLP Integration document.\nFurther Reading Official Documentation Grafana Loki Documentation - Official documentation home Loki Architecture - Detailed architecture overview Loki Components - Component reference Loki Deployment Modes - Deployment mode comparison LogQL Language - Query language reference Label Best Practices - Label design guidelines Cardinality Management - Cardinality considerations Configuration Reference - Full configuration documentation Installation and Setup Install Loki with Docker - Docker and Docker Compose setup Quick Start Guide - Getting started tutorial Helm Installation - Kubernetes Helm charts Configure Storage - Storage backend configuration Best Practices and Guides How Labels Work in Loki - Label design deep dive The Concise Guide to Grafana Loki Labels - Comprehensive label guide Loki 2.4 Simple Scalable Deployment - Simple scalable mode introduction Grafana Loki Architecture Guide - Architecture deep dive Storage and Integration Using MinIO with Loki - MinIO integration guide Loki and MinIO Configuration - MinIO setup tutorial Storage Configuration - Storage backend options Comparisons and Decision Making Loki vs Elasticsearch - Detailed comparison Grafana Loki vs ELK Stack - Use case comparison Loki vs ELK: A Light Alternative - Lightweight alternative perspective Community and Source Code Loki GitHub Repository - Source code and issues Loki Examples - Configuration examples Grafana Community Forums - Community discussions Related BattleBots Documentation OpenTelemetry Collector Overview - OTel Collector architecture OTel Collector Logs - Log handling in OTel Collector Loki OTLP Integration - Detailed OTLP integration guide User Journey 0001: POC - Observability requirements ","categories":"","description":"Comprehensive overview of Grafana Loki log aggregation system, covering architecture, deployment modes, and operational best practices.\n","excerpt":"Comprehensive overview of Grafana Loki log aggregation system, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/logs/loki/loki-overview/","tags":"","title":"Loki: Overview"},{"body":"Overview Metrics storage is essential for the BattleBots platform’s observability infrastructure, enabling:\nReal-time monitoring of battle events and game state Historical analysis of bot performance and system behavior Capacity planning and infrastructure optimization Alerting on critical system conditions Long-term trend analysis and reporting The metrics storage backend must handle time-series data at scale, support efficient querying, and integrate seamlessly with the OpenTelemetry Collector to provide a unified observability platform alongside logs and traces.\nWhy Metrics Storage Matters for BattleBots The BattleBots platform generates metrics across multiple dimensions:\nGame Metrics Battle Events: Damage calculations, bot actions, victory conditions Performance Metrics: Bot response times, action execution latency Game State: Active battles, queued matches, player counts Resource Utilization: CPU, memory, and network usage per bot container Infrastructure Metrics Container Metrics: Resource usage, restart counts, health checks Host Metrics: Node-level CPU, memory, disk, and network utilization Network Metrics: Request rates, latency distributions, error rates Kubernetes Metrics: Pod status, deployments, scaling events Observability Stack Metrics OpenTelemetry Collector: Pipeline throughput, batch sizes, export success rates Log Storage: Ingestion rates, query performance, storage utilization Trace Storage: Span ingestion, sampling rates, trace completeness Components Grafana Mimir Research on Grafana Mimir, a horizontally scalable, highly available, multi-tenant metrics storage system built for long-term Prometheus data storage.\nMimir transforms Prometheus from a single-server monitoring system into a distributed platform capable of handling over 1 billion active time series, providing:\nNative OTLP Support: Direct integration with OpenTelemetry Collector via OTLP over HTTP Horizontal Scalability: Independent scaling of write path, read path, and backend components Long-Term Storage: Object storage backend (S3, GCS, MinIO) enables months to years of retention Multi-Tenancy: Built-in tenant isolation with per-tenant limits and resource controls High Availability: Replication and distributed architecture eliminate single points of failure PromQL Compatibility: Full Prometheus query language support for dashboards and alerts Includes detailed analysis of:\nArchitecture components and deployment modes Native OTLP ingestion and OpenTelemetry Collector integration Object storage backends and retention policies Multi-tenancy and cardinality management Comparison with Prometheus, Thanos, and Cortex Production deployment and operational best practices BattleBots Integration Context For the BattleBots platform, metrics storage serves as the foundation for understanding system behavior and performance:\nReal-Time Monitoring Monitor active battles and player engagement in real-time Alert on critical conditions (bot container failures, API errors, resource exhaustion) Track game server health and availability Identify performance degradation before user impact Historical Analysis Analyze battle outcome patterns and bot performance trends Capacity planning based on player growth and peak usage patterns Cost optimization through resource utilization analysis Root cause analysis for system incidents Unified Observability Metric-to-trace correlation through shared labels and exemplars Jumping from metric anomalies to related distributed traces Linking metrics to logs for comprehensive debugging Grafana dashboards combining metrics, logs, and traces in single views Example Metrics for BattleBots Bot Performance:\n# Bot action latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) # Bot health over time avg(bot_health_points) by (battle_id, bot_id) Infrastructure Health:\n# Container resource utilization container_memory_usage_bytes{namespace=\"battlebots\", container=\"game-server\"} / container_spec_memory_limit_bytes{namespace=\"battlebots\", container=\"game-server\"} # API request rate and errors sum(rate(http_requests_total{service=\"battle-api\"}[5m])) by (status_code) Game State:\n# Active battles sum(battles_active{environment=\"production\"}) # Player queue depth avg(player_queue_length) by (region) Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the metrics storage backend for BattleBots. Key decision factors include:\nFunctional Fit: Does the solution meet metrics storage, query, and correlation requirements? Scalability: Can it handle expected growth from POC to production scale? Integration: How well does it integrate with OpenTelemetry Collector, Grafana, and other observability components? Operational Complexity: What is the operational burden for deployment, monitoring, and maintenance? Cost: What are the infrastructure and operational costs at POC and production scale? Multi-Tenancy: Does it support per-player or per-battle isolation if needed? Related Documentation R\u0026D Documentation Observability Overview - Parent observability analysis section OpenTelemetry Collector Analysis - Metrics collection and processing Loki Analysis - Log storage for correlation with metrics User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources Prometheus Documentation PromQL Query Language OpenTelemetry Metrics Specification Grafana Metrics Documentation Contributing These analysis documents are living documents that should be updated as:\nNew metrics storage solutions emerge or mature BattleBots observability requirements evolve Team members gain operational experience with metrics backends Best practices and patterns are discovered Comparative analysis reveals new insights Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Analysis of metrics storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\n","excerpt":"Analysis of metrics storage backends for the BattleBots observability …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/metrics/","tags":"","title":"Metrics Storage"},{"body":"Overview This document provides comprehensive guidance on integrating Grafana Mimir with the OpenTelemetry Collector, addressing two critical questions:\nDoes Mimir work with OTLP? → YES - Native OTLP ingestion since version 2.3.0 Can it be integrated with the OTel Collector? → YES - Full integration via otlphttp or prometheusremotewrite exporters The integration enables a vendor-neutral observability pipeline where the OpenTelemetry Collector collects metrics from instrumented applications and forwards them to Mimir for long-term storage, querying, and alerting.\nWhy OTLP Matters for Metrics OpenTelemetry Protocol (OTLP) is the native protocol of the OpenTelemetry project, designed as a vendor-neutral standard for telemetry data transmission. Using OTLP with Mimir provides:\nFuture-Proof: OTLP is becoming the industry standard for telemetry data Simplified Pipeline: No protocol translation required (OTel → OTLP → Mimir) Resource Attributes: OTel resource attributes preserved in target_info metric Unified Stack: Same protocol for logs (Loki), metrics (Mimir), and traces (Tempo) Vendor Independence: Easy migration between OTLP-compatible backends Mimir’s Position in the OTLP Ecosystem Mimir acts as an OTLP-compatible metrics backend, receiving metrics via:\nPrimary Path: OpenTelemetry Collector → OTLP/HTTP → Mimir /otlp/v1/metrics endpoint Alternative Path: OpenTelemetry Collector → Prometheus Remote Write → Mimir /api/v1/push endpoint Both paths are fully supported, with OTLP recommended by Grafana for new deployments.\nOTLP Support in Mimir Native OTLP Support: YES Status: Grafana Mimir has native OTLP ingestion support.\nEndpoint: /otlp/v1/metrics\nVersion History:\nv2.3.0 (September 2022): OTLP support introduced (experimental) v2.15.0 (January 2025): OTLP support matured (no longer experimental) Removed experimental -distributor.direct-otlp-translation-enabled flag Added support for lz4 compression Added support for integer exemplar values v3.0.0 (October 2025): Additional OTLP enhancements Experimental -distributor.otel-translation-strategy flag for metric name translation Experimental -distributor.otel-native-delta-ingestion for native delta metric ingestion Protocol Support:\nOTLP over HTTP: Primary protocol (recommended) Encoding: Protocol Buffers Compression: GZIP and lz4 OTLP Endpoint Configuration Endpoint URL The OTLP endpoint in Mimir is:\nhttp://\u003cmimir-endpoint\u003e/otlp/v1/metrics Important: OpenTelemetry Collector clients automatically append /v1/metrics to the base path, so you only need to configure:\nhttp://\u003cmimir-endpoint\u003e/otlp Example Requests Using curl:\n# Send OTLP metrics to Mimir curl -X POST http://mimir:8080/otlp/v1/metrics \\ -H \"Content-Type: application/x-protobuf\" \\ -H \"X-Scope-OrgID: tenant-123\" \\ --data-binary @metrics.pb Using OpenTelemetry Collector:\nexporters: otlphttp: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: tenant-123 OTLP Features Supported Metric Types:\n✅ Gauge: Point-in-time values ✅ Sum (Counter): Cumulative or delta monotonic sums ✅ Histogram: Distribution of values with buckets ⚠️ Exponential Histogram: Requires enabling Prometheus Native Histograms first ✅ Summary: Pre-computed quantiles (compatibility mode) Resource Attributes:\n✅ Promoted Attributes: Converted to Prometheus labels (e.g., service.name → service_name) ✅ Target Info: Non-promoted attributes stored in separate target_info metric ✅ Queryable: Use info() function or join queries to access resource attributes Exemplars:\n✅ Metric Exemplars: Link metrics to traces via trace_id and span_id ✅ Integer Values: Support for integer exemplar values (v2.15.0+) Compression:\n✅ GZIP: Standard compression ✅ lz4: Faster compression (v2.15.0+) OTLP vs. Prometheus Remote Write Both protocols are supported by Mimir. Here’s a detailed comparison:\nFeature OTLP Prometheus Remote Write Grafana Recommendation ✅ Recommended Alternative Resource Attributes ✅ Stored in target_info ❌ Lost during conversion Bandwidth Efficiency Moderate ✅ Better (remote write 2.0: 40% reduction) Native Protocol ✅ OpenTelemetry native Prometheus native Exponential Histograms Requires config N/A (not applicable) Protocol Maturity Mature (since v2.15.0) Very Mature Configuration Complexity Simple Simple Future-Proof ✅ Industry standard Prometheus ecosystem Official Recommendation from Grafana:\n“It’s recommended that you use the OpenTelemetry protocol (OTLP).”\nGrafana Mimir Documentation Why Use OTLP Over Prometheus Remote Write? Advantages of OTLP:\nResource Attributes Preserved: Mimir stores OTel resource attributes in target_info metric, enabling rich context Native OpenTelemetry: Direct protocol compliance without translation Future Development: Active OTLP feature development in each Mimir release Unified Observability: Same protocol for logs (Loki), metrics (Mimir), traces (Tempo) Vendor Neutrality: Easy migration between OTLP-compatible backends When to Use Prometheus Remote Write:\nBandwidth Critical: Remote write 2.0 saves 40% bandwidth vs. remote write 1.0 Existing Prometheus Infrastructure: Already using Prometheus remote write Compatibility Issues: If encountering specific OTLP compatibility issues (rare) For BattleBots: Use OTLP as the primary integration path for future-proof observability.\nKnown OTLP Limitations (Minor) Request Size Units: OTel Collector uses samples-per-batch, Mimir uses bytes-per-batch (alignment difficult) Exponential Histograms: Must enable Prometheus Native Histograms in Mimir first Out-of-Order Samples: No ordering guarantees (can cause issues with Prometheus’s ordered data expectations) Response Format: Error responses don’t fully comply with OTLP spec (returns plain string vs. Protobuf Status) These are minor edge cases that don’t affect typical deployments.\nOpenTelemetry Collector Configuration This section provides complete configuration examples for integrating the OpenTelemetry Collector with Mimir.\nArchitecture Overview graph LR subgraph \"Application\" App[Instrumented App\u003cbr/\u003eOpenTelemetry SDK] end subgraph \"Collection\" OTel[OpenTelemetry Collector] App --\u003e|OTLP| OTel end subgraph \"Processing\" OTel --\u003e Batch[Batch Processor] Batch --\u003e Resource[Resource Processor] end subgraph \"Storage\" Resource --\u003e|OTLP HTTP| Mimir[(Grafana Mimir)] end subgraph \"Visualization\" Grafana[Grafana] --\u003e Mimir end style OTel fill:#e1f5ff style Mimir fill:#fff4e1 style Grafana fill:#ffe0b2 Option 1: OTLP HTTP Exporter (Recommended) The otlphttp exporter sends metrics using the native OTLP over HTTP protocol.\nBasic Configuration receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: batch: send_batch_size: 8192 timeout: 10s exporters: otlphttp: endpoint: http://mimir:8080/otlp compression: gzip service: pipelines: metrics: receivers: [otlp] processors: [batch] exporters: [otlphttp] Production Configuration with Authentication extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] timeout: 5s override: false resource: attributes: - key: environment value: ${ENVIRONMENT} action: upsert - key: cluster value: ${CLUSTER_NAME} action: upsert exporters: otlphttp: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/otlp timeout: 30s compression: gzip retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 headers: X-Scope-OrgID: ${MIMIR_TENANT_ID} service: extensions: [basicauth] pipelines: metrics: receivers: [otlp] processors: [memory_limiter, resourcedetection, resource, batch] exporters: [otlphttp] Option 2: Prometheus Remote Write Exporter The prometheusremotewrite exporter sends metrics using the Prometheus Remote Write protocol.\nBasic Configuration receivers: otlp: protocols: http: endpoint: 0.0.0.0:4318 processors: batch: send_batch_size: 8192 timeout: 10s exporters: prometheusremotewrite: endpoint: http://mimir:8080/api/v1/push service: pipelines: metrics: receivers: [otlp] processors: [batch] exporters: [prometheusremotewrite] Production Configuration extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] resource: attributes: - key: environment value: ${ENVIRONMENT} action: upsert exporters: prometheusremotewrite: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/api/v1/push # External labels added to all metrics external_labels: cluster: ${CLUSTER_NAME} environment: ${ENVIRONMENT} # Resource to telemetry conversion resource_to_telemetry_conversion: enabled: true # Target info generation target_info: enabled: true # Retry configuration retry_on_failure: enabled: true initial_interval: 1s max_interval: 30s max_elapsed_time: 1800s # Queue configuration remote_write_queue: enabled: true queue_size: 10000 num_consumers: 5 # WAL (Write-Ahead Log) for durability wal: directory: /var/lib/otelcol/wal buffer_size: 300 truncate_frequency: 1m service: extensions: [basicauth] pipelines: metrics: receivers: [otlp] processors: [memory_limiter, resourcedetection, resource, batch] exporters: [prometheusremotewrite] Batch Processor Configuration The batch processor is CRITICAL for production deployments. It improves efficiency by batching metrics before export.\nRecommended Settings processors: batch: # Number of items to batch before sending send_batch_size: 8192 # Default, works for most cases # Maximum time to wait before sending (even if batch not full) timeout: 10s # Maximum batch size (safety valve) send_batch_max_size: 10000 # Maximum metadata keys per batch metadata_keys: 1000 # Maximum cardinality per metadata key metadata_cardinality_limit: 1000 Tuning Guidelines High-Throughput Deployments:\nprocessors: batch: send_batch_size: 16384 # 2x default timeout: 5s # Shorter timeout for lower latency send_batch_max_size: 20000 Low-Latency Requirements:\nprocessors: batch: send_batch_size: 1000 # Smaller batches timeout: 1s # Quick flush send_batch_max_size: 2000 Key Principles:\nAlways use batch processor before network-based exporters Place after memory_limiter in pipeline Balance latency vs. efficiency: Larger batches = better compression but higher latency Retry and Timeout Configuration Configure retry logic to handle transient failures:\nexporters: otlphttp: # Request timeout timeout: 30s # Retry configuration retry_on_failure: enabled: true initial_interval: 5s # Wait 5s before first retry max_interval: 30s # Cap exponential backoff at 30s max_elapsed_time: 300s # Give up after 5 minutes # Sending queue (buffer during retries) sending_queue: enabled: true num_consumers: 10 # Parallel export workers queue_size: 5000 # Buffer size Guidelines:\ninitial_interval: Start with 1-5 seconds (avoid retry storms) max_interval: Cap at 30-60 seconds (prevent infinite backoff) max_elapsed_time: 300s (5 min): Low-latency, loss-tolerant 1800s (30 min): Standard production 3600s+ (1+ hour): Critical data that cannot be lost num_consumers: More consumers = more parallel requests (ensure backend can handle load) queue_size: Balance memory vs. buffering capacity Resource Attribute Mapping OpenTelemetry resource attributes provide context about the source of metrics. Mimir handles these attributes through two mechanisms:\nPromoted Attributes Certain resource attributes are automatically converted to Prometheus labels:\nDefault Promoted Attributes:\nservice.namespace + service.name → job label service.instance.id → instance label Example:\n# OpenTelemetry resource attributes resource: service.namespace: \"battlebots\" service.name: \"battle-api\" service.instance.id: \"pod-abc123\" # Resulting Prometheus labels { job=\"battlebots/battle-api\", instance=\"pod-abc123\" } Target Info Metric Non-promoted resource attributes are stored in a separate target_info metric:\nExample:\n# OpenTelemetry resource attributes resource: service.name: \"battle-api\" service.version: \"v1.2.3\" k8s.namespace.name: \"battlebots\" k8s.pod.name: \"battle-api-abc123\" k8s.deployment.name: \"battle-api\" cloud.provider: \"aws\" cloud.region: \"us-east-1\" # target_info metric created target_info{ job=\"battle-api\", instance=\"pod-abc123\", service_version=\"v1.2.3\", k8s_namespace_name=\"battlebots\", k8s_pod_name=\"battle-api-abc123\", k8s_deployment_name=\"battle-api\", cloud_provider=\"aws\", cloud_region=\"us-east-1\" } 1 Querying with Resource Attributes Direct Query (promoted attributes):\nhttp_requests_total{job=\"battlebots/battle-api\"} Join Query (non-promoted attributes):\n# Join metric with target_info to access resource attributes http_requests_total * on(job, instance) group_left(k8s_namespace_name, k8s_pod_name) target_info Using info() Function (Prometheus 3.0+):\n# Simpler syntax for joining with target_info http_requests_total * info(target_info) Label Name Conversion Prometheus labels don’t support . or - characters. OpenTelemetry attributes are converted:\nservice.name → service_name k8s-cluster → k8s_cluster http.method → http_method Resource Processor for Attribute Transformation Use the resource processor to add, modify, or remove resource attributes:\nprocessors: resource: attributes: # Add new attribute - key: environment value: production action: insert # Update existing attribute - key: service.version value: v2.0.0 action: update # Insert or update (upsert) - key: cluster value: us-east-1-prod action: upsert # Rename attribute - key: cluster_name from_attribute: k8s.cluster.name action: insert # Delete attribute - key: sensitive.data action: delete # Extract with regex - key: environment pattern: ^(dev|staging|prod)-.*$ action: extract Resource Detection Processor Automatically detect resource attributes from the environment:\nprocessors: resourcedetection: # Ordered list of detectors (first match wins) detectors: [env, system, docker, gcp, ec2, azure, k8s] timeout: 5s override: false # Don't overwrite existing attributes # System detector configuration system: hostname_sources: [\"os\", \"dns\", \"cname\", \"lookup\"] # Docker detector docker: resource_attributes: host.name: enabled: true os.type: enabled: true # GCP detector gcp: resource_attributes: gcp.project.id: enabled: true cloud.platform: enabled: true cloud.region: enabled: true # Kubernetes detector k8s: resource_attributes: k8s.namespace.name: enabled: true k8s.pod.name: enabled: true k8s.deployment.name: enabled: true k8s.node.name: enabled: true Detected Attributes by Detector:\nDetector Attributes env Reads from OTEL_RESOURCE_ATTRIBUTES environment variable system host.name, host.id, host.arch, os.type docker host.name, os.type from Docker environment gcp cloud.provider, cloud.platform, cloud.region, gcp.project.id, gcp.gce.instance.id ec2 cloud.provider, cloud.platform, cloud.region, cloud.account.id, host.id, host.type azure cloud.provider, cloud.platform, cloud.region, azure.vm.name, azure.resourcegroup.name k8s k8s.namespace.name, k8s.pod.name, k8s.deployment.name, k8s.node.name, k8s.cluster.name Label Strategy and Cardinality Control Managing cardinality is critical when using OpenTelemetry with Mimir. High cardinality can cause performance issues and increased costs.\nUnderstanding Cardinality in OTel Context Every unique combination of metric name + label key-value pairs = one time series.\nLow Cardinality (Good):\n# Resource attributes service.name: \"battle-api\" # Limited number of services environment: \"production\" # 3 values: dev, staging, prod region: \"us-east-1\" # Limited AWS regions # Metric-level attributes http.method: \"GET\" # ~7 HTTP methods http.status_code: \"200\" # ~50 HTTP status codes # Total series per metric = 10 services × 3 environments × 5 regions × 7 methods × 50 status codes = 52,500 series (ACCEPTABLE) High Cardinality (Bad):\n# Adding unbounded attributes user.id: \"user-12345\" # Millions of users # New total series = 52,500 × 1,000,000 users = 52.5 billion series (UNSUSTAINABLE!) Cardinality Best Practices 1. Avoid Unbounded Resource Attributes:\nBad:\nresource: user.id: \"12345\" # Unbounded session.id: \"abc-def-ghi\" # Unbounded request.id: \"uuid-...\" # Unbounded timestamp: \"1634567890\" # Unbounded Good:\nresource: service.name: \"battle-api\" # Bounded environment: \"production\" # Bounded (3 values) region: \"us-east-1\" # Bounded (AWS regions) version: \"v1.2.3\" # Bounded (release versions) 2. Use Metric Attributes Sparingly:\nOpenTelemetry SDK allows setting attributes on individual metric data points. Use bounded sets only:\n// Good: Bounded attribute meter.NewInt64Counter(\"http.requests\", metric.WithDescription(\"HTTP requests\"), ).Add(ctx, 1, attribute.String(\"method\", \"GET\"), // ~7 values attribute.Int(\"status_code\", 200), // ~50 values ) // Bad: Unbounded attribute meter.NewInt64Counter(\"http.requests\").Add(ctx, 1, attribute.String(\"user_id\", userID), // Unbounded! ) 3. Drop High-Cardinality Attributes:\nUse the resource processor to remove problematic attributes:\nprocessors: resource: attributes: # Drop user-specific attributes - key: user.id action: delete - key: session.id action: delete - key: request.id action: delete 4. Aggregate Before Storing:\nFor user-specific metrics, aggregate at collection time:\n// Instead of per-user metrics: // user_requests{user_id=\"123\"} → High cardinality // Use aggregated metrics: // requests_by_service{service=\"api\"} → Low cardinality // Then query logs for user-specific debugging Attribute Transformation Strategies Strategy 1: Bounded Enumeration\nConvert unbounded values to bounded categories:\nprocessors: attributes: actions: # Convert specific status codes to categories - key: http.status_code action: update pattern: ^2\\d\\d$ value: \"2xx\" - key: http.status_code action: update pattern: ^4\\d\\d$ value: \"4xx\" - key: http.status_code action: update pattern: ^5\\d\\d$ value: \"5xx\" Strategy 2: Drop After Threshold\nOnly keep top N values, drop the rest:\nThis requires custom collector processing or accept all values with limits configured in Mimir:\n# Mimir limits limits: max_global_series_per_metric: 50000 # Cap per metric Recommended Labels for BattleBots Infrastructure Labels:\nresource: service.name: \"battle-api\" # Service name service.namespace: \"battlebots\" # Application namespace service.instance.id: \"pod-abc123\" # Pod/container ID service.version: \"v1.2.3\" # Release version deployment.environment: \"production\" # dev/staging/prod cloud.region: \"us-east-1\" # Geographic region k8s.cluster.name: \"us-east-1-prod\" # Cluster identifier k8s.namespace.name: \"battlebots\" # Kubernetes namespace Game-Specific Labels (metric-level attributes):\n# Battle events battle.type: \"team-deathmatch\" # Limited game modes bot.type: \"tank\" # Enumerable bot types game.region: \"us-east\" # Geographic game region # DO NOT USE AS LABELS: # battle.id: \"12345\" # If battles are long-lived and accumulate # player.id: \"user-abc\" # High cardinality # bot.id: \"bot-xyz\" # High cardinality if bots are per-player Estimated Cardinality:\nhttp_requests_total{ service_name: 10 services environment: 3 environments region: 5 regions method: 7 methods status_code: 50 codes endpoint: 50 endpoints battle_type: 5 game modes } = 1 × 10 × 3 × 5 × 7 × 50 × 50 × 5 = 13,125,000 series # Acceptable for Mimir Authentication and Multi-Tenancy Basic Authentication Use the basicauth extension for username/password authentication:\nextensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} exporters: otlphttp: auth: authenticator: basicauth endpoint: http://mimir:8080/otlp service: extensions: [basicauth] pipelines: metrics: exporters: [otlphttp] Bearer Token Authentication Use the bearertoken extension for token-based auth:\nStatic Token:\nextensions: bearertoken: token: ${MIMIR_API_TOKEN} exporters: otlphttp: auth: authenticator: bearertoken endpoint: http://mimir:8080/otlp service: extensions: [bearertoken] pipelines: metrics: exporters: [otlphttp] Token from File (rotating tokens):\nextensions: bearertoken: filename: /var/run/secrets/mimir-token exporters: otlphttp: auth: authenticator: bearertoken endpoint: http://mimir:8080/otlp service: extensions: [bearertoken] pipelines: metrics: exporters: [otlphttp] Multi-Tenancy with X-Scope-OrgID Mimir uses the X-Scope-OrgID header for tenant identification:\nSingle Tenant:\nexporters: otlphttp: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-production\" Multiple Tenants (separate pipelines):\nexporters: otlphttp/tenant1: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-1\" otlphttp/tenant2: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-2\" service: pipelines: metrics/tenant1: receivers: [otlp] processors: [batch] exporters: [otlphttp/tenant1] metrics/tenant2: receivers: [otlp] processors: [batch] exporters: [otlphttp/tenant2] Dynamic Tenant Routing:\nFor dynamic tenant routing based on resource attributes, use the routing processor:\nprocessors: routing: from_attribute: tenant.id default_exporters: [otlphttp/default] table: - value: \"tenant-1\" exporters: [otlphttp/tenant1] - value: \"tenant-2\" exporters: [otlphttp/tenant2] exporters: otlphttp/tenant1: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-1\" otlphttp/tenant2: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-2\" otlphttp/default: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"default\" service: pipelines: metrics: receivers: [otlp] processors: [routing, batch] exporters: [otlphttp/tenant1, otlphttp/tenant2, otlphttp/default] TLS Configuration Enable TLS for secure communication:\nexporters: otlphttp: endpoint: https://mimir.example.com/otlp tls: insecure: false # Verify server certificate cert_file: /path/to/client-cert.pem # Client certificate key_file: /path/to/client-key.pem # Client private key ca_file: /path/to/ca-cert.pem # CA certificate for server verification Mutual TLS (mTLS):\nexporters: otlphttp: endpoint: https://mimir.example.com/otlp tls: insecure: false cert_file: /path/to/client-cert.pem key_file: /path/to/client-key.pem ca_file: /path/to/ca-cert.pem min_version: \"1.2\" # Minimum TLS version max_version: \"1.3\" # Maximum TLS version Complete Configuration Example This example demonstrates a complete, production-ready OpenTelemetry Collector configuration for BattleBots integrating with Mimir.\n# OpenTelemetry Collector Configuration for BattleBots + Mimir # Production-ready configuration with OTLP HTTP exporter extensions: # Health check endpoint health_check: endpoint: 0.0.0.0:13133 # Memory ballast to reduce GC pressure memory_ballast: size_mib: 165 # 1/3 of memory_limiter limit_mib # Authentication for Mimir basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: # OTLP receiver for metrics from instrumented apps otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 # Prometheus receiver for scraping Prometheus exporters prometheus: config: scrape_configs: # Scrape OpenTelemetry Collector's own metrics - job_name: 'otel-collector' scrape_interval: 30s static_configs: - targets: ['localhost:8888'] # Scrape BattleBots services - job_name: 'battlebots' kubernetes_sd_configs: - role: pod namespaces: names: ['battlebots'] relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ processors: # Memory limiter (MUST be first in pipeline) memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 # 25% of limit_mib # Resource detection resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] timeout: 5s override: false # Resource attribute transformation resource: attributes: # Add deployment environment - key: deployment.environment value: ${ENVIRONMENT} action: upsert # Add cluster name - key: cluster.name value: ${CLUSTER_NAME} action: upsert # Add BattleBots platform identifier - key: platform value: battlebots action: upsert # Add version - key: version value: ${APP_VERSION} action: upsert # Remove sensitive attributes - key: host.id action: delete # Filter processor (optional - drop unwanted metrics) filter: metrics: exclude: match_type: strict metric_names: - unwanted_metric_1 - unwanted_metric_2 # Batch processor (CRITICAL for production) batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 exporters: # Primary: OTLP HTTP to Mimir otlphttp: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/otlp timeout: 30s compression: gzip retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 headers: X-Scope-OrgID: ${MIMIR_TENANT_ID} # Debug: Logging exporter (disable in production) logging: loglevel: info sampling_initial: 5 sampling_thereafter: 200 service: extensions: [health_check, memory_ballast, basicauth] # Collector's own telemetry telemetry: logs: level: info metrics: address: 0.0.0.0:8888 level: detailed pipelines: # Metrics pipeline metrics: receivers: [otlp, prometheus] processors: [memory_limiter, resourcedetection, resource, filter, batch] exporters: [otlphttp] # Optional: Separate pipeline for debugging # metrics/debug: # receivers: [otlp] # processors: [batch] # exporters: [logging] Environment Variables # Mimir connection export MIMIR_ENDPOINT=\"http://mimir-gateway:8080\" export MIMIR_USERNAME=\"battlebots-collector\" export MIMIR_PASSWORD=\"supersecret\" export MIMIR_TENANT_ID=\"battlebots-production\" # Application metadata export ENVIRONMENT=\"production\" export CLUSTER_NAME=\"us-east-1-prod\" export APP_VERSION=\"v1.2.3\" # Memory configuration export GOMEMLIMIT=\"410MiB\" # 80% of memory_limiter limit_mib Kubernetes Deployment apiVersion: v1 kind: ConfigMap metadata: name: otel-collector-config namespace: battlebots data: config.yaml: | # Paste complete configuration from above --- apiVersion: v1 kind: Secret metadata: name: mimir-credentials namespace: battlebots type: Opaque stringData: username: battlebots-collector password: supersecret --- apiVersion: apps/v1 kind: Deployment metadata: name: otel-collector namespace: battlebots spec: replicas: 3 selector: matchLabels: app: otel-collector template: metadata: labels: app: otel-collector spec: containers: - name: otel-collector image: otel/opentelemetry-collector-contrib:0.115.0 args: - --config=/conf/config.yaml env: - name: GOMEMLIMIT value: \"410MiB\" - name: MIMIR_ENDPOINT value: \"http://mimir-gateway.mimir.svc.cluster.local:8080\" - name: MIMIR_USERNAME valueFrom: secretKeyRef: name: mimir-credentials key: username - name: MIMIR_PASSWORD valueFrom: secretKeyRef: name: mimir-credentials key: password - name: MIMIR_TENANT_ID value: \"battlebots-production\" - name: ENVIRONMENT value: \"production\" - name: CLUSTER_NAME value: \"us-east-1-prod\" - name: APP_VERSION value: \"v1.2.3\" resources: limits: memory: 512Mi cpu: 500m requests: memory: 256Mi cpu: 200m ports: - containerPort: 4317 # OTLP gRPC name: otlp-grpc - containerPort: 4318 # OTLP HTTP name: otlp-http - containerPort: 8888 # Metrics name: metrics - containerPort: 13133 # Health check name: health livenessProbe: httpGet: path: / port: health readinessProbe: httpGet: path: / port: health volumeMounts: - name: config mountPath: /conf volumes: - name: config configMap: name: otel-collector-config --- apiVersion: v1 kind: Service metadata: name: otel-collector namespace: battlebots spec: selector: app: otel-collector ports: - name: otlp-grpc port: 4317 targetPort: 4317 - name: otlp-http port: 4318 targetPort: 4318 - name: metrics port: 8888 targetPort: 8888 Docker Compose Example version: '3.8' services: otel-collector: image: otel/opentelemetry-collector-contrib:0.115.0 command: [\"--config=/etc/otelcol/config.yaml\"] environment: - GOMEMLIMIT=410MiB - MIMIR_ENDPOINT=http://mimir:8080 - MIMIR_USERNAME=battlebots - MIMIR_PASSWORD=supersecret - MIMIR_TENANT_ID=battlebots - ENVIRONMENT=development - CLUSTER_NAME=local - APP_VERSION=v1.0.0 ports: - \"4317:4317\" # OTLP gRPC - \"4318:4318\" # OTLP HTTP - \"8888:8888\" # Metrics - \"13133:13133\" # Health check volumes: - ./otel-config.yaml:/etc/otelcol/config.yaml networks: - battlebots mimir: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] ports: - \"8080:8080\" volumes: - ./mimir-config.yaml:/etc/mimir.yaml - mimir-data:/data networks: - battlebots grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin networks: - battlebots networks: battlebots: volumes: mimir-data: Troubleshooting Common Issues and Solutions 1. Connection Refused / Network Errors Symptoms:\nfailed to export metrics: connection refused Diagnosis:\nVerify Mimir is running and reachable:\ncurl http://mimir:8080/ready Check network connectivity:\n# From collector pod/container nc -zv mimir 8080 Verify endpoint configuration:\n# Ensure correct endpoint format endpoint: http://mimir:8080/otlp # Correct # NOT: http://mimir:8080/otlp/v1/metrics (client auto-appends) Solutions:\nFix network policies/firewall rules Verify Kubernetes service DNS resolution Check load balancer configuration Ensure Mimir port 8080 is exposed 2. Authentication Failures Symptoms:\nfailed to export metrics: 401 Unauthorized failed to export metrics: 403 Forbidden Diagnosis:\nCheck credentials in environment variables Verify basicauth extension configuration Test authentication manually: curl -u username:password http://mimir:8080/ready Solutions:\n# Verify basicauth configuration extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} # Check env var set password: ${MIMIR_PASSWORD} # Check env var set exporters: otlphttp: auth: authenticator: basicauth # Must reference extension service: extensions: [basicauth] # Must be listed here 3. Multi-Tenancy Header Issues Symptoms:\nfailed to export metrics: no org id Solution:\nexporters: otlphttp: headers: X-Scope-OrgID: \"your-tenant-id\" # Must include header Or disable multi-tenancy in Mimir:\n# mimir-config.yaml multitenancy_enabled: false 4. Invalid Metric Names or Labels Symptoms:\nerr-mimir-metric-name-invalid err-mimir-label-invalid Cause: Metric names must match [a-zA-Z_:][a-zA-Z0-9_:]*\nSolution: Use the metricstransform processor to rename metrics:\nprocessors: metricstransform: transforms: - include: .* match_type: regexp action: update operations: # Replace invalid characters with underscores - action: update_label label: \"invalid-label\" new_label: \"invalid_label\" 5. High Memory Usage / OOM Kills Symptoms:\nCollector pod/container killed with OOM High memory usage in metrics Diagnosis:\n# Monitor collector memory process_runtime_go_mem_heap_alloc_bytes{job=\"otel-collector\"} Solutions:\nTune memory_limiter:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 # Adjust based on container limit spike_limit_mib: 128 # 25% of limit Set GOMEMLIMIT (80% of container memory):\nenv: - name: GOMEMLIMIT value: \"410MiB\" # For 512Mi container limit Reduce batch sizes:\nprocessors: batch: send_batch_size: 4096 # Reduce from 8192 send_batch_max_size: 5000 # Reduce from 10000 Increase num_consumers:\nexporters: otlphttp: sending_queue: num_consumers: 20 # More parallel exports 6. Metrics Not Appearing in Mimir Diagnosis Checklist:\nCheck collector logs for export errors:\nkubectl logs -f deployment/otel-collector Verify metrics received by collector:\n# Check collector's own metrics otelcol_receiver_accepted_metric_points{receiver=\"otlp\"} otelcol_exporter_sent_metric_points{exporter=\"otlphttp\"} Check Mimir ingester logs:\nkubectl logs -f deployment/mimir-ingester Query Mimir directly:\ncurl -H \"X-Scope-OrgID: your-tenant\" \\ http://mimir:8080/prometheus/api/v1/query?query=up Common Causes:\nMetrics filtered out by filter processor Batch processor holding data (check timeout) Invalid metric names (check for errors in logs) Wrong tenant ID in query vs. export 7. Cardinality Limit Errors Symptoms:\nerr-mimir-max-series-per-metric err-mimir-max-series-per-user Diagnosis:\n# Check active series cortex_ingester_active_series # Check per-metric cardinality topk(10, count by (__name__) ({__name__=~\".+\"})) Solutions:\nIdentify high-cardinality metrics:\n# Use Mimir's cardinality analysis API curl -H \"X-Scope-OrgID: tenant\" \\ http://mimir:8080/prometheus/api/v1/cardinality/label_names Drop high-cardinality labels:\nprocessors: resource: attributes: - key: user_id action: delete - key: request_id action: delete Increase Mimir limits (temporary):\n# Mimir config limits: max_global_series_per_user: 10000000 # Increase limit max_global_series_per_metric: 100000 # Increase limit 8. Slow Query Performance Symptoms:\nQueries timeout High query latency Solutions:\nEnable caching in Mimir:\n# Mimir config query_frontend: results_cache: backend: memcached Reduce query time range:\n# Instead of querying 30 days: rate(http_requests_total[30d]) # Query smaller range: rate(http_requests_total[1h]) Optimize PromQL queries:\n# Inefficient sum(rate(metric[5m])) # Efficient (specify labels) sum(rate(metric{job=\"api\"}[5m])) by (status) Monitoring Collector Health Key Metrics to Monitor:\n# Successful exports rate(otelcol_exporter_sent_metric_points{exporter=\"otlphttp\"}[5m]) # Failed exports rate(otelcol_exporter_send_failed_metric_points{exporter=\"otlphttp\"}[5m]) # Batch processor metrics otelcol_processor_batch_batch_send_size otelcol_processor_batch_timeout_trigger_send # Memory limiter backpressure rate(otelcol_processor_refused_metric_points{processor=\"memory_limiter\"}[5m]) # Queue depth otelcol_exporter_queue_size{exporter=\"otlphttp\"} Recommended Alerts:\ngroups: - name: otel-collector-alerts rules: # Export failures - alert: OTelCollectorExportFailures expr: | rate(otelcol_exporter_send_failed_metric_points[5m]) \u003e 0 for: 5m annotations: summary: \"OpenTelemetry Collector failing to export metrics\" # Memory pressure - alert: OTelCollectorMemoryPressure expr: | rate(otelcol_processor_refused_metric_points[5m]) \u003e 0 for: 5m annotations: summary: \"OpenTelemetry Collector under memory pressure\" # Queue filling up - alert: OTelCollectorQueueFull expr: | otelcol_exporter_queue_size / otelcol_exporter_queue_capacity \u003e 0.8 for: 10m annotations: summary: \"OpenTelemetry Collector export queue filling up\" BattleBots Integration Points Complete Observability Pipeline graph TB subgraph \"BattleBots Services\" Bot[Bot Containers\u003cbr/\u003eOTel SDK] Game[Game Servers\u003cbr/\u003eOTel SDK] API[Battle API\u003cbr/\u003eOTel SDK] end subgraph \"Kubernetes\" K8s[kube-state-metrics] Node[node-exporter] end subgraph \"OpenTelemetry Collector\" OTLP[OTLP Receiver\u003cbr/\u003e:4317 / :4318] Prom[Prometheus Receiver] Bot --\u003e OTLP Game --\u003e OTLP API --\u003e OTLP K8s --\u003e Prom Node --\u003e Prom OTLP --\u003e ResDetect[Resource Detection] Prom --\u003e ResDetect ResDetect --\u003e ResTrans[Resource Transform] ResTrans --\u003e Batch[Batch Processor] Batch --\u003e Export[OTLP HTTP Exporter] end subgraph \"Grafana Mimir\" Export --\u003e|OTLP/HTTP| Dist[Distributor] Dist --\u003e Ing[Ingesters] Ing --\u003e ObjStore[(Object Storage)] Grafana[Grafana] --\u003e QF[Query Frontend] QF --\u003e Querier[Querier] Querier --\u003e Ing Querier --\u003e SG[Store Gateway] SG --\u003e ObjStore end style OTLP fill:#e1f5ff style Export fill:#e1f5ff style Dist fill:#fff4e1 style Grafana fill:#ffe0b2 Example Metrics for BattleBots Bot Action Latency (from OpenTelemetry SDK):\n// In bot implementation histogram, _ := meter.Int64Histogram( \"bot.action.duration\", metric.WithDescription(\"Bot action execution time\"), metric.WithUnit(\"ms\"), ) histogram.Record(ctx, durationMs, attribute.String(\"action\", \"attack\"), attribute.String(\"bot.type\", \"tank\"), ) Query in Mimir (after OTLP export):\n# 95th percentile attack latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) API Request Metrics:\n// In Battle API counter, _ := meter.Int64Counter( \"http.server.requests\", metric.WithDescription(\"HTTP requests\"), ) counter.Add(ctx, 1, attribute.String(\"http.method\", \"POST\"), attribute.String(\"http.route\", \"/api/v1/battles\"), attribute.Int(\"http.status_code\", 201), ) Query in Mimir:\n# Request rate by endpoint and status sum(rate(http_server_requests[5m])) by (http_route, http_status_code) # Error rate sum(rate(http_server_requests{http_status_code=~\"5..\"}[5m])) / sum(rate(http_server_requests[5m])) Resource Attribute Examples for BattleBots # Detected by resourcedetection processor resource: # Kubernetes attributes k8s.namespace.name: \"battlebots\" k8s.pod.name: \"battle-api-abc123\" k8s.deployment.name: \"battle-api\" k8s.node.name: \"node-us-east-1a\" # Cloud attributes cloud.provider: \"aws\" cloud.platform: \"aws_ec2\" cloud.region: \"us-east-1\" cloud.availability_zone: \"us-east-1a\" # Service attributes (from OTel SDK) service.name: \"battle-api\" service.namespace: \"battlebots\" service.version: \"v1.2.3\" service.instance.id: \"pod-abc123\" # Added by resource processor deployment.environment: \"production\" cluster.name: \"us-east-1-prod\" platform: \"battlebots\" Example PromQL Queries with Resource Attributes Query with promoted attributes:\n# All metrics from battle-api service {job=\"battlebots/battle-api\"} Query with target_info join:\n# Metrics filtered by Kubernetes deployment http_server_requests * on(job, instance) group_left(k8s_deployment_name) target_info{k8s_deployment_name=\"battle-api\"} Using info() function (Prometheus 3.0+):\n# Simpler syntax http_server_requests{job=\"battlebots/battle-api\"} * info(target_info) BattleBots Dashboards Request Rate Dashboard:\n{ \"title\": \"BattleBots API Request Rate\", \"targets\": [ { \"expr\": \"sum(rate(http_server_requests{job=~\\\"battlebots/.*\\\"}[5m])) by (http_route)\" } ] } Bot Performance Dashboard:\n{ \"title\": \"Bot Action Latency (p95)\", \"targets\": [ { \"expr\": \"histogram_quantile(0.95, sum(rate(bot_action_duration_bucket[5m])) by (bot_type, action, le))\" } ] } Linked Dashboard (Metrics → Traces):\nClick on metric spike in Grafana Grafana shows exemplars (trace IDs embedded in metrics) Click exemplar → Opens trace in Tempo Trace shows detailed spans with logs Further Reading Official Documentation Configure OpenTelemetry Collector for Mimir - Official integration guide OTLP Format Considerations - OTLP best practices Mimir OTLP Endpoint - API reference Prometheus Resource Attribute Promotion - How resource attributes become labels OpenTelemetry Collector Documentation OTLP HTTP Exporter - Official exporter docs Prometheus Remote Write Exporter - Alternative exporter Batch Processor - Batching guide Resource Detection Processor - Attribute detection Resource Processor - Attribute transformation Guides and Tutorials OpenTelemetry at Grafana Labs 2025 - Latest updates Using Prometheus as OpenTelemetry Backend - Prometheus perspective Mastering the Batch Processor - Deep dive Mastering the Memory Limiter - Prevent OOM OpenTelemetry Processors Best Practices - Configuration tips Troubleshooting OpenTelemetry Collector Troubleshooting - Official guide Mimir Runbooks - Operational guides Using Authenticator Extension - Auth setup Performance and Optimization Prometheus Remote Write Tuning - Optimize ingestion High Cardinality Management - Cardinality strategies Metric Cardinality Explained - Understanding cardinality Community Resources OpenTelemetry Community - Get help, contribute Grafana Community Forum - Ask questions CNCF OpenTelemetry Project - Project homepage ","categories":"","description":"Deep dive into Grafana Mimir's native OTLP support and integration with the OpenTelemetry Collector, including configuration examples, best practices, and troubleshooting guidance.\n","excerpt":"Deep dive into Grafana Mimir's native OTLP support and integration …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/metrics/mimir/mimir-otlp-integration/","tags":"","title":"Mimir: OTLP and OpenTelemetry Collector Integration"},{"body":"Overview Grafana Mimir is a horizontally scalable, highly available, multi-tenant, long-term storage solution for Prometheus metrics. It transforms Prometheus’s single-server architecture into a distributed microservices platform capable of handling over 1 billion active time series with unlimited retention backed by object storage.\nWhat is Mimir? Mimir takes the Prometheus Time Series Database (TSDB) and splits it into microservices, creating a distributed system where each component can scale independently. While Prometheus excels at real-time monitoring on a single machine, Mimir extends this capability to enterprise scale with:\nHorizontal Scalability: Scale from thousands to billions of active time series Long-Term Storage: Store metrics for months or years using cost-effective object storage High Availability: Built-in replication and distributed architecture eliminate single points of failure Multi-Tenancy: Isolated metrics per tenant with per-tenant resource limits Prometheus Compatibility: 100% PromQL compatibility ensures existing queries and dashboards work unchanged Core Innovation Mimir’s key innovation is separating Prometheus into write path, read path, and backend components, each independently scalable:\nWrite Path (Distributor → Ingester): Handles metric ingestion with validation and replication Read Path (Query-Frontend → Querier → Store-Gateway): Executes queries across recent and historical data Backend (Compactor, Ruler): Background processing for storage optimization and alerting This separation enables scaling write throughput independently from query performance, and both independently from long-term storage management.\nRelationship to Prometheus and Cortex Prometheus Foundation: Mimir uses the Prometheus TSDB format and PromQL query language, providing seamless migration paths and familiar operations.\nCortex Successor: Mimir began as a fork of Cortex in March 2022 when Grafana Labs stopped contributing to Cortex. Mimir inherits Cortex’s distributed architecture while adding:\nReduced operational complexity through monolithic deployment mode Split-and-merge compactor that overcomes TSDB’s 64GB index limit Performance optimizations and simplified configuration Active development and feature additions Recommendation: Always choose Mimir over Cortex for new deployments. Cortex is in maintenance mode with minimal active development.\nKey Concepts Blocks Storage Architecture Mimir uses a blocks-based storage system derived from Prometheus TSDB:\nTSDB Blocks Time series data is broken into fixed-time blocks (default: 2 hours) containing:\nChunks: Highly compressed time series sample data (~1.5 bytes per sample) Index: Inverted index mapping metric names and labels to time series Meta.json: Block metadata including time range, statistics, and compaction level Block Lifecycle Creation: Ingesters create 2-hour blocks from in-memory data and upload to object storage Compaction: Compactor merges small blocks into larger, optimized blocks (2h → 12h → 24h) Querying: Queriers fetch recent data from ingesters, historical data from store-gateways Retention: Compactor deletes blocks older than configured retention period Cleanup: Soft-deleted blocks removed after deletion delay (default: 12 hours) Storage Backends Mimir requires object storage for long-term block storage:\nAmazon S3 or S3-compatible services (MinIO, Ceph, etc.) Google Cloud Storage (GCS) Microsoft Azure Blob Storage OpenStack Swift Object storage provides:\nDurability: Built-in replication and fault tolerance Cost-Effectiveness: ~$0.02-0.03/GB/month vs. ~$0.10-0.20/GB/month for SSD Unlimited Capacity: No practical storage limits Geographic Distribution: Multi-region replication for disaster recovery Time Series and Cardinality Time Series Definition A time series is uniquely identified by a metric name and a set of label key-value pairs:\nhttp_requests_total{method=\"GET\", endpoint=\"/api/battles\", status=\"200\"} This creates one time series. Each unique combination of labels creates a separate time series.\nCardinality Cardinality is the number of unique time series (distinct label combinations). High cardinality occurs when labels have many possible values:\nLow Cardinality (Good):\nhttp_requests{method=\"GET\"} # method has ~10 values (GET, POST, PUT, DELETE, etc.) http_requests{status=\"200\"} # status has ~50 values (HTTP status codes) High Cardinality (Problematic):\nhttp_requests{user_id=\"12345\"} # user_id could have millions of values http_requests{request_id=\"abc123\"} # request_id has infinite possible values Best Practice: Avoid labels with unbounded values (UUIDs, timestamps, user IDs, email addresses). Use labels with bounded, enumerable values (service names, environments, HTTP methods, status codes).\nMimir’s Scale: Tested at 1 billion active series, real-world deployments at 500 million series.\nTenants and Tenant Isolation Multi-Tenancy by Default Mimir is multi-tenant by default. Each request must include a tenant ID via the X-Scope-OrgID HTTP header:\ncurl -H \"X-Scope-OrgID: tenant-123\" \\ http://mimir:8080/prometheus/api/v1/query?query=up Tenant Isolation Mechanisms Automatic Creation: Tenants created on first write (no pre-registration needed) Data Segregation: Complete separation of data between tenants in object storage Resource Limits: Per-tenant limits for ingestion rate, active series, query concurrency Query Isolation: Tenants can only query their own data No Authentication: Mimir trusts the X-Scope-OrgID header; add authentication layer (reverse proxy/API gateway) for production Tenant Federation Query across multiple tenants using pipe-separated tenant IDs:\nX-Scope-OrgID: tenant-1|tenant-2|tenant-3 This enables cross-tenant analytics and aggregation while maintaining isolation for writes.\nDisabling Multi-Tenancy For single-tenant deployments, disable multi-tenancy:\nmultitenancy_enabled: false All requests use a default tenant ID without requiring the X-Scope-OrgID header.\nPromQL Compatibility Mimir provides 100% PromQL (Prometheus Query Language) compatibility:\nAll Prometheus query functions supported Recording rules and alerting rules work unchanged Grafana dashboards require no modifications Existing Prometheus alerts can be migrated directly Example Queries:\n# Instant query: Current values up{job=\"battlebots-api\"} # Range query: Historical data rate(http_requests_total[5m]) # Aggregation: Summary across labels sum(rate(http_requests_total[5m])) by (status_code) # Histogram quantiles: Latency percentiles histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service) ) Architecture Components Mimir’s microservices architecture comprises multiple horizontally scalable components that operate independently and in parallel.\nComponent Overview graph TB subgraph \"Write Path\" Prom[Prometheus\u003cbr/\u003eRemote Write] --\u003e Dist[Distributor] OTel[OpenTelemetry\u003cbr/\u003eCollector OTLP] --\u003e Dist Dist --\u003e Ing1[Ingester 1] Dist --\u003e Ing2[Ingester 2] Dist --\u003e Ing3[Ingester 3] end subgraph \"Storage\" Ing1 --\u003e ObjStore[(Object Storage\u003cbr/\u003eS3/GCS/MinIO)] Ing2 --\u003e ObjStore Ing3 --\u003e ObjStore Compact[Compactor] --\u003e ObjStore end subgraph \"Read Path\" Client[Grafana/API] --\u003e QF[Query Frontend] QF --\u003e QS[Query Scheduler] QS --\u003e Q1[Querier 1] QS --\u003e Q2[Querier 2] Q1 --\u003e Ing1 Q1 --\u003e Ing2 Q1 --\u003e Ing3 Q2 --\u003e Ing1 Q2 --\u003e Ing2 Q2 --\u003e Ing3 Q1 --\u003e SG[Store Gateway] Q2 --\u003e SG SG --\u003e ObjStore end subgraph \"Backend\" Ruler[Ruler] --\u003e QF Ruler --\u003e SG end style Dist fill:#e1f5ff style Ing1 fill:#fff4e1 style Ing2 fill:#fff4e1 style Ing3 fill:#fff4e1 style ObjStore fill:#e8f5e9 style QF fill:#f3e5f5 style Q1 fill:#f3e5f5 style Q2 fill:#f3e5f5 Write Path Components Distributor Role: Entry point for the write path; receives and validates incoming metrics.\nKey Functions:\nReceives write requests from Prometheus remote write or OTLP endpoints Validates metric format and labels (must match [a-zA-Z_:][a-zA-Z0-9_:]*) Enforces per-tenant rate limits and metadata limits Shards incoming time series across ingesters based on consistent hashing Replicates data to multiple ingesters (default: 3 replicas) Characteristics:\nStateless: Can be scaled horizontally without coordination CPU-Bound: Scales with sample ingestion rate Resource Estimate: 1 core per 25,000 samples/second Configuration Example:\ndistributor: ring: kvstore: store: memberlist # Service discovery pool: health_check_ingesters: true Ingester Role: Writes incoming time series to long-term storage; serves recent data for queries.\nKey Functions:\nReceives samples from distributors and appends to per-tenant TSDB on local disk Writes samples to Write-Ahead Log (WAL) for crash recovery Compacts in-memory samples into TSDB blocks (default: every 2 hours) Uploads newly created blocks to object storage Serves queries for recent data (within ingester retention window) Participates in hash ring for data sharding and replication Characteristics:\nStateful: Stores active series in memory and on local disk Memory-Bound: Scales with number of active series Resource Estimates: CPU: 1 core per 300,000 in-memory series Memory: 2.5 GB per 300,000 in-memory series Disk: 5 GB per 300,000 in-memory series (for WAL and blocks) Recommended Limits:\nConservative: Up to 1.5 million series per ingester Maximum: Up to 5 million series per ingester (with sufficient memory) Configuration Example:\ningester: ring: replication_factor: 3 # Number of ingester replicas kvstore: store: memberlist blocks_storage: tsdb: dir: /data/tsdb # Local TSDB directory block_ranges_period: [2h] # Block creation interval retention_period: 6h # Keep blocks locally for 6 hours Read Path Components Query-Frontend Role: Receives and optimizes PromQL queries before dispatching to queriers.\nKey Functions:\nProvides the same HTTP API as Prometheus (/prometheus/api/v1/query, etc.) Splits large time-range queries into smaller sub-queries (query splitting) Caches query results to avoid re-computation (query result caching) Shards queries across time series to enable parallel execution (query sharding) Dispatches queries to queriers via query scheduler for better load distribution Retries failed queries automatically Characteristics:\nStateless: Can be scaled horizontally CPU-Bound: Scales with query rate Resource Estimate: 1 core per 250 queries/second Configuration Example:\nquery_frontend: results_cache: backend: memcached memcached: addresses: memcached:11211 split_queries_by_interval: 24h # Split queries into 24h chunks Query-Scheduler (Optional) Role: Intermediary between query-frontend and queriers for better queue management.\nKey Functions:\nReceives queries from query-frontends and maintains a queue Dispatches queries to available queriers Provides fair scheduling across tenants (prevents single tenant monopolizing queriers) Enables scaling query-frontends independently from queriers Characteristics:\nStateless: Lightweight component Resource Estimate: 1 core per 500 queries/second Configuration Example:\nquery_scheduler: max_outstanding_requests_per_tenant: 100 Querier Role: Executes PromQL queries by fetching data from ingesters and store-gateways.\nKey Functions:\nReceives query requests from query-frontend or query-scheduler Fetches recent data (within retention window) from ingesters Fetches historical data (older than retention window) from store-gateways Merges data from multiple sources and evaluates PromQL expression Returns results to query-frontend Characteristics:\nStateless: Can be scaled horizontally CPU + Memory Bound: Scales with query complexity and time range Resource Estimate: 1 core per 10 queries/second (assumes ~100ms average latency) Configuration Example:\nquerier: max_concurrent: 20 # Maximum concurrent queries per querier timeout: 2m # Query timeout query_ingesters_within: 13h # Query ingesters for data within 13h query_store_after: 12h # Query store-gateway for data older than 12h Backend Components Store-Gateway Role: Provides access to historical blocks stored in object storage.\nKey Functions:\nSynchronizes the list of blocks from object storage (bucket index) Downloads and memory-maps index-header files for fast block querying Serves queries from queriers and rulers for historical data Implements block-level sharding (each store-gateway responsible for subset of blocks) Downloads only necessary portions of blocks (chunks and index sections), not entire blocks Characteristics:\nStateful: Memory-maps index-headers, participates in hash ring Disk I/O Bound: Benefits from SSD for index-header operations Resource Estimates: CPU: 1 core per 10 queries/second Memory: 1 GB per 10 queries/second Disk: 13 GB per 1 million active series (for index-header files) Configuration Example:\nstore_gateway: sharding_ring: replication_factor: 3 kvstore: store: memberlist blocks_storage: bucket_store: sync_dir: /data/tsdb-sync index_cache: backend: memcached memcached: addresses: memcached:11211 Compactor Role: Compacts and optimizes TSDB blocks; manages retention and cleanup.\nKey Functions:\nVertical Compaction: Merges blocks from same tenant covering same time range Deduplicates replicated samples (from replication factor \u003e 1) Reduces index size and chunk overhead Horizontal Compaction: Combines blocks across adjacent time periods (2h → 12h → 24h) Split-and-Merge Compaction: For large tenants (\u003e20M series), splits compaction into shards to overcome TSDB 64GB index limit Maintains per-tenant bucket index (metadata about all blocks) Deletes blocks older than configured retention period Implements two-stage deletion: soft delete (mark) → hard delete (remove after delay) Characteristics:\nStateful: Participates in hash ring for per-tenant compaction ownership I/O Bound: Downloads source blocks, writes compacted blocks Resource Estimates: CPU: 1 core per compactor instance Memory: 4 GB per instance Disk: 300 GB per instance (for downloading/uploading blocks) Scaling Guideline: 1 compactor instance per 20 million active series\nConfiguration Example:\ncompactor: data_dir: /data/compactor compaction_interval: 30m block_ranges: [2h, 12h, 24h] # Compaction levels sharding_ring: kvstore: store: memberlist # For large tenants (\u003e20M series) split_and_merge_shards: 4 # Number of shards split_groups: 4 # Number of groups (match shards or use next power of 2) Ruler Role: Evaluates Prometheus recording and alerting rules.\nKey Functions:\nExecutes Prometheus recording rules (pre-compute expensive queries) Evaluates alerting rules and sends alerts to Alertmanager Supports multi-tenant rules (per-tenant rule groups) Stores recording rule results back to Mimir via remote write Uses store-gateway for querying when evaluating rules Characteristics:\nStateful: Participates in hash ring for rule group sharding CPU Bound: Scales with number and complexity of rules Configuration Example:\nruler: enable_api: true # Enable ruler API for rule management rule_path: /data/rules ring: kvstore: store: memberlist ruler_storage: backend: s3 s3: bucket_name: mimir-ruler endpoint: s3.amazonaws.com Deployment Modes Mimir supports three deployment modes with different trade-offs between simplicity and scalability.\nComparison Table Feature Monolithic Read-Write Microservices Complexity Low Medium High Scalability Limited (all together) Medium (3 groups) Maximum (per-component) Resource Efficiency Lower (over-provisioning) Medium Highest (fine-grained) Failure Isolation Single process failure Tier-level failure Component-level isolation Ideal Scale \u003c1M series 1-10M series 10M+ series Operational Overhead Minimal Medium High Deployment Tool Docker/VM Kubernetes Kubernetes + Helm Monolithic Mode Architecture: All Mimir components run in a single process.\nConfiguration:\ntarget: all # Run all components in one process # Or via environment variable: # MIMIR_MODE=all Characteristics:\nSimplest deployment model with lowest operational overhead All components scale together (cannot scale independently) Single binary to deploy and monitor Suitable for development, testing, and small production deployments High Availability: Deploy multiple -target=all instances with shared object storage:\nEach instance runs full component stack Ingesters replicate data across instances (default: 3x replication) Queriers query all ingesters and merge results Provides HA without microservices complexity Resource Requirements:\nMemory: Sized for peak ingester + querier memory needs CPU: Sum of all component CPU needs Disk: Sized for WAL and local TSDB blocks When to Use:\nDevelopment and testing environments POC deployments Production deployments with \u003c1M active series Teams preferring operational simplicity over granular scalability When total resource requirements fit on a single machine (vertically scaled) Limitations:\nCannot scale write path independently from read path Resource-intensive queries impact ingestion performance Maximum scale limited by largest available machine Not supported in Jsonnet deployment tooling Example Docker Compose Configuration:\nversion: '3.8' services: mimir: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] ports: - \"9009:9009\" volumes: - ./mimir.yaml:/etc/mimir.yaml - mimir-data:/data volumes: mimir-data: Read-Write Mode (Experimental) Architecture: Three-tier deployment separating read, write, and backend responsibilities.\nTiers:\nWrite Tier: Distributors + Ingesters (handles data ingestion) Read Tier: Query-Frontends + Queriers (handles queries) Backend Tier: Store-Gateways + Compactors + Rulers (background processing) Configuration:\n# Write tier target: write # Read tier target: read # Backend tier target: backend Characteristics:\nSimpler than full microservices (3 tiers vs. 7+ components) Independent scaling of read vs. write workloads Logical grouping of related components Requires multi-zone ingesters and store-gateways When to Use:\nMedium-scale deployments (1-10M active series) Organizations wanting simpler architecture than microservices Workloads with varying read vs. write loads Teams with some distributed systems experience Limitations:\nLess granular scaling than microservices mode Currently experimental (use with caution in production) Cannot scale individual components within a tier Scaling Example:\nWrite-heavy workload: Scale write tier (more distributors + ingesters) Query-heavy workload: Scale read tier (more query-frontends + queriers) Storage optimization: Scale backend tier (more compactors) Microservices Mode Architecture: Each component runs as a separate process/deployment.\nConfiguration: Each process invoked with specific -target parameter:\n# Distributor mimir -target=distributor -config.file=mimir.yaml # Ingester mimir -target=ingester -config.file=mimir.yaml # Query-frontend mimir -target=query-frontend -config.file=mimir.yaml # Querier mimir -target=querier -config.file=mimir.yaml # Store-gateway mimir -target=store-gateway -config.file=mimir.yaml # Compactor mimir -target=compactor -config.file=mimir.yaml # Ruler mimir -target=ruler -config.file=mimir.yaml # Optional: Query-scheduler mimir -target=query-scheduler -config.file=mimir.yaml # Optional: Alertmanager mimir -target=alertmanager -config.file=mimir.yaml Characteristics:\nMaximum scalability and flexibility Each component scales independently based on workload Granular failure domains (component failures don’t affect entire system) Component-specific resource allocation and optimization Recommended for production environments When to Use:\nProduction deployments with \u003e10M active series Large-scale systems requiring fine-grained control Deployments with highly variable component loads Organizations with distributed systems expertise Kubernetes environments with operator/Helm support Scaling Strategies:\nWrite-Heavy Workload:\nScale distributors (1 core per 25K samples/sec) Scale ingesters (1 core per 300K series) Query-Heavy Workload:\nScale query-frontends (1 core per 250 queries/sec) Scale queriers (1 core per 10 queries/sec) Scale store-gateways (1 core per 10 queries/sec) Large Data Volume:\nScale compactors (1 per 20M series) Increase store-gateway replicas Example Kubernetes Deployment (using Helm):\nhelm repo add grafana https://grafana.github.io/helm-charts helm install mimir grafana/mimir-distributed \\ --namespace mimir \\ --values production-values.yaml Benefits:\nIndependent failure domains (ingester crash doesn’t affect queriers) Fine-tuned resource allocation (queriers get more CPU, ingesters get more memory) Targeted scaling based on bottlenecks Easier capacity planning and cost optimization Trade-offs:\nHigher operational complexity More components to monitor and maintain Network communication overhead between components Requires service discovery and coordination (Kubernetes, Consul, etcd) How to Run Mimir Quick Start with Docker Compose For POC environments and local development, Docker Compose provides the fastest path to running Mimir.\nPrerequisites Docker and Docker Compose installed At least 4 GB RAM available 10 GB free disk space Complete Docker Compose Stack This example deploys:\nMinIO: S3-compatible object storage Mimir (3 instances): High-availability monolithic deployment NGINX: Load balancer distributing traffic across Mimir instances Grafana: Visualization and dashboarding Directory Structure:\nmimir-poc/ ├── docker-compose.yml ├── config/ │ ├── mimir.yaml │ ├── nginx.conf │ └── alertmanager-fallback-config.yaml └── data/ (created automatically) docker-compose.yml:\nversion: '3.8' services: # MinIO object storage minio: image: minio/minio entrypoint: [\"\"] command: [\"sh\", \"-c\", \"mkdir -p /data/mimir-blocks /data/mimir-ruler /data/mimir-alertmanager \u0026\u0026 minio server --quiet /data --console-address :9001\"] environment: - MINIO_ROOT_USER=mimir - MINIO_ROOT_PASSWORD=supersecret ports: - \"9000:9000\" # S3 API - \"9001:9001\" # Web console volumes: - minio-data:/data healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 10s retries: 3 # Mimir instance 1 mimir-1: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-1 depends_on: - minio ports: - \"8001:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-1-data:/data # Mimir instance 2 mimir-2: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-2 depends_on: - minio ports: - \"8002:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-2-data:/data # Mimir instance 3 mimir-3: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-3 depends_on: - minio ports: - \"8003:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-3-data:/data # NGINX load balancer nginx: image: nginx:alpine ports: - \"9009:9009\" volumes: - ./config/nginx.conf:/etc/nginx/nginx.conf:ro depends_on: - mimir-1 - mimir-2 - mimir-3 # Grafana for visualization grafana: image: grafana/grafana:latest environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin ports: - \"3000:3000\" volumes: - grafana-data:/var/lib/grafana volumes: minio-data: mimir-1-data: mimir-2-data: mimir-3-data: grafana-data: config/mimir.yaml:\n# Monolithic mode configuration target: all,alertmanager # Multi-tenancy (disable for single-tenant POC) multitenancy_enabled: false # Server configuration server: http_listen_port: 8080 log_level: info # Common configuration (shared by all components) common: storage: backend: s3 s3: endpoint: minio:9000 access_key_id: mimir secret_access_key: supersecret insecure: true # Use HTTP (not HTTPS) for local MinIO # Blocks storage configuration blocks_storage: backend: s3 s3: bucket_name: mimir-blocks tsdb: dir: /data/tsdb bucket_store: sync_dir: /data/tsdb-sync # Compactor configuration compactor: data_dir: /data/compactor sharding_ring: kvstore: store: memberlist # Distributor configuration distributor: ring: instance_addr: 127.0.0.1 kvstore: store: memberlist # Ingester configuration ingester: ring: instance_addr: 127.0.0.1 kvstore: store: memberlist replication_factor: 3 # 3 replicas across 3 Mimir instances # Ruler storage configuration ruler_storage: backend: s3 s3: bucket_name: mimir-ruler # Alertmanager configuration alertmanager_storage: backend: s3 s3: bucket_name: mimir-alertmanager alertmanager: fallback_config_file: /etc/alertmanager-fallback-config.yaml data_dir: /data/alertmanager # Store-gateway configuration store_gateway: sharding_ring: replication_factor: 3 # Limits configuration limits: # Ingestion limits ingestion_rate: 10000 # Samples per second per tenant ingestion_burst_size: 20000 # Series limits max_global_series_per_user: 1000000 # 1M series max max_global_series_per_metric: 50000 # Query limits max_query_lookback: 0 # No limit on query time range # Retention compactor_blocks_retention_period: 0 # Indefinite retention (0 = disabled) config/nginx.conf:\nevents { worker_connections 1024; } http { upstream mimir { server mimir-1:8080; server mimir-2:8080; server mimir-3:8080; } server { listen 9009; location / { proxy_pass http://mimir; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } config/alertmanager-fallback-config.yaml:\nroute: receiver: 'default-receiver' group_wait: 10s group_interval: 10s repeat_interval: 1h receivers: - name: 'default-receiver' Launch the Stack # Start all services docker-compose up -d # Verify services are running docker-compose ps # Check Mimir logs docker-compose logs -f mimir-1 # Access MinIO console open http://localhost:9001 # Username: mimir, Password: supersecret # Access Grafana open http://localhost:3000 Configure Prometheus Remote Write prometheus.yml:\nglobal: scrape_interval: 15s scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] remote_write: - url: http://localhost:9009/api/v1/push queue_config: capacity: 10000 max_shards: 10 min_shards: 1 Configure Grafana Data Source Navigate to Connections → Data sources Click Add data source Select Prometheus Configure: Name: Mimir URL: http://nginx:9009/prometheus Prometheus type: Mimir Click Save \u0026 test Production Configuration with MinIO For on-premises production deployments or when AWS S3 is not available, MinIO provides enterprise-grade S3-compatible object storage.\nDifferences from POC Setup:\nDistributed MinIO: Deploy 4+ MinIO servers for high availability (erasure coding) Persistent Volumes: Use network-attached storage or cloud block storage TLS Encryption: Enable HTTPS for MinIO and Mimir Authentication: Implement proper access control and API key rotation Resource Limits: Set CPU/memory limits based on capacity planning Monitoring: Deploy Prometheus + Grafana to monitor Mimir and MinIO Example Production Values:\n# Mimir production configuration with MinIO common: storage: backend: s3 s3: endpoint: minio.storage.svc.cluster.local:9000 access_key_id: ${MINIO_ACCESS_KEY} # From secret secret_access_key: ${MINIO_SECRET_KEY} # From secret insecure: false # Use HTTPS bucket_name: mimir-blocks ingester: ring: replication_factor: 3 instance_addr: ${POD_IP} kvstore: store: memberlist limits: ingestion_rate: 50000 # Higher for production max_global_series_per_user: 10000000 # 10M series blocks_storage: tsdb: retention_period: 24h # Ingesters retain blocks for 24h before relying on object storage block_ranges_period: [2h] compactor: compaction_interval: 30m cleanup_interval: 15m block_ranges: [2h, 12h, 24h] Resource Requirements Based on the capacity planning guide:\nSmall Deployment (~1M active series):\nDistributors: 2 instances × (1 core, 1 GB RAM) Ingesters: 3 instances × (2 cores, 8 GB RAM, 20 GB disk) Queriers: 2 instances × (1 core, 2 GB RAM) Query-Frontends: 2 instances × (1 core, 1 GB RAM) Store-Gateways: 2 instances × (1 core, 2 GB RAM, 15 GB disk) Compactor: 1 instance × (1 core, 4 GB RAM, 300 GB disk) Object Storage: ~50 GB (1 year retention) Large Deployment (~10M active series):\nTotal Resources: ~140 CPUs, 800 GB memory Use the Mimir capacity calculator for detailed estimates Grafana Data Source Setup After deploying Mimir, configure Grafana to query metrics:\nVia UI:\nConnections → Data sources → Add data source → Prometheus URL: http://mimir-gateway:8080/prometheus (adjust for your deployment) Prometheus type: Select “Mimir” from dropdown Save \u0026 test Via Provisioning (GitOps):\n# grafana/provisioning/datasources/mimir.yaml apiVersion: 1 datasources: - name: Mimir type: prometheus access: proxy url: http://mimir-gateway.mimir.svc.cluster.local:8080/prometheus jsonData: prometheusType: Mimir timeInterval: 15s httpMethod: POST Best Practices Label Strategy and Cardinality Management Golden Rule: Avoid unbounded labels (infinite possible values).\nGood Labels (bounded cardinality):\n{service=\"battle-api\"} # Limited number of services {environment=\"production\"} # dev, staging, production (3 values) {region=\"us-east-1\"} # Limited AWS regions {status_code=\"200\"} # ~50 HTTP status codes {method=\"GET\"} # GET, POST, PUT, DELETE, PATCH (~7 values) Bad Labels (unbounded cardinality):\n{user_id=\"12345\"} # Millions of users = millions of series {request_id=\"abc-123-def\"} # Every request unique = infinite series {email=\"user@example.com\"} # Unbounded user emails {timestamp=\"1634567890\"} # Infinite timestamp values {session_id=\"xyz789\"} # Every session unique Cardinality Calculation:\nTotal Series = Metric × (Label1_Values × Label2_Values × ... × LabelN_Values) Example:\nhttp_requests_total{service, environment, region, method, status_code} = 1 metric × (10 services × 3 environments × 5 regions × 7 methods × 50 status codes) = 1 × 52,500 = 52,500 time series Adding user_id (1M users):\n= 52,500 × 1,000,000 = 52.5 billion time series (UNSUSTAINABLE!) Best Practices:\nUse Bounded Sets: Labels should have enumerable values Aggregate Before Storing: Use recording rules to pre-aggregate high-cardinality metrics Drop Unused Labels: Use metric_relabel_configs in Prometheus to drop unnecessary labels Monitor Cardinality: Enable cardinality analysis in Mimir Set Limits: Configure per-tenant series limits to prevent runaway cardinality Configuration Best Practices Ingester Tuning File Descriptors:\n# Set system limits for ingester pods/containers ulimit -n 65536 # Minimum ulimit -n 1048576 # For 1000+ tenants Memory and Disk:\ningester: # Target 1.5M series per ingester (conservative) # Max 5M series per ingester (with sufficient memory) blocks_storage: tsdb: # Reduce block creation interval for lower memory usage block_ranges_period: [1h] # vs. default 2h # Tune for high tenant count head_chunks_write_buffer_size_bytes: 2097152 # 2MB (default 4MB) stripe_size: 8192 # Default 16384 Zone-Aware Replication:\ningester: ring: zone_awareness_enabled: true instance_availability_zone: ${AZ} # us-east-1a, us-east-1b, etc. Querier and Store-Gateway Optimization Enable Caching (CRITICAL for production):\nblocks_storage: bucket_store: # Metadata cache metadata_cache: backend: memcached memcached: addresses: memcached-metadata:11211 # Index cache (high CPU usage) index_cache: backend: memcached memcached: addresses: memcached-index:11211 # Chunks cache (high bandwidth usage) chunks_cache: backend: memcached memcached: addresses: memcached-chunks:11211 max_item_size: 5242880 # 5MB max chunk size query_frontend: # Query results cache results_cache: backend: memcached memcached: addresses: memcached-results:11211 timeout: 500ms File Descriptors:\n# Store-gateway needs many open files for index-headers ulimit -n 65536 # Minimum SSD Recommendations:\nIngesters: SSD for WAL performance Store-Gateways: SSD for index-header operations Compactor: SSD for faster compaction Compactor Configuration For Standard Tenants:\ncompactor: compaction_interval: 30m cleanup_interval: 15m data_dir: /data/compactor block_ranges: [2h, 12h, 24h] For Large Tenants (\u003e20M series):\ncompactor: # Enable split-and-merge compaction split_and_merge_shards: 4 # 1 shard per 8M series split_groups: 4 # Match shard count or next power of 2 Retention Configuration:\nlimits: # Global default: 1 year retention compactor_blocks_retention_period: 1y # Per-tenant overrides via runtime config overrides: tenant-production: compactor_blocks_retention_period: 2y # 2 years for production tenant-development: compactor_blocks_retention_period: 4w # 4 weeks for dev Storage Backend Selection Cloud Deployments:\nAWS: Use Amazon S3 (native integration, lowest latency in AWS) GCP: Use Google Cloud Storage (native integration, lowest latency in GCP) Azure: Use Azure Blob Storage (disable hierarchical namespace!) On-Premises Deployments:\nMinIO: Deploy distributed MinIO (4+ nodes with erasure coding) Ceph: Use Ceph RADOS Gateway (S3-compatible) OpenStack Swift: For OpenStack environments Bucket Configuration:\n# CRITICAL: Use separate buckets for each storage type blocks_storage: s3: bucket_name: mimir-blocks # TSDB blocks ruler_storage: s3: bucket_name: mimir-ruler # Recording/alerting rules alertmanager_storage: s3: bucket_name: mimir-alertmanager # Alertmanager state Bucket Lifecycle Policies (cost optimization):\n\u003c!-- AWS S3 Lifecycle Policy Example --\u003e \u003cLifecycleConfiguration\u003e \u003cRule\u003e \u003cID\u003eTransitionOldMetrics\u003c/ID\u003e \u003cStatus\u003eEnabled\u003c/Status\u003e \u003cTransition\u003e \u003cDays\u003e90\u003c/Days\u003e \u003cStorageClass\u003eSTANDARD_IA\u003c/StorageClass\u003e \u003c/Transition\u003e \u003cTransition\u003e \u003cDays\u003e180\u003c/Days\u003e \u003cStorageClass\u003eGLACIER_IR\u003c/StorageClass\u003e \u003c/Transition\u003e \u003c/Rule\u003e \u003c/LifecycleConfiguration\u003e Performance Tuning gRPC Compression Enable compression between components to reduce network bandwidth:\n# Ingester → Object Storage compression blocks_storage: s3: # Enable gzip compression for block uploads send_content_encoding: gzip # Query-Frontend → Querier compression querier: frontend_client: grpc_client_config: # Enable gzip compression grpc_compression: gzip Compression Trade-offs:\nSnappy: ~5x compression, 400 MiB/s throughput (low CPU) Gzip: 6-8x compression, 50-135 MiB/s throughput (higher CPU) Query Optimization Query Splitting:\nquery_frontend: # Split large time-range queries into smaller chunks split_queries_by_interval: 24h # Split into 24-hour chunks align_queries_with_step: true Query Sharding:\nquery_frontend: # Shard queries across time series for parallel execution parallelize_shardable_queries: true Query Caching:\nquery_frontend: cache_results: true results_cache: backend: memcached memcached: addresses: memcached:11211 # Cache queries for 10 minutes cache_unaligned_requests: true Avoid Querying Non-Compacted Blocks Use default values for these settings to avoid querying uncompacted blocks:\nquerier: # Query store-gateway for data older than 12h query_store_after: 12h # Query ingesters for data within 13h query_ingesters_within: 13h blocks_storage: bucket_store: # Store-gateway ignores blocks uploaded within 10h ignore_blocks_within: 10h This ensures queriers only fetch compacted, optimized blocks from store-gateways.\nMulti-Tenancy Best Practices Always Use Reverse Proxy:\nClient → Auth Proxy (validates user, injects X-Scope-OrgID) → Mimir Never expose Mimir directly to untrusted clients.\nPer-Tenant Limits:\n# Global defaults (conservative) limits: ingestion_rate: 10000 max_global_series_per_user: 1000000 # Runtime config for per-tenant overrides overrides: premium-tenant: ingestion_rate: 100000 max_global_series_per_user: 10000000 standard-tenant: ingestion_rate: 25000 max_global_series_per_user: 2500000 Enable Shuffle Sharding:\ningester: ring: # Reduce blast radius: each tenant uses subset of ingesters instance_enable_ipv6: false unregister_on_shutdown: true Common Pitfalls to Avoid Not Enabling Caching: Queries will hammer object storage (high cost, poor performance) Unbounded Label Cardinality: User IDs, request IDs, timestamps in labels Insufficient File Descriptors: Ingesters and store-gateways need high limits Same Bucket for Different Stores: Always use separate buckets Azure Hierarchical Namespace: Must be disabled (causes orphaned directories) No Monitoring: Deploy mimir-mixin dashboards and alerts from day one Under-provisioning Compactor Disk: Needs space for source + destination blocks Ignoring Latency Spikes: Upgrade to v2.15+ for improved block-cutting When to Use Mimir Ideal Use Cases 1. Enterprise Scale (\u003e10M active series)\nPrometheus hits memory and disk limits around 10-50M series depending on hardware. Mimir scales horizontally to billions of series.\nExample: Multi-region infrastructure with 100+ Kubernetes clusters, each running hundreds of services.\n2. Long-Term Retention (months to years)\nPrometheus retention limited by local disk capacity (typically days to weeks). Mimir uses object storage for years of retention.\nExample: Compliance requirements for 2-year metrics retention, capacity planning based on historical trends.\n3. Multi-Cluster Aggregation\nMultiple Prometheus instances across regions/environments need unified querying.\nExample: Global view of service health across US, EU, and APAC deployments.\n4. Multi-Tenancy\nPer-customer, per-team, or per-environment isolation with resource limits.\nExample: SaaS platform providing per-customer metrics dashboards, or large organization isolating team metrics.\n5. High Availability Requirements\nNo tolerance for data loss or query unavailability.\nExample: Financial services, healthcare, or e-commerce platforms requiring 99.9%+ uptime.\nAnti-Patterns (When NOT to Use Mimir) 1. Small-Scale Deployments (\u003c1M active series, \u003c30 days retention)\nProblem: Mimir’s complexity unjustified for workloads Prometheus handles easily.\nSolution: Use standalone Prometheus until scale demands distributed storage.\n2. Operational Complexity Constraints\nProblem: Team lacks distributed systems expertise or Kubernetes experience.\nSolution: Start with Prometheus, build expertise, migrate to Mimir when needed.\n3. Cost-Constrained Environments\nProblem: Mimir has higher baseline infrastructure costs (multiple components + object storage).\nSolution: Prometheus is more cost-effective at small scale.\n4. Real-Time Only (No Historical Analysis)\nProblem: If only real-time alerting needed (no dashboards, no historical queries).\nSolution: Prometheus sufficient; Mimir’s long-term storage unused.\nMimir vs. Prometheus Comparison Factor Prometheus Grafana Mimir Active Series Limit 10-50M (single machine) 1B+ (distributed) Retention Days to weeks (disk-limited) Months to years (object storage) Scalability Vertical only Horizontal (all components) High Availability Manual (federation/replica pairs) Built-in (replication + distribution) Multi-Tenancy Not built-in Native support Operational Complexity Low (single binary) High (microservices) Setup Time Minutes Hours to days Cost (Small Scale) Lower Higher Cost (Large Scale) Not feasible Medium (object storage efficient) Query Performance Excellent (local disk) Good (distributed, cached) Use Case Small-medium, real-time Enterprise, long-term, multi-cluster Mimir vs. Thanos Comparison Both Mimir and Thanos solve similar problems but with different approaches.\nFactor Thanos Grafana Mimir Architecture Sidecar or Receiver mode Receiver mode only (push-based) Design Focus Operational simplicity, cost Performance, scalability Compaction Standard TSDB compaction Split-and-merge (overcomes 64GB index limit) Max Index Size 64GB (TSDB limit) No practical limit Query Caching Metadata caching only Full query result caching Maturity Mature, CNCF project Newer, actively developed Deployment Model Pull (sidecar) or push (receiver) Push only (remote write) Grafana Integration Good Excellent (same vendor) Community Large CNCF community Grafana Labs + community When to Choose Mimir:\nNeed maximum performance and scalability Prefer Grafana ecosystem integration Large tenants (\u003e20M series per tenant) Active development and new features valued When to Choose Thanos:\nPrefer sidecar deployment model (pull-based) CNCF governance important Operational simplicity over raw performance Existing Thanos deployments Mimir vs. Cortex Relationship: Mimir is the successor to Cortex (forked March 2022).\nKey Differences:\nDevelopment: Mimir actively developed by Grafana Labs; Cortex in maintenance mode Features: Mimir has newer features (monolithic mode, improved compactor, OTLP support matured first) Complexity: Mimir simplified some operational aspects of Cortex Migration: Cortex → Mimir migration supported and documented Recommendation: Always use Mimir for new deployments. Only run Cortex if already deployed and not ready to migrate.\nDecision Criteria Matrix Use this matrix to determine if Mimir is right for your use case:\nCriterion Prometheus Mimir Thanos Active Series \u003c10M \u003e10M \u003e10M Retention Requirement \u003c30 days \u003e30 days \u003e30 days Multi-Cluster Single cluster Multiple clusters Multiple clusters Multi-Tenancy Not needed Required Via labels Team Expertise Limited Distributed systems Distributed systems Budget Constrained Medium-High Medium Deployment Model Any Kubernetes preferred Kubernetes or VMs Vendor Preference Neutral Grafana ecosystem CNCF ecosystem Decision Guide:\nChoose Prometheus if:\nActive series \u003c 10M Retention \u003c 30 days Single Kubernetes cluster Team prefers simplicity Choose Mimir if:\nActive series \u003e 10M (or expect to reach soon) Retention \u003e 30 days Multiple Prometheus instances to aggregate Multi-tenancy required Grafana ecosystem preferred Choose Thanos if:\nWant to keep Prometheus sidecar model CNCF governance important Operational simplicity over maximum performance Existing Thanos deployment BattleBots Integration Points For the BattleBots platform, Mimir would serve as the centralized metrics storage backend within the broader observability stack.\nObservability Stack Architecture graph TB subgraph \"Metric Sources\" Bot[Bot Containers] Game[Game Servers] API[Battle API] K8s[Kubernetes] Host[Host Systems] end subgraph \"Collection Layer\" OTel[OpenTelemetry Collector] Bot --\u003e OTel Game --\u003e OTel API --\u003e OTel K8s --\u003e OTel Host --\u003e OTel end subgraph \"Storage Layer\" OTel --\u003e|OTLP Metrics| Mimir[(Grafana Mimir\u003cbr/\u003eMetrics Storage)] OTel --\u003e|OTLP Logs| Loki[(Grafana Loki\u003cbr/\u003eLog Storage)] OTel --\u003e|OTLP Traces| Tempo[(Grafana Tempo\u003cbr/\u003eTrace Storage)] end subgraph \"Query \u0026 Visualization\" Grafana[Grafana] Grafana --\u003e Mimir Grafana --\u003e Loki Grafana --\u003e Tempo end subgraph \"Alerting\" Mimir --\u003e|Alerts| AM[Alertmanager] AM --\u003e|Notifications| Slack[Slack/PagerDuty] end style OTel fill:#e1f5ff style Mimir fill:#fff4e1 style Loki fill:#e8f5e9 style Tempo fill:#f3e5f5 style Grafana fill:#ffe0b2 Game Metrics Use Cases Bot Performance Metrics Action Latency:\n# 95th percentile attack action latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) # Slow bots (p95 \u003e 100ms) histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket[5m])) by (bot_id, le) ) \u003e 0.1 Bot Health Tracking:\n# Average bot health per battle avg(bot_health_points) by (battle_id, bot_id) # Bots eliminated in last hour count(bot_health_points == 0) by (battle_id) Resource Usage:\n# CPU usage per bot container rate(container_cpu_usage_seconds_total{ namespace=\"battlebots\", pod=~\"bot-.*\" }[5m]) # Memory usage per bot container_memory_working_set_bytes{ namespace=\"battlebots\", pod=~\"bot-.*\" } / 1024 / 1024 # Convert to MB Battle Event Metrics Battle State:\n# Active battles sum(battle_state{state=\"active\"}) # Average battle duration rate(battle_duration_seconds_sum[5m]) / rate(battle_duration_seconds_count[5m]) # Battles per minute rate(battles_total[1m]) * 60 Matchmaking Metrics:\n# Players in queue player_queue_length # Average queue wait time rate(queue_wait_seconds_sum[5m]) / rate(queue_wait_seconds_count[5m]) # Matchmaking success rate rate(matchmaking_success_total[5m]) / rate(matchmaking_attempts_total[5m]) Infrastructure Metrics Use Cases API Performance:\n# Request rate by endpoint and status sum(rate(http_requests_total{service=\"battle-api\"}[5m])) by (endpoint, status_code) # Error rate sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) # p99 latency histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (endpoint, le) ) Kubernetes Metrics:\n# Pod restart rate rate(kube_pod_container_status_restarts_total{ namespace=\"battlebots\" }[1h]) # Pods not ready count(kube_pod_status_phase{ namespace=\"battlebots\", phase!=\"Running\" }) # Node resource utilization sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes) Label Strategy for BattleBots Recommended Labels:\n{ # Infrastructure labels cluster=\"us-east-1-prod\", namespace=\"battlebots\", service=\"battle-api\", environment=\"production\", # Game labels battle_id=\"12345\", # OK if battles are finite and expire bot_type=\"tank\", # Enumerable bot types game_mode=\"team-deathmatch\", # Bounded game modes region=\"us-east\", # Geographic regions # Avoid these! # player_id=\"...\", # High cardinality # session_id=\"...\", # Unbounded # request_id=\"...\", # Infinite values } Cardinality Estimate:\nhttp_requests_total { service: 10 values (battle-api, game-server, matchmaking, etc.) environment: 3 values (dev, staging, prod) region: 5 values (us-east, us-west, eu-west, ap-south, ap-east) method: 7 values (GET, POST, PUT, DELETE, PATCH, OPTIONS, HEAD) endpoint: 50 values (API endpoints) status_code: 50 values (HTTP status codes) } = 1 × 10 × 3 × 5 × 7 × 50 × 50 = 2,625,000 time series # Acceptable cardinality for 1 metric Example PromQL Queries for BattleBots Battle Analytics:\n# Total battles completed today increase(battles_completed_total[24h]) # Win rate by bot type sum(rate(battle_outcomes_total{outcome=\"victory\"}[1h])) by (bot_type) / sum(rate(battle_outcomes_total[1h])) by (bot_type) # Average players online by hour avg_over_time(players_online_total[1h]) Capacity Planning:\n# CPU headroom (sum(node_cpu_capacity_cores) - sum(rate(node_cpu_usage_seconds_total[5m]))) / sum(node_cpu_capacity_cores) # Memory headroom (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemUsed_bytes)) / sum(node_memory_MemTotal_bytes) # Projected series growth predict_linear(mimir_ingester_active_series[24h], 7*24*3600) Cost Optimization:\n# Underutilized bot containers (CPU \u003c 10%) avg(rate(container_cpu_usage_seconds_total{pod=~\"bot-.*\"}[5m])) by (pod) \u003c 0.1 # Idle game servers count(game_server_active_battles == 0) by (instance) Alerting Examples Critical Alerts:\ngroups: - name: battlebots-critical interval: 30s rules: # Battle API down - alert: BattleAPIDown expr: up{job=\"battle-api\"} == 0 for: 1m labels: severity: critical annotations: summary: \"Battle API is down\" description: \"Battle API {{ $labels.instance }} has been down for more than 1 minute\" # High error rate - alert: HighErrorRate expr: | sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) \u003e 0.05 for: 5m labels: severity: critical annotations: summary: \"High error rate detected\" description: \"Error rate is {{ $value | humanizePercentage }}\" # Ingester approaching series limit - alert: MimirIngesterSeriesLimit expr: | mimir_ingester_active_series / mimir_limits_overrides{limit_name=\"max_global_series_per_user\"} \u003e 0.8 for: 15m labels: severity: warning annotations: summary: \"Mimir ingester approaching series limit\" Integration with Loki and Tempo Metric-to-Log Correlation:\n# In Grafana, link from metric spike to logs {namespace=\"battlebots\", service=\"battle-api\"} |= \"error\" | logfmt | battle_id=\"12345\" Metric-to-Trace Correlation:\nUse exemplars in Prometheus metrics to link to traces Query by trace_id in Tempo from metric anomalies Grafana’s Explore view shows metrics → traces → logs correlation Unified Dashboard Example:\n┌─────────────────────────────────────┐ │ Battle API Request Rate (Mimir) │ │ [Graph showing spike at 14:30] │ └─────────────────────────────────────┘ ↓ Click spike ┌─────────────────────────────────────┐ │ Traces at 14:30 (Tempo) │ │ [Slow traces listed] │ └─────────────────────────────────────┘ ↓ Click trace ┌─────────────────────────────────────┐ │ Logs for trace_id (Loki) │ │ [Error logs with stack trace] │ └─────────────────────────────────────┘ Further Reading Official Documentation Grafana Mimir Documentation - Comprehensive official docs Mimir GitHub Repository - Source code, issues, discussions Mimir Architecture Overview - Detailed architecture guide Mimir Configuration Parameters - Complete config reference Mimir Runbooks - Troubleshooting guides Deployment and Operations Helm Chart Documentation - Kubernetes deployment guide Production Tips - Best practices Capacity Planning - Resource estimation Mimir Capacity Calculator - Interactive sizing tool Monitor Mimir Health - Self-monitoring setup Integration Guides Configure OpenTelemetry Collector for Mimir - OTLP integration Migrate from Prometheus to Mimir - Migration guide Remote Write Tuning - Optimize Prometheus → Mimir ingestion Grafana Dashboards for Mimir - Pre-built dashboards Performance and Scaling How We Scaled Mimir to 1 Billion Active Series - Grafana Labs blog Scaling Mimir to 500M Series (Customer Story) - Pipedrive case study Mimir vs Prometheus Scalability - Real-world comparison Query Performance Optimization - PromQL tuning Comparisons and Decision Guides Mimir vs Prometheus Comparison - Detailed comparison Mimir vs Thanos Discussion - Community comparison Prometheus and Centralized Storage - When to use centralized metrics Community and Support Mimir Community Forum - Ask questions, share knowledge Grafana Slack #mimir Channel - Real-time community support Mimir Release Notes - Version history and breaking changes CNCF OpenTelemetry Project - Related OTLP ecosystem Tutorials and Workshops Play with Grafana Mimir - Hands-on tutorial Mimir Workshop - Full Docker Compose examples OTLP Integration Tutorial - OpenTelemetry + Mimir guide Advanced Topics Cardinality Management - Label strategy Multi-Tenancy Setup - Tenant isolation guide Ingest Storage Architecture - Kafka-based ingestion (Mimir 3.0+) Split-and-Merge Compactor - Large tenant optimization ","categories":"","description":"Comprehensive overview of Grafana Mimir architecture, deployment modes, storage backends, and operational best practices for long-term Prometheus metrics storage.\n","excerpt":"Comprehensive overview of Grafana Mimir architecture, deployment …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/metrics/mimir/mimir-overview/","tags":"","title":"Mimir: Overview and Architecture"},{"body":"Overview This section contains research and analysis of observability solutions for the BattleBots platform. Observability is critical for:\nMonitoring real-time battle events and game state Tracking bot performance and system health Debugging issues in distributed game architecture Analyzing player behavior and system usage patterns Ensuring reliable service operation Components OpenTelemetry Collector Analysis of the OpenTelemetry Collector, a vendor-agnostic telemetry data pipeline that can receive, process, and export logs, metrics, and traces to multiple backends.\nThe OpenTelemetry Collector serves as a centralized telemetry hub, providing:\nVendor neutrality for any observability backend Protocol translation between Prometheus, Jaeger, Zipkin, and OTLP Unified collection pipeline for logs, metrics, and traces Flexible deployment in agent, gateway, or hybrid modes Signal correlation linking traces, metrics, and logs Includes detailed analysis of:\nArchitecture and core components Logs, metrics, and traces support Self-monitoring and operational considerations BattleBots platform integration patterns Log Storage Analysis of log storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nLog storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Grafana Loki Research on Grafana Loki, a horizontally scalable, multi-tenant log aggregation system designed for cost-effective log storage and querying.\nLoki uses an index-free approach that indexes only metadata labels rather than full log content, providing:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and operational best practices OTLP compatibility and OTel Collector integration Label strategy and performance considerations Metrics Storage Analysis of metrics storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nMetrics storage is essential for:\nReal-time monitoring of battle events and game state Historical analysis of bot performance and system behavior Capacity planning and infrastructure optimization Alerting on critical system conditions Long-term trend analysis and reporting Grafana Mimir Research on Grafana Mimir, a horizontally scalable, highly available, multi-tenant metrics storage system for long-term Prometheus data retention.\nMimir transforms Prometheus from a single-server monitoring system into a distributed platform capable of handling over 1 billion active time series, providing:\nNative OTLP support for direct integration with OpenTelemetry Collector Horizontal scalability through independent scaling of write path, read path, and backend components Long-term storage using object storage backends (S3, GCS, MinIO) with months to years of retention Built-in multi-tenancy with per-tenant resource limits and isolation Full Prometheus (PromQL) compatibility for queries, dashboards, and alerts High availability through replication and distributed architecture Includes detailed analysis of:\nArchitecture components (distributor, ingester, querier, store-gateway, compactor) and deployment modes Native OTLP ingestion endpoints and OpenTelemetry Collector integration (otlphttp and prometheusremotewrite exporters) Object storage backends, blocks storage architecture, and retention policies Multi-tenancy setup, cardinality management, and label strategy Comparison with Prometheus, Thanos, and Cortex Production deployment patterns, resource requirements, and operational best practices Future ADR Dependencies This analysis will inform:\nADR-NNNN: Observability Stack Selection - Which backends to use (Loki, Prometheus, Jaeger, etc.) ADR-NNNN: Telemetry Collection Strategy - Agent vs. gateway deployment, sampling policies ADR-NNNN: Telemetry Data Retention - Storage duration and cost management Related Documentation R\u0026D Documentation User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of observability solutions for the BattleBots platform.\n","excerpt":"Research and analysis of observability solutions for the BattleBots …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/","tags":"","title":"Observability Analysis"},{"body":"Overview The OpenTelemetry Collector serves as a centralized telemetry hub, removing the need to run multiple agents or collectors for different formats and backends. It provides:\nVendor neutrality: Works with any observability backend Protocol translation: Converts between Prometheus, Jaeger, Zipkin, and OTLP formats Unified collection: Single pipeline for logs, metrics, and traces Flexible deployment: Agent mode, gateway mode, or hybrid Signal correlation: Links traces, metrics, and logs through shared context Document Structure The analysis is organized into the following documents:\nOpenTelemetry Collector Overview High-level architectural overview covering:\nCore components (receivers, processors, exporters, extensions) Pipeline-based architecture and data flow Deployment patterns (agent, gateway, hybrid) Configuration fundamentals When to use the Collector vs. direct exports Audience: Everyone—provides foundational understanding for all subsequent documents.\nLogs Support Deep dive into log data handling:\nOTLP logs data model and structure Log receivers (filelog, syslog, OTLP) Log processors (attributes, filter, transform) Log exporters (Loki, Elasticsearch, OTLP) Log correlation with traces and metrics Configuration patterns for log collection Audience: Developers implementing log collection, operations teams configuring log pipelines.\nMetrics Support Deep dive into metrics data handling:\nOpenTelemetry metrics data model Metric types (counters, gauges, histograms, summaries) Temporality (delta vs. cumulative) Metrics receivers (Prometheus, hostmetrics, OTLP) Metrics processors and exporters Performance and cardinality considerations Audience: Developers instrumenting applications, SREs monitoring infrastructure.\nTraces Support Deep dive into distributed tracing:\nTrace and span data model Context propagation mechanisms Trace receivers (OTLP, Jaeger, Zipkin) Sampling strategies (head vs. tail sampling) Trace processors and exporters Multi-backend routing Audience: Developers implementing distributed tracing, architects designing observability strategy.\nSelf-Monitoring How to observe the Collector itself:\nInternal metrics and telemetry Extensions (health_check, zpages, pprof) Debugging and troubleshooting techniques Performance monitoring and optimization Production monitoring best practices Audience: Operations teams, SREs responsible for Collector reliability.\nBattleBots Platform Context For the BattleBots platform, the OpenTelemetry Collector would support:\nGame Event Observability Logs: Battle events, bot actions, game state transitions, error conditions Metrics: Match duration, action rates, player counts, system resource usage Traces: Request flows from player action to state update to broadcast Infrastructure Monitoring Host metrics: Server CPU, memory, disk, network utilization Application metrics: Go runtime metrics, HTTP latency, WebSocket connections Container metrics: Resource limits, restart counts, health status Cross-Signal Correlation The Collector enables powerful debugging workflows:\nAlert fires on high error rate (metrics) Drill down to traces showing failing requests View logs associated with failing trace spans Identify root cause with full context This unified observability is particularly valuable during live battles when quick diagnosis is essential.\nImplementation Considerations Deployment Architecture For BattleBots, a recommended deployment would include:\nAgent Mode:\nCollectors running alongside each game server Local log file collection with filelog receiver Host metrics collection for server monitoring OTLP receiver for application telemetry Gateway Mode:\nCentralized collectors receiving data from agents Tail sampling for intelligent trace retention Multi-backend routing (analytics, debugging, long-term storage) Buffering and retry for backend resilience Signal-Specific Patterns Logs:\nCollect structured JSON logs from game servers Parse and enrich with resource attributes Filter debug logs in production Route to Loki or Elasticsearch Metrics:\nScrape Prometheus metrics from Go services Collect host metrics from servers Aggregate and downsample for cost efficiency Export to Prometheus or cloud backends Traces:\nInstrument Go services with OpenTelemetry SDK Use head sampling for baseline reduction (10%) Apply tail sampling to always capture errors Export to Jaeger or Grafana Tempo External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of the OpenTelemetry Collector for logs, metrics, and traces collection and processing.\n","excerpt":"Research and analysis of the OpenTelemetry Collector for logs, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/","tags":"","title":"OpenTelemetry Collector"},{"body":"Overview The OpenTelemetry Collector is a vendor-agnostic application that receives, processes, and exports telemetry data (traces, metrics, and logs). It serves as a centralized component in observability architectures, removing the need to run multiple agents or collectors for different telemetry formats and backends.\nThe Collector supports open-source observability data formats including Jaeger, Prometheus, Fluent Bit, and others, while providing a unified approach to telemetry handling. It enables services to offload telemetry data quickly while the Collector handles retries, batching, encryption, and sensitive data filtering.\nKey benefits include vendor independence, reduced operational complexity, and the ability to route telemetry data to multiple backends simultaneously without modifying application code.\nKey Concepts The Collector is built around five guiding principles:\nUsability: Provides functional defaults with support for popular protocols out-of-the-box Performance: Maintains stability under varying loads with predictable resource usage Observability: Designed as an observable service itself, exposing its own metrics and health status Extensibility: Allows customization through plugins without requiring modifications to core code Unification: Single codebase supporting all three telemetry signals (traces, metrics, logs) Core Architecture The Collector uses a pipeline-based architecture where data flows through three primary component types, orchestrated by a configuration file.\ngraph LR A[Telemetry Sources] --\u003e B[Receivers] B --\u003e C[Processors] C --\u003e D[Exporters] D --\u003e E[Observability Backends] F[Extensions] -.Optional.-\u003e B F -.Optional.-\u003e C F -.Optional.-\u003e D style A fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style C fill:#f3e5f5 style D fill:#e8f5e9 style F fill:#fce4ec Data Flow Telemetry data flows unidirectionally through the Collector:\nReceivers accept incoming telemetry in various formats (OTLP, Jaeger, Prometheus) Processors transform, filter, or enrich the data in a sequential chain Exporters send processed data to one or more backend destinations This pipeline architecture allows the same data to be simultaneously:\nSampled differently for different backends Enriched with environment-specific attributes Routed to multiple observability platforms Components Receivers Receivers gather telemetry data through two mechanisms:\nPush-based: Listen on network endpoints for incoming data Pull-based: Actively scrape metrics from instrumented services Multiple receivers can feed into a single pipeline, with their outputs merged before reaching processors. Common receivers include:\notlp - OpenTelemetry Protocol (gRPC/HTTP) prometheus - Prometheus metrics scraping jaeger - Jaeger trace data filelog - Log file collection For comprehensive receiver documentation, see the OpenTelemetry Collector Receivers.\nProcessors Processors form a sequential chain that transforms data between receivers and exporters. Each processor in the chain operates on the data before passing it to the next processor.\nCommon operations include:\nAdding or removing attributes Sampling (probabilistic, tail-based) Batching for efficient transmission Filtering unwanted telemetry Resource detection (cloud provider, Kubernetes metadata) Important: Each pipeline maintains independent processor instances, even when referencing the same configuration name across multiple pipelines. This prevents unintended state sharing between pipelines.\nFor comprehensive processor documentation, see the OpenTelemetry Collector Processors.\nExporters Exporters forward processed data to external destinations, typically by sending to network endpoints or writing to logging systems. Multiple exporters can receive identical data copies through a fan-out mechanism, enabling simultaneous transmission to different backends.\nCommon exporters include:\notlp - OpenTelemetry Protocol destinations prometheus - Prometheus remote write jaeger - Jaeger backend logging - Standard output for debugging file - Local file storage For comprehensive exporter documentation, see the OpenTelemetry Collector Exporters.\nConnectors Connectors enable inter-pipeline communication and data flow between different telemetry signal types. A connector acts as both an exporter (for one pipeline) and a receiver (for another pipeline).\nUse cases include:\nGenerating metrics from trace spans Creating logs from metric anomalies Correlating signals for enhanced observability For more information, see the OpenTelemetry Collector Connectors.\nExtensions Extensions provide auxiliary functionality that doesn’t directly process telemetry data but supports Collector operations:\nhealth_check - HTTP endpoint for health monitoring pprof - Go profiling endpoint for performance analysis zpages - In-process debugging pages ballast - Memory ballast for GC optimization Extensions are optional but highly recommended for production deployments. See self-monitoring documentation for more details.\nConfiguration The Collector uses YAML configuration files (default: /etc/otelcol/config.yaml) with four main sections:\nreceivers: # Define how to collect telemetry processors: # Define how to transform telemetry exporters: # Define where to send telemetry service: pipelines: # Connect receivers → processors → exporters Pipeline Types Three pipeline types handle different telemetry signals:\ntraces: Distributed tracing data metrics: Time-series measurements logs: Event records Each pipeline independently connects receivers to exporters through an optional processor chain. The same receiver can feed multiple pipelines simultaneously, though this creates a potential bottleneck if one processor blocks.\nConfiguration Best Practices Validate configurations before deployment: otelcol validate --config=file.yaml Use environment variables for sensitive values: ${env:API_KEY} Bind endpoints to localhost for local-only access Apply TLS certificates for production environments Use the type/name syntax for multiple instances of the same component type For detailed configuration guidance, see the Configuration Documentation.\nDeployment Patterns The Collector supports multiple deployment models based on operational requirements and scale.\nAgent Mode In agent mode, lightweight Collector instances run as daemons alongside applications (on the same host, container, or Kubernetes pod). This pattern:\nCollects telemetry from co-located services Performs local sampling and aggregation Reduces network overhead by local processing Simplifies application configuration (default OTLP exporters assume localhost:4317) Use when: You need local collection with per-host resource limits and want to offload telemetry handling from application processes.\nGateway Mode In gateway mode, centralized Collector instances receive data from distributed agents and application libraries, then route to backend systems. This pattern:\nProvides centralized control over data transformation Enables consistent sampling decisions across services Supports complex routing logic to multiple backends Allows for sensitive data filtering before egress Use when: You need centralized policy enforcement, multi-backend routing, or want to isolate backend credentials from application environments.\nHybrid Mode Many production deployments use both agent and gateway patterns:\nAgents perform local collection and basic processing Gateways handle aggregation, complex transformations, and backend routing This hybrid approach balances local efficiency with centralized control.\nNo Collector For development environments or initial experimentation, services can export directly to backends using OTLP. However, this approach loses the benefits of buffering, retries, and transformation capabilities.\nWhen to Use the Collector Recommended Use Cases Deploy a Collector when you need to:\nSupport multiple telemetry formats (Jaeger, Prometheus, custom formats) Route telemetry to multiple backends simultaneously Perform data transformation or enrichment before export Sample or filter telemetry based on configurable rules Isolate backend credentials from application deployments Buffer telemetry during backend outages Offload retry and batching logic from applications Direct Export Scenarios Direct service-to-backend export may be sufficient for:\nDevelopment and testing environments Single-backend deployments with no transformation needs Prototyping and proof-of-concept work Very small-scale deployments Integration Points BattleBots Observability Requirements For the BattleBots platform, the Collector would support:\nBattle Event Tracking: Collecting logs of bot actions and game state changes Performance Metrics: Gathering metrics on bot response times and system resource usage Distributed Tracing: Tracking request flows across client/server or P2P architectures See User Journey 0001: POC for observability requirements context.\nMulti-Signal Correlation The Collector enables correlation between:\nTrace spans and related logs (via trace context) Metrics and traces (via exemplars) Resource attributes across all signals This correlation is particularly valuable for debugging battle scenarios where you need to understand both the timeline of events (traces), the quantitative measurements (metrics), and the detailed context (logs).\nFurther Reading Official Documentation OpenTelemetry Collector Main Documentation Collector Architecture Configuration Reference Component Registry Specifications OTLP Specification OpenTelemetry Specification Implementation Resources Collector GitHub Repository Collector Contrib Repository (additional components) Related Analysis Documents Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"High-level architectural overview of the OpenTelemetry Collector, covering core components, deployment patterns, and use cases.\n","excerpt":"High-level architectural overview of the OpenTelemetry Collector, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/opentelemetry-collector-overview/","tags":"","title":"OpenTelemetry Collector Overview"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting log data from various sources to multiple backends. Unlike traditional logging agents that focus on specific formats or destinations, the Collector treats logs as first-class observability signals alongside metrics and traces.\nThe Collector’s log support enables correlation between logs and traces through shared execution context (TraceId and SpanId), allowing unified observability across all three signal types. This correlation is particularly valuable for debugging complex distributed systems where understanding both the quantitative measurements and the detailed event context is essential.\nKey capabilities include parsing structured and unstructured log formats, enriching logs with resource attributes, filtering and transforming log content, and routing logs to multiple backends simultaneously.\nKey Concepts OTLP Logs Data Model OpenTelemetry defines a standardized log data model to establish a common understanding of what a LogRecord is and what data needs to be recorded, transferred, stored, and interpreted by logging systems.\nLogRecord Structure:\nA LogRecord contains several key components:\nTimestamp: The moment in time when the event occurred TraceId and SpanId: Execution context identifiers enabling correlation between logs and traces Resource: Describes the origin of the log (host name, container name, pod name, etc.) Instrumentation Scope: Identifies the library or component that generated the log Severity: Indicates the importance or criticality of the log entry Body: The actual log message content (string, structured data, or binary) Attributes: Structured key-value pairs providing additional context This standardized model allows logs from different sources to be processed uniformly while preserving correlation capabilities.\nLog Correlation The logs data model enables correlation across three dimensions:\nTemporal correlation: Based on timestamp alignment Execution context correlation: Using TraceId and SpanId to link logs to specific trace spans Origin correlation: Through Resource context describing the source infrastructure and application This unified approach allows observability backends to perform exact and unambiguous correlation between logs, metrics, and traces.\nLog Receivers Receivers collect log data from various sources and convert it into the OpenTelemetry logs data model.\nFilelog Receiver The filelog receiver tails and parses logs from files, making it ideal for collecting logs from applications that write to local disk.\nKey Capabilities:\nFile monitoring: Tracks multiple log files using glob patterns for inclusion and exclusion Automatic rotation handling: Detects and follows rotated log files and symlinks Compression support: Reads gzip-compressed files with auto-detection Multiline support: Combines log entries spanning multiple lines using custom patterns Format parsing: Built-in parsers for JSON, regex patterns, and structured text Metadata extraction: Parses timestamps, severity levels, and custom fields Persistent offsets: Maintains file positions across collector restarts Example use cases:\nCollecting application logs written to /var/log/ Parsing container logs from Kubernetes Reading structured JSON logs from microservices Configuration note: The filelog receiver uses a pipeline of operators that transform raw file content into structured LogRecords. Each operator performs a specific transformation (parsing, extraction, modification) before passing data to the next operator.\nOTLP Receiver The OTLP receiver accepts log data transmitted using the OpenTelemetry Protocol.\nSupported transports:\ngRPC (default port 4317): Uses unary requests with ExportLogsServiceRequest messages HTTP (default port 4318): POST requests to /v1/logs endpoint Encoding formats:\nBinary Protobuf (application/x-protobuf) JSON Protobuf (proto3 JSON mapping) Use when: Collecting logs directly from applications instrumented with OpenTelemetry SDKs or from upstream OpenTelemetry Collectors in a multi-tier deployment.\nSyslog Receiver The syslog receiver listens for syslog messages over TCP or UDP, supporting RFC 3164 and RFC 5424 formats.\nUse cases:\nCollecting system logs from Linux/Unix hosts Receiving logs from network devices (routers, switches, firewalls) Integrating with legacy applications that use syslog Other Log Receivers The OpenTelemetry Collector ecosystem includes receivers for:\njournald: Reads logs from systemd journal tcplog/udplog: Generic TCP/UDP log receivers windowseventlog: Collects Windows Event Logs kafka: Consumes logs from Kafka topics For a comprehensive list, see the Receiver Components documentation.\nLog Processors Processors transform, filter, and enrich log data as it flows through the pipeline.\nAttributes Processor The attributes processor modifies attributes of log records.\nCapabilities:\nInsert new attributes Update existing attributes Delete attributes Hash attribute values for privacy Extract values from one attribute to another Common use cases:\nAdding environment labels (e.g., environment=production) Removing sensitive data from log attributes Normalizing attribute names across different sources For more details, see the Mastering the OpenTelemetry Attributes Processor guide.\nFilter Processor The filter processor drops log records that match specified conditions using the OpenTelemetry Transformation Language (OTTL).\nCapabilities:\nFilter by log severity level Drop logs matching specific patterns Exclude logs from certain resources Reduce log volume by dropping debug logs in production Example scenarios:\nDropping health check logs to reduce noise Filtering out logs below a certain severity threshold Excluding logs from specific namespaces or services See also the Filter Processor for OpenTelemetry Collector documentation.\nTransform Processor The transform processor modifies log records using OTTL statements.\nCapabilities:\nParse log body content into structured attributes Modify log severity based on content Extract values using regex or JSON path Compute new attributes from existing ones Normalize timestamps Use cases:\nConverting unstructured log messages to structured attributes Extracting user IDs or request IDs from log text Standardizing log formats from multiple sources For transformation guidance, see Transforming telemetry.\nResource Detection Processor The resource detection processor enriches logs with metadata about their execution environment:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information (Docker, containerd) Host information (hostname, OS, architecture) This automatic enrichment enables filtering and grouping logs by infrastructure context without manual configuration.\nBatch Processor The batch processor groups log records before sending to exporters, improving throughput and reducing network overhead.\nConfiguration considerations:\nTimeout: Maximum time to wait before sending a batch Batch size: Number of log records per batch Memory limits: Prevents excessive memory usage Batching is recommended for production deployments to optimize resource usage.\nLog Exporters Exporters send processed log data to observability backends and storage systems.\nOTLP HTTP Exporter The OTLP HTTP exporter is the recommended exporter for modern observability backends that support native OTLP ingestion.\nSupported destinations:\nGrafana Loki (v3+): Send logs to Loki’s native OTLP endpoint at http://loki:3100/otlp Elasticsearch: Use OTLP-compatible endpoints Commercial backends: Datadog, New Relic, Honeycomb, Dynatrace Important: The Loki-specific exporter is deprecated. Use the standard otlphttp/logs exporter for Loki v3+ which supports native OTLP ingestion.\nFor setup guidance, see Getting started with the OpenTelemetry Collector and Loki tutorial.\nElasticsearch Exporter The Elasticsearch exporter sends logs, metrics, traces, and profiles directly to Elasticsearch.\nSupported versions:\nElasticsearch 7.17.x Elasticsearch 8.x Elasticsearch 9.x Features:\nIndex routing based on log attributes Dynamic index naming with time-based patterns Bulk API for efficient ingestion File Exporter The file exporter writes logs to local files, useful for:\nDebugging collector pipelines Creating log archives Forwarding to systems that process files Note: Not recommended for production logging backends—use OTLP or dedicated exporters instead.\nLogging Exporter The logging (debug) exporter writes logs to the collector’s standard output. Use this for:\nDevelopment and testing Troubleshooting pipeline configuration Verifying data transformation For additional exporters, see the Exporter Components documentation.\nLog Pipeline Flow A typical log pipeline in the Collector follows this pattern:\ngraph LR A[Log Files] --\u003e B[Filelog Receiver] C[OTLP SDK] --\u003e D[OTLP Receiver] B --\u003e E[Resource Detection] D --\u003e E E --\u003e F[Transform Processor] F --\u003e G[Filter Processor] G --\u003e H[Attributes Processor] H --\u003e I[Batch Processor] I --\u003e J[OTLP HTTP Exporter] J --\u003e K[Loki] I --\u003e L[Elasticsearch Exporter] L --\u003e M[Elasticsearch] style A fill:#e1f5ff style C fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#e8f5e9 style L fill:#e8f5e9 style K fill:#e1f5ff style M fill:#e1f5ff Configuration Considerations Multiline Log Handling Many applications emit logs spanning multiple lines (e.g., stack traces, JSON objects). The filelog receiver supports multiline patterns to combine these entries:\nreceivers: filelog: include: [/var/log/app/*.log] multiline: line_start_pattern: '^\\d{4}-\\d{2}-\\d{2}' Parsing Structured Logs For JSON-formatted logs, configure the filelog receiver with JSON parsing:\nreceivers: filelog: include: [/var/log/app/*.log] operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' Log Volume Management High log volumes can overwhelm collectors and backends. Strategies include:\nSampling: Use the probabilistic sampler processor to keep a percentage of logs Filtering: Drop debug/trace logs in production using the filter processor Batching: Configure appropriate batch sizes to balance latency and throughput Tail sampling: Keep only logs associated with interesting traces Performance Tuning For high-throughput log collection:\nIncrease the number of concurrent file readers in filelog receiver Tune batch processor settings (size, timeout) Use multiple collector instances with load balancing Consider gateway deployment to centralize processing Integration Points BattleBots Log Collection For the BattleBots platform, log collection would capture:\nGame events: Bot actions, state transitions, match outcomes System logs: Server startup, configuration changes, errors Client logs: User actions, connection events, performance issues The filelog receiver can parse structured JSON logs from the game server while the OTLP receiver collects logs directly from instrumented Go services.\nLog-Trace Correlation When both logs and traces are collected, correlation enables:\nFinding all logs for a specific trace (query by TraceId) Jumping from a log entry to its parent trace span Identifying logs that occurred during slow requests This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically.\nCross-Signal Analysis Logs complement metrics and traces:\nMetrics show aggregate patterns (error rate spike) Traces show request flow (which service failed) Logs show detailed context (exception message, variable values) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Logs Specification OTLP Specification Receiver Components Processor Components Exporter Components Transforming Telemetry Component-Specific Resources Filelog Receiver Attributes Processor Guide Filter Processor Guide Transform Processor Elasticsearch Exporter Integration Guides Ingesting logs to Loki using OpenTelemetry Collector Getting started with the OpenTelemetry Collector and Loki tutorial Honeycomb Filter Processor Documentation Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles log data, including receivers, processors, exporters, and the OTLP logs data model.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles log data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-logs/","tags":"","title":"OpenTelemetry Collector: Logs Support"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting metrics data from diverse sources to multiple backends. It serves as a bridge between different metrics ecosystems, enabling seamless integration of Prometheus metrics, host system metrics, and custom application metrics within a unified observability platform.\nThe Collector’s metrics support emphasizes signal correlation—connecting metrics to traces through exemplars and enriching attributes via Baggage and Context. This enables powerful observability patterns such as jumping from a metric anomaly to related traces or finding metrics that explain slow trace spans.\nKey capabilities include scraping Prometheus endpoints, collecting host system metrics, transforming metric formats, aggregating data points, and routing metrics to multiple backends simultaneously while handling different temporality preferences.\nKey Concepts OpenTelemetry Metrics Data Model The OpenTelemetry Metrics data model defines how metrics are represented and processed. The data model serves to:\nCapture raw measurements efficiently and simultaneously Decouple instrumentation from the SDK implementation Enable correlation with traces and logs Support migration from OpenCensus and Prometheus Architecture layers:\nMeterProvider \u0026 Instruments: Applications collect measurements through Meters and their associated Instruments In-Memory Aggregation: Measurements aggregate into an intermediate representation MetricReader: Processes aggregated metrics for export to backends Metric Types OpenTelemetry supports four primary metric types, each suited for different measurement scenarios.\nCounter (Sum) Counters represent cumulative or delta measurements that can only increase over time (or be reset to zero). Common examples include:\nRequest count Error count Bytes transmitted Items processed Characteristics:\nMonotonically increasing Supports both delta and cumulative temporality Can be aggregated across instances Typically visualized as rate-of-change For detailed specifications, see OTLP Metrics Types.\nGauge Gauges represent sampled values that can arbitrarily increase or decrease over time. Unlike counters, gauges are not cumulative—they reflect the current value at the time of measurement.\nCommon examples include:\nCPU usage percentage Memory utilization Queue depth Active connection count Temperature readings Characteristics:\nNon-monotonic (can increase or decrease) No aggregation temporality (uses “last sample value”) Represents point-in-time state Cannot be meaningfully aggregated across instances without additional context Histogram Histograms convey a population of recorded measurements in a compressed format by grouping measurements into configurable buckets. This enables statistical analysis without storing individual data points.\nCommon examples include:\nRequest latency distribution Response size distribution Query execution time Message size distribution Characteristics:\nProvides count, sum, and bucket distributions Supports both delta and cumulative temporality Enables percentile calculations (p50, p95, p99) More efficient than storing individual measurements Histograms are particularly valuable for understanding the distribution of latency or size measurements, revealing whether most requests are fast with occasional slow outliers, or if performance degrades uniformly.\nSummary Summaries provide pre-calculated quantile values (percentiles) over a time window. Unlike histograms, which send bucket distributions for backend calculation, summaries compute quantiles client-side.\nImportant: Summary points cannot always be merged meaningfully. This point type is not recommended for new applications and exists primarily for compatibility with other formats like Prometheus summaries.\nFor comprehensive metric type details, see OpenTelemetry Metrics.\nTemporality Temporality defines how metric values are accumulated and reported over time. OpenTelemetry supports two aggregation temporality modes:\nDelta Temporality Delta temporality reports the change since the last collection period. Each data point represents only new measurements since the previous export.\nCharacteristics:\nNon-overlapping time windows Measures rate of change Preferred by some backends (StatsD, Carbon) Requires stateless aggregation Example: A request counter shows +100 requests in period 1, then +150 requests in period 2.\nCumulative Temporality Cumulative temporality reports the total value since process start (or a fixed start point). Each data point includes all measurements from the beginning.\nCharacteristics:\nOverlapping time windows from fixed start Accumulates over application lifetime Preferred by Prometheus Resilient to collection gaps Example: A request counter shows 100 total requests in period 1, then 250 total requests in period 2.\nImportant note: Set the temporality preference to DELTA when possible, as setting it to CUMULATIVE may discard some data points during application or collector startup. However, Prometheus backends require cumulative temporality.\nFor more details, see OpenTelemetry Metrics Aggregation.\nMetrics Receivers Receivers collect metrics data from various sources and convert it into the OpenTelemetry metrics data model.\nPrometheus Receiver The Prometheus receiver enables the OpenTelemetry Collector to act as a Prometheus server by scraping Prometheus-compatible endpoints, then converting the metrics into OTLP format.\nKey capabilities:\nScrapes any Prometheus /metrics endpoint Supports service discovery mechanisms Converts Prometheus metrics to OpenTelemetry format Handles metric relabeling and filtering Scales with Target Allocator for large deployments Configuration example:\nreceivers: prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 15s static_configs: - targets: ['localhost:8888'] Scaling with Target Allocator:\nThe Target Allocator decouples service discovery and metric collection, allowing independent scaling. Each Collector pod registers with the Target Allocator, which uses consistent hashing to distribute discovered targets evenly among active Collectors, ensuring each target is scraped exactly once without overlap.\nFor comprehensive guidance, see Prometheus and OpenTelemetry Collector Integration.\nHost Metrics Receiver The hostmetrics receiver collects comprehensive system-level metrics from host machines, providing visibility into infrastructure health.\nAvailable scrapers:\ncpu: CPU utilization, time, and frequency disk: Disk I/O operations and throughput filesystem: Filesystem usage and available space memory: Memory utilization and swap usage network: Network interface statistics and errors load: System load averages paging: Paging and swapping activity processes: Process count and resource usage Configuration example:\nreceivers: hostmetrics: collection_interval: 30s scrapers: cpu: disk: filesystem: memory: network: The hostmetrics receiver is essential for infrastructure monitoring and provides context for application-level metrics. When deployed in Kubernetes, appropriate volumes and volumeMounts are automatically configured when the hostMetrics preset is enabled.\nOTLP Receiver The OTLP receiver accepts metrics data transmitted using the OpenTelemetry Protocol from instrumented applications or upstream collectors.\nSupported transports:\ngRPC (default port 4317) HTTP (default port 4318, endpoint /v1/metrics) Use cases:\nCollecting metrics directly from OpenTelemetry SDKs Multi-tier collector deployments (agent → gateway) Receiving metrics from serverless functions Other Metrics Receivers The ecosystem includes receivers for diverse metrics sources:\nstatsd: Receives StatsD protocol metrics kafka: Consumes metrics from Kafka topics influxdb: Receives InfluxDB line protocol carbon: Receives Graphite carbon metrics collectd: Receives collectd metrics postgresql: Scrapes PostgreSQL metrics redis: Scrapes Redis metrics mongodb: Scrapes MongoDB metrics For a complete list, see the Receiver Components documentation.\nMetrics Processors Processors transform and enrich metrics data as it flows through pipelines.\nMetrics Transform Processor The metrics transform processor modifies metric names, types, and attributes using transformation rules.\nCommon operations:\nRename metrics for consistency Change metric types (e.g., gauge to counter) Add or modify resource attributes Aggregate metrics across dimensions Filter Processor The filter processor drops metrics matching specified conditions, reducing data volume and costs.\nUse cases:\nDropping debug metrics in production Filtering metrics from test environments Excluding high-cardinality metrics Removing specific metric names or attribute values Cumulative to Delta Processor This processor converts cumulative temporality metrics to delta temporality, useful when backends prefer delta metrics.\nAttributes Processor Adds, updates, or deletes metric attributes and resource attributes, enabling:\nEnvironment labeling (environment=production) Team ownership tags (team=platform) Cost allocation labels Normalization across different metric sources Batch Processor Groups metrics before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nConfiguration considerations:\nBatch size: Number of metric data points per batch Timeout: Maximum wait before sending partial batch Memory limits: Prevents unbounded memory growth Metrics Exporters Exporters send processed metrics to observability backends and time-series databases.\nOTLP Exporter The OTLP exporter sends metrics using the OpenTelemetry Protocol to OTLP-compatible backends.\nSupported destinations:\nOpenTelemetry-native backends Cloud vendor endpoints (AWS CloudWatch, Google Cloud Monitoring, Azure Monitor) Commercial observability platforms (Datadog, New Relic, Honeycomb) Important: OTLP is now the recommended protocol for sending metrics to modern backends. For example, Prometheus can now accept OTLP directly:\nexport OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://localhost:9090/api/v1/otlp/v1/metrics Prometheus Remote Write Exporter The Prometheus Remote Write exporter sends OpenTelemetry metrics to Prometheus remote write compatible backends such as:\nCortex Grafana Mimir Thanos Amazon Managed Service for Prometheus Google Cloud Managed Prometheus Capabilities:\nTLS support (required by default) Queued retry mechanisms Authentication options (basic auth, bearer token, OAuth2) Important limitation: Non-cumulative monotonic, histogram summary, and exponential histogram OTLP metrics are dropped by this exporter.\nFor Grafana Mimir specifically, it’s recommended to use OTLP rather than Prometheus remote write.\nPrometheus Exporter The Prometheus exporter exposes metrics in Prometheus format on an HTTP endpoint for Prometheus servers to scrape.\nUse cases:\nExisting Prometheus deployments Push-based collection converted to pull-based Multi-backend export (push to one, expose for scraping by another) File Exporter Writes metrics to local files for debugging, archival, or processing by batch systems.\nFor additional exporters, see the Exporter Components documentation.\nMetrics Pipeline Flow A typical metrics pipeline demonstrates collection, processing, and export to multiple backends:\ngraph LR A[Prometheus Endpoints] --\u003e B[Prometheus Receiver] C[Host System] --\u003e D[Host Metrics Receiver] E[OTLP SDK] --\u003e F[OTLP Receiver] B --\u003e G[Attributes Processor] D --\u003e G F --\u003e G G --\u003e H[Filter Processor] H --\u003e I[Transform Processor] I --\u003e J[Batch Processor] J --\u003e K[OTLP Exporter] K --\u003e L[Prometheus Server] J --\u003e M[Prometheus Remote Write] M --\u003e N[Grafana Mimir] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#e8f5e9 style M fill:#e8f5e9 style L fill:#e1f5ff style N fill:#e1f5ff Configuration Considerations Temporality Management Different backends have different temporality preferences:\nPrometheus: Requires cumulative temporality StatsD-like systems: Prefer delta temporality Cloud vendors: Often accept both Configure the Collector to convert between temporalities based on backend requirements using the cumulative-to-delta processor.\nCardinality Control High cardinality metrics (many unique label combinations) can overwhelm backends and increase costs. Strategies include:\nFiltering high-cardinality dimensions Aggregating metrics before export Dropping rarely-used labels Using metric relabeling to reduce dimensions Scrape Interval Tuning Balance data freshness with resource consumption:\nShort intervals (5-15s): Real-time monitoring, higher costs Medium intervals (30-60s): Standard monitoring Long intervals (5m+): Capacity planning, cost optimization Exemplar Support Exemplars link metrics to traces by attaching trace IDs to specific metric data points. This enables:\nJumping from a high-latency histogram bucket to example slow traces Finding traces that contributed to an error rate spike Correlating metrics anomalies with detailed trace analysis Enable exemplar support in the Prometheus receiver and ensure trace context propagation in applications.\nIntegration Points BattleBots Metrics Collection For the BattleBots platform, metrics collection would capture:\nGame Metrics:\nMatch duration and outcome distribution Bot action rates (attacks, defenses, moves) Game state transition frequency Performance Metrics:\nRequest latency percentiles WebSocket connection counts Message throughput rates Server CPU and memory usage Business Metrics:\nActive user count Matches per hour Bot creation rate The combination of Prometheus receiver (for Go runtime metrics), hostmetrics receiver (for infrastructure), and OTLP receiver (for custom metrics) provides comprehensive visibility.\nMetrics-Trace Correlation Connecting metrics and traces enables powerful workflows:\nAlerting on metrics: High error rate triggers investigation Drill-down to traces: Click exemplar to see example failing requests Root cause analysis: Examine detailed trace spans to identify cause Fix validation: Monitor metrics to confirm fix effectiveness This requires:\nApplications emit both metrics and traces Exemplars enabled in metric collection Unified storage backend (or cross-backend linking) Cross-Signal Analysis Metrics complement logs and traces:\nMetrics identify anomalies at scale (response time spike) Traces show affected request flows (which service is slow) Logs provide detailed context (exception messages, stack traces) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Metrics Specification Metrics Data Model Receiver Components Processor Components Exporter Components Integration Guides Collecting Prometheus Metrics with the OpenTelemetry Collector Prometheus and OpenTelemetry Collector Integration OpenTelemetry Host Metrics receiver Using Prometheus as your OpenTelemetry backend Configure the OpenTelemetry Collector to write metrics into Mimir How to collect Prometheus metrics with the OpenTelemetry Collector and Grafana Component-Specific Resources Prometheus Remote Write Exporter OpenTelemetry Collector Chart (Kubernetes) Prometheus and OpenTelemetry - Better Together Analysis and Best Practices OTLP Metrics Types Understanding OpenTelemetry Metrics OpenTelemetry Metrics Aggregation How Prometheus Exporters Work With OpenTelemetry Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles metrics data, including the metrics data model, receivers, processors, exporters, and temporality concepts.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles metrics data, …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-metrics/","tags":"","title":"OpenTelemetry Collector: Metrics Support"},{"body":"Overview The OpenTelemetry Collector is designed as an observable service itself, following the principle that observability infrastructure must be observable. The Collector exposes its own telemetry (metrics, logs, and optionally traces) to enable monitoring health, diagnosing issues, and optimizing performance.\nSelf-monitoring is critical for production deployments—without visibility into the Collector’s operation, data loss or performance degradation can go undetected. The Collector provides built-in telemetry, diagnostic extensions, and debugging capabilities to ensure reliable operation at scale.\nKey capabilities include internal metrics exposed via Prometheus endpoints, health check endpoints for liveness/readiness probes, diagnostic extensions for real-time inspection, and structured logging for troubleshooting pipeline issues.\nKey Concepts Internal Telemetry The OpenTelemetry Collector generates internal telemetry by default to expose its operational state. This self-generated observability data helps operators monitor Collector health and performance.\nTelemetry types:\nMetrics: Quantitative measurements of Collector operation (default: Prometheus format on port 8888) Logs: Structured event records emitted to stderr by default Traces: Optional internal tracing of data flow through pipelines Internal telemetry enables:\nReal-time health monitoring Capacity planning and resource optimization Troubleshooting data loss or pipeline issues Performance profiling and bottleneck identification Observability vs. Debugging The Collector provides two complementary approaches:\nObservability (production):\nContinuous metrics collection Health check endpoints Structured logging External monitoring integration Debugging (development/troubleshooting):\nzPages for live data inspection pprof for performance profiling Debug exporters for pipeline validation Verbose logging modes Internal Metrics The Collector exposes comprehensive metrics about its operation through a Prometheus-compatible endpoint.\nMetrics Endpoint By default, the Collector exposes metrics at http://localhost:8888/metrics in Prometheus format. This endpoint can be scraped by Prometheus or any compatible metrics collector.\nConfiguration:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed # Options: none, basic, normal, detailed Key Metrics Categories Data Ingress Metrics Monitor data received by receivers:\notelcol_receiver_accepted_log_records - Log records accepted otelcol_receiver_accepted_spans - Spans accepted otelcol_receiver_accepted_metric_points - Metric points accepted otelcol_receiver_refused_* - Refused data (errors) These metrics help identify:\nData ingestion rates Receiver errors or rejections Source-specific throughput patterns Data Egress Metrics Monitor data sent by exporters:\notelcol_exporter_sent_log_records - Log records sent otelcol_exporter_sent_spans - Spans sent otelcol_exporter_sent_metric_points - Metric points sent otelcol_exporter_send_failed_* - Failed export attempts These metrics reveal:\nExport success rates Backend connectivity issues Data loss from failed exports Queue Metrics Monitor internal buffer state:\notelcol_exporter_queue_capacity - Queue capacity in batches otelcol_exporter_queue_size - Current queue utilization otelcol_exporter_enqueue_failed_* - Failed enqueues (buffer full) Alert on: Queue size approaching capacity indicates backpressure from slow exporters or high ingestion rates.\nProcessor Metrics Monitor processor operation:\notelcol_processor_batch_batch_send_size - Batch sizes sent otelcol_processor_batch_timeout_trigger_send - Timeouts triggering sends Processor-specific metrics (sampling rates, dropped data, etc.) Resource Metrics Monitor Collector resource usage:\nprocess_runtime_* - Go runtime metrics (memory, goroutines) process_cpu_seconds_total - CPU time consumed process_resident_memory_bytes - Memory usage For comprehensive metric details, see Internal telemetry and How to Monitor Open Telemetry Collector Performance.\nSelf-Monitoring Dashboards Several platforms provide pre-built dashboards for Collector monitoring:\nDynatrace OpenTelemetry Collector Self-Monitoring (June 2025 release) Grafana dashboards from the community Vendor-specific monitoring integrations Extensions for Observability Extensions provide auxiliary functionality that supports Collector operation and debugging.\nHealth Check Extension The health_check extension enables an HTTP endpoint that can be probed to check the Collector’s status.\nConfiguration:\nextensions: health_check: endpoint: 0.0.0.0:13133 path: /health/status check_collector_pipeline: enabled: false # Not recommended—use at own risk service: extensions: [health_check] Endpoints:\n/health/status - Returns 200 OK if the Collector is running Important note: The check_collector_pipeline feature is not working as expected and should not be used. Use metrics-based monitoring instead for pipeline health.\nUse cases:\nKubernetes liveness probes Load balancer health checks Container orchestration health monitoring Service mesh integration For more details, see Health Check Monitoring With OpenTelemetry.\nzPages Extension The zpages extension serves HTTP endpoints that provide live data for debugging different components without depending on external backends.\nConfiguration:\nextensions: zpages: endpoint: 0.0.0.0:55679 service: extensions: [zpages] Available pages:\n/debug/servicez - Service summary and version information /debug/pipelinez - Pipeline configuration and status /debug/extensionz - Loaded extensions /debug/tracez - Sample trace data (if internal tracing enabled) Use cases:\nInspecting live data flowing through pipelines Validating configuration changes Debugging data transformation issues In-process diagnostics during development zPages are particularly useful for answering questions like “Is the Collector receiving data?” and “What does the data look like after processing?”\nFor detailed usage, see Monitoring and Debugging the OpenTelemetry Collector.\npprof Extension The pprof extension enables the Go net/http/pprof endpoint for performance profiling.\nConfiguration:\nextensions: pprof: endpoint: 0.0.0.0:1777 service: extensions: [pprof] Available profiles:\n/debug/pprof/profile - CPU profile /debug/pprof/heap - Memory allocation profile /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/block - Blocking profile /debug/pprof/mutex - Mutex contention profile Use cases:\nInvestigating CPU hotspots Analyzing memory leaks Identifying goroutine leaks Profiling lock contention Collecting profiles:\n# CPU profile (30 seconds) curl http://localhost:1777/debug/pprof/profile?seconds=30 \u003e cpu.prof # Heap profile curl http://localhost:1777/debug/pprof/heap \u003e heap.prof # Analyze with pprof go tool pprof cpu.prof Security note: pprof endpoints should only be exposed internally, never to the public internet, as they can reveal sensitive information and consume resources.\nLogging and Debugging Structured Logging The Collector emits structured logs to stderr by default, which can be redirected to files or collected by log aggregation systems.\nLog levels:\ndebug - Verbose debugging information info - General operational messages (default) warn - Warning conditions error - Error conditions Configuration:\nservice: telemetry: logs: level: info encoding: json # Options: json, console Common log patterns:\nReceiver connection failures Exporter send failures Processor errors Configuration validation warnings Debug Exporter The debug (logging) exporter writes telemetry data to the Collector’s standard output, useful for confirming that data is being received, processed, and exported correctly.\nConfiguration:\nexporters: debug: verbosity: detailed # Options: basic, normal, detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, otlp] # Add debug alongside production exporters Use cases:\nValidating pipeline configuration Inspecting data transformations Troubleshooting receiver issues Confirming data format Warning: Debug exporters should be removed or disabled in production due to performance impact and log volume.\nFor troubleshooting guidance, see the Troubleshooting documentation.\nCommon Issues and Troubleshooting Data Loss Symptoms:\notelcol_exporter_send_failed_* metrics increasing Queue size approaching capacity Export errors in logs Common causes:\nExporter destination unavailable or slow Collector under-provisioned (insufficient CPU/memory) Network connectivity issues Backend rate limiting Solutions:\nIncrease queue size and retry parameters Scale Collector instances horizontally Add buffering through load balancers Implement backpressure handling High Memory Usage Symptoms:\nprocess_resident_memory_bytes continuously increasing OOM kills in container environments Slow garbage collection Common causes:\nLarge batch sizes Tail sampling buffer accumulation Queue size too large Memory leaks in processors Solutions:\nReduce batch size and timeout Tune tail sampling buffer limits Enable memory limiting in batch processor Update to latest Collector version (bug fixes) Use pprof to identify memory leaks Note: Memory usage increases in steps due to Go’s garbage collection characteristics, which is normal.\nCPU Spikes Symptoms:\nprocess_cpu_seconds_total rate spikes Request latency increases Throttled container CPU Common causes:\nBatch processing overhead Complex processor logic High ingestion rates Inefficient regex patterns in processors Solutions:\nOptimize processor configuration Distribute load across multiple instances Use simpler transformation patterns Profile with pprof to identify hotspots For detailed debugging workflows, see Guide — How to Debug OpenTelemetry Pipelines.\nSelf-Monitoring Architecture A production self-monitoring setup exports Collector telemetry to external systems:\ngraph TB A[OTel Collector Instance] --\u003e|Internal Metrics| B[Prometheus Exporter :8888] A --\u003e|Internal Logs| C[Stderr Logs] A --\u003e|Health Checks| D[Health Check Extension :13133] A --\u003e|Debugging| E[zPages Extension :55679] A --\u003e|Profiling| F[pprof Extension :1777] B --\u003e|Scrape| G[Prometheus/Metrics Backend] C --\u003e|Collect| H[Log Aggregation System] D --\u003e|Probe| I[Kubernetes/Load Balancer] G --\u003e J[Alerting \u0026 Dashboards] H --\u003e J K[Monitoring Collector] --\u003e|Scrape :8888| B K --\u003e|Send to Backends| L[External Observability Platform] style A fill:#e1f5ff style B fill:#fff4e6 style C fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#e8f5e9 style H fill:#e8f5e9 style I fill:#e8f5e9 style J fill:#ffe6e6 style K fill:#e1f5ff style L fill:#e8f5e9 Configuration Best Practices Production Monitoring Setup 1. Enable comprehensive internal metrics:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed logs: level: info encoding: json 2. Deploy monitoring Collector:\nCreate a dedicated Collector instance to scrape other Collectors:\nreceivers: prometheus: config: scrape_configs: - job_name: otel-collector scrape_interval: 15s static_configs: - targets: ['collector-1:8888', 'collector-2:8888'] exporters: otlp: endpoint: monitoring-backend:4317 service: pipelines: metrics: receivers: [prometheus] exporters: [otlp] 3. Configure health checks:\nextensions: health_check: endpoint: 0.0.0.0:13133 service: extensions: [health_check] 4. Set up alerts:\nKey alerts to configure:\nQueue size \u003e 80% capacity Export failure rate \u003e 1% Memory usage \u003e 80% limit CPU throttling detected Receiver refused rate \u003e 0 Development/Debugging Setup Enable all diagnostic extensions:\nextensions: health_check: endpoint: 0.0.0.0:13133 zpages: endpoint: 0.0.0.0:55679 pprof: endpoint: 0.0.0.0:1777 service: extensions: [health_check, zpages, pprof] telemetry: logs: level: debug Add debug exporters:\nexporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, jaeger] # Debug alongside production Security Considerations Restrict extension endpoints to internal networks only Never expose pprof to the internet Use TLS for metrics endpoints in production Implement authentication for sensitive endpoints Rate limit health check endpoints to prevent DoS Integration Points BattleBots Collector Monitoring For the BattleBots platform, Collector self-monitoring would track:\nOperational metrics:\nGame event ingestion rate (log records/second) Battle trace throughput (spans/second) Bot performance metric collection (metric points/second) Health indicators:\nExport success rate to observability backends Queue utilization during peak match activity Resource usage (CPU, memory) per Collector instance Alerting scenarios:\nQueue capacity exceeded during tournament events Export failures to game analytics backend High latency in telemetry pipeline affecting real-time dashboards Kubernetes Integration In Kubernetes deployments:\nLiveness probe:\nlivenessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 30 periodSeconds: 10 Readiness probe:\nreadinessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 5 periodSeconds: 5 Metrics scraping:\nannotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8888\" prometheus.io/path: \"/metrics\" Further Reading Official Documentation Internal telemetry Troubleshooting Configuration Extensions README Extension Documentation Health Check Extension zPages Extension Collector Observability Documentation Guides and Best Practices Monitoring and Debugging the OpenTelemetry Collector How to Monitor Open Telemetry Collector Performance Guide — How to Debug OpenTelemetry Pipelines Health Check Monitoring With OpenTelemetry Vendor Resources Dynatrace: Introducing OpenTelemetry Collector Self-Monitoring Dashboards Dynatrace: OpenTelemetry Collector self-monitoring OpenTelemetry Collector from A to Z: A Production-Ready Guide Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces ","categories":"","description":"How to observe and debug the OpenTelemetry Collector itself through internal telemetry, extensions, and monitoring strategies.\n","excerpt":"How to observe and debug the OpenTelemetry Collector itself through …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-self-monitoring/","tags":"","title":"OpenTelemetry Collector: Self-Monitoring"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for distributed tracing, enabling collection, processing, and export of trace data from multiple sources to various backend systems. Distributed tracing tracks requests as they flow through distributed systems, providing visibility into service interactions, latency bottlenecks, and error propagation paths.\nThe Collector acts as a central hub for trace data, accepting traces in multiple formats (OTLP, Jaeger, Zipkin), performing intelligent sampling decisions, and routing to multiple tracing backends simultaneously. This unified approach simplifies observability infrastructure while preserving the ability to use best-of-breed tools for different use cases.\nKey capabilities include protocol translation between trace formats, sophisticated sampling strategies (head-based and tail-based), trace enrichment with resource and span attributes, and correlation with metrics and logs through shared context identifiers.\nKey Concepts Traces and Spans A trace represents the full journey of one request or transaction across services, while a span is a timed unit of work inside that journey such as a function call, database query, or external API call.\nTrace structure:\nA trace consists of one or more spans organized in a tree structure Each span represents an operation with a start time and duration Spans have parent-child relationships forming the call graph The root span represents the initial request entry point Span characteristics:\nName: Describes the operation (e.g., “GET /api/battles”) Start time and duration: Timing information Status: Success, error, or unset Span kind: Client, server, internal, producer, or consumer Span Context and Propagation Span context is the portion of a span that must be serialized and propagated between services to maintain trace continuity.\nContext components:\nTraceId: Unique identifier for the entire trace (shared across all spans) SpanId: Unique identifier for the specific span TraceFlags: Sampling and other flags TraceState: System-specific trace state values Propagation mechanism:\nContext propagation transmits context between services via protocols such as HTTP headers, gRPC metadata, or message queues. The default propagator uses the W3C TraceContext specification with traceparent and tracestate headers.\nExample HTTP headers:\ntraceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01 tracestate: vendor1=value1,vendor2=value2 For detailed propagation concepts, see An overview of Context Propagation in OpenTelemetry.\nSpan Attributes Attributes provide additional context about the operation represented by a span. They are key-value pairs that describe request parameters, database queries, HTTP methods, status codes, and other relevant details.\nCommon attribute categories:\nHTTP attributes: http.method, http.status_code, http.route Database attributes: db.system, db.statement, db.name RPC attributes: rpc.service, rpc.method Network attributes: net.peer.name, net.peer.port Best practice: Set attributes at span creation rather than later, since samplers can only consider information present during span creation.\nSpan Events Span events are structured log messages or annotations on a span, typically used to denote meaningful singular points in time during the span’s duration.\nUse cases:\nException events (including stack traces) Checkpoint markers in long operations State transitions Cache hits/misses Retry attempts Events include a name, timestamp, and optional attributes, providing detailed debugging context without creating separate spans for every sub-operation.\nSpan Links Span links establish relationships between spans in different traces or between causally-related but non-parent-child spans. Common scenarios include:\nBatch processing where one span processes multiple input messages Following redirects across multiple traces Async operations spawned from a parent request Trace Receivers Receivers collect trace data from various sources and convert it into the OpenTelemetry traces data model.\nOTLP Receiver The OTLP receiver accepts trace data transmitted using the OpenTelemetry Protocol, the native and recommended format for OpenTelemetry traces.\nSupported transports:\ngRPC (default port 4317): High-performance binary protocol HTTP (default port 4318): RESTful endpoint at /v1/traces Configuration example:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 Use cases:\nCollecting traces from OpenTelemetry-instrumented applications Multi-tier collector deployments (agent → gateway) Modern observability architectures Important: Jaeger V2 natively supports OTLP, making OTLP the recommended protocol for Jaeger backends.\nJaeger Receiver The Jaeger receiver receives trace data in Jaeger format and translates it to OpenTelemetry format. This enables migration from Jaeger-instrumented applications without requiring code changes.\nSupported protocols:\ngRPC (default port 14250): Binary Jaeger protocol thrift_compact (default port 6831): UDP-based compact Thrift thrift_http (default port 14268): HTTP-based Thrift thrift_binary: TCP-based binary Thrift Configuration example:\nreceivers: jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_http: endpoint: 0.0.0.0:14268 thrift_compact: endpoint: 0.0.0.0:6831 Use cases:\nMigrating from Jaeger agent/collector infrastructure Supporting legacy applications instrumented with Jaeger SDKs Gradual transition to OpenTelemetry Zipkin Receiver The Zipkin receiver receives spans in Zipkin V1 and V2 formats and translates them to OpenTelemetry format.\nConfiguration example:\nreceivers: zipkin: endpoint: 0.0.0.0:9411 Use cases:\nMigrating from Zipkin instrumentation Supporting applications instrumented with Zipkin libraries Integration with Zipkin-compatible systems Protocol Translation The Collector acts as a protocol translator, accepting traces in one format and exporting in another. This enables:\nJaeger-instrumented apps → OTLP export to modern backends OpenTelemetry apps → Zipkin export for legacy systems Unified collection from heterogeneous instrumentation Sampling Strategies Sampling controls which traces are retained for analysis, balancing observability value with storage costs and performance impact.\nHead Sampling Head sampling makes sampling decisions at trace creation time, before seeing the complete trace. The decision applies to the entire trace and propagates to downstream services.\nCommon algorithms:\nAlways On: Sample 100% of traces (development/debugging) Always Off: Sample 0% of traces (disable tracing) TraceID Ratio: Sample a percentage based on TraceId hash (e.g., 10%) Rate Limiting: Sample at most N traces per second Advantages:\nLow latency decision (immediate) Low memory overhead (no buffering) Consistent across distributed services Limitations:\nCannot make decisions based on complete trace data Cannot guarantee capturing all error traces Cannot sample based on span attributes or duration For consistent probability sampling details, see OpenTelemetry Sampling.\nTail Sampling Tail sampling makes sampling decisions after seeing all or most spans in a trace, enabling more intelligent sampling based on trace characteristics.\nAvailable policies:\nLatency: Sample traces exceeding duration threshold Status code: Always sample traces with errors Numeric attribute: Sample based on attribute values (min/max thresholds) Probabilistic: Sample a percentage of traces String attribute: Sample traces matching string attributes Rate limiting: Limit traces per second per policy Composite: Combine multiple policies (AND/OR logic) Configuration example:\nprocessors: tail_sampling: policies: - name: errors-policy type: status_code status_code: status_codes: [ERROR] - name: slow-requests type: latency latency: threshold_ms: 1000 - name: sample-10-percent type: probabilistic probabilistic: sampling_percentage: 10 Architecture requirements:\nAll spans for a given trace MUST be received by the same collector instance for effective sampling decisions. This requires:\nLoad balancing exporter: Routes spans by TraceId to consistent collectors Two-tier architecture: Agent collectors → tail sampling gateway collectors For implementation guidance, see Tail Sampling with OpenTelemetry and New Relic and Sampling at scale with OpenTelemetry.\nAdvantages:\nSample all error traces regardless of volume Capture slow requests while dropping fast ones Make sampling decisions based on complete trace data Challenges:\nHigher memory overhead (buffering complete traces) Increased latency (waiting for trace completion) Requires stateful, coordinated collectors Sampling Best Practices For production deployments:\nUse head sampling for baseline traffic reduction (e.g., 10% sampling) Add tail sampling to always capture errors and slow traces Implement two-tier architecture for tail sampling at scale Monitor sampled vs. unsampled trace ratios Adjust policies based on traffic patterns and costs For recent sampling updates, see OpenTelemetry Sampling update.\nTrace Processors Processors transform and enrich trace data as it flows through pipelines.\nSpan Processor The span processor modifies span names, attributes, and other properties.\nCommon operations:\nRename spans for consistency Add/remove span attributes Set span status Modify span kind Attributes Processor Adds, updates, or deletes span and resource attributes, enabling:\nEnvironment labeling Team ownership tags PII removal Attribute normalization Resource Detection Processor Enriches traces with environment metadata:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information Host details This automatic enrichment enables filtering and grouping traces by infrastructure context.\nBatch Processor Groups spans before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nService Graph Processor Generates metrics representing service call relationships from trace data, creating:\nRequest rate between services Error rate between services Latency between services These derived metrics enable service dependency visualization without query-time trace aggregation.\nTrace Exporters Exporters send processed trace data to observability backends and storage systems.\nOTLP Exporter (Recommended) The OTLP exporter sends traces using the OpenTelemetry Protocol to OTLP-compatible backends. OTLP is the recommended choice for new deployments as it’s designed with the OpenTelemetry data model in mind, emitting trace data without loss of information.\nSupported destinations:\nJaeger V2 (native OTLP support) Commercial platforms (Datadog, New Relic, Honeycomb, Dynatrace) Cloud vendor endpoints (AWS X-Ray, Google Cloud Trace, Azure Monitor) Open source backends (Uptrace, Grafana Tempo) Configuration example:\nexporters: otlp: endpoint: jaeger:4317 tls: insecure: false For Jaeger V2 integration, see Using OpenTelemetry to send traces to Jaeger V2.\nJaeger Exporter The Jaeger exporter sends traces to Jaeger backends using the Jaeger gRPC protocol.\nNote: For Jaeger V2, use the OTLP exporter instead. The dedicated Jaeger exporter is maintained for backward compatibility with Jaeger V1 deployments.\nZipkin Exporter The Zipkin exporter sends traces to Zipkin-compatible backends.\nUse cases:\nLegacy Zipkin deployments Systems expecting Zipkin format Gradual migration scenarios Logging Exporter Writes traces to collector standard output for debugging and development.\nFor comprehensive exporter documentation, see OpenTelemetry Collector Exporters.\nTrace Pipeline Flow A sophisticated trace pipeline with multiple receivers, sampling, and multi-backend export:\ngraph LR A[OTLP SDK] --\u003e B[OTLP Receiver] C[Jaeger SDK] --\u003e D[Jaeger Receiver] E[Zipkin SDK] --\u003e F[Zipkin Receiver] B --\u003e G[Resource Detection] D --\u003e G F --\u003e G G --\u003e H[Attributes Processor] H --\u003e I{Load Balancer} I --\u003e|By TraceId| J[Tail Sampling Processor] J --\u003e K[Batch Processor] K --\u003e L[OTLP Exporter] L --\u003e M[Jaeger V2] K --\u003e N[OTLP Exporter] N --\u003e O[Cloud Backend] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#f3e5f5 style L fill:#e8f5e9 style N fill:#e8f5e9 style M fill:#e1f5ff style O fill:#e1f5ff Configuration Considerations Context Propagation Ensure consistent propagation across all services:\nConfigure the same propagators in all SDKs Use W3C TraceContext (standard default) Include Baggage propagation if using cross-cutting concerns Test propagation across language boundaries Sampling Trade-offs Balance observability and cost:\nHigh sampling (50-100%): Development, debugging, low-traffic systems Medium sampling (10-30%): Production with moderate traffic Low sampling (1-10%): High-traffic production systems Tail sampling: Always capture errors regardless of base rate Performance Tuning For high-throughput trace collection:\nEnable batching with appropriate size/timeout Use multiple collector instances with load balancing Configure adequate memory for tail sampling buffers Monitor collector CPU and memory usage Consider two-tier architecture (agent + gateway) Storage Optimization Manage trace storage costs:\nImplement retention policies in backends Use sampling to reduce volume Drop high-cardinality attributes if needed Compress trace data before export Integration Points BattleBots Trace Collection For the BattleBots platform, distributed tracing would track:\nRequest flows:\nClient WebSocket connection → authentication → game state sync Player action → validation → state update → broadcast Match creation → bot pairing → game initialization Service interactions:\nAPI gateway → game service → persistence layer Event publisher → message broker → subscriber services Load balancer → multiple game server instances Timing analysis:\nEnd-to-end battle action latency Database query performance WebSocket message propagation time The OTLP receiver collects traces from Go services instrumented with the OpenTelemetry Go SDK, while Jaeger/Zipkin receivers support any legacy instrumentation.\nTrace-Log Correlation Connecting traces and logs enables powerful debugging workflows:\nStart with trace: Identify slow or failing request Find associated logs: Query logs by TraceId and SpanId Examine context: Read detailed log messages and exceptions Understand causation: See timeline of events leading to issue This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically when both signals are instrumented.\nTrace-Metric Correlation Link traces and metrics through exemplars:\nHistogram buckets contain sample trace IDs Click from high-latency metric to example slow trace Correlate error rate spike with specific failing traces Validate fixes by monitoring metrics and inspecting traces Further Reading Official Documentation OpenTelemetry Traces Specification Context Propagation Sampling Receiver Components Processor Components Exporter Components Sampling Resources Tail Sampling Processor OpenTelemetry Sampling Tail Sampling with OpenTelemetry and New Relic Sampling at scale with OpenTelemetry OpenTelemetry Sampling update Integration Guides Getting Started with the Jaeger and Zipkin Receivers Using OpenTelemetry to send traces to Jaeger V2 OpenTelemetry Collector Exporters Context Propagation Deep Dives An overview of Context Propagation in OpenTelemetry OpenTelemetry Context Propagation Explained Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles distributed tracing data, including the span model, receivers, sampling strategies, and exporters.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles distributed …","ref":"/battlebots/pr-preview/pr-150/research_and_development/analysis/observability/otel-collector/otel-collector-traces/","tags":"","title":"OpenTelemetry Collector: Traces Support"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-150/tags/","tags":"","title":"Tags"}]