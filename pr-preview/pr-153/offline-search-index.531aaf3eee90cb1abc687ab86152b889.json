[{"body":"Overview gRPC is a modern, open-source, high-performance Remote Procedure Call (RPC) framework developed by Google. It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features like authentication, bidirectional streaming, and flow control. This analysis evaluates gRPC’s suitability for the Battle Bots bot-to-server communication interface.\nPerformance Characteristics Latency Unary RPC: Typically 1-2ms for local networks, comparable to optimized REST Streaming RPC: Near-zero latency for subsequent messages after connection establishment HTTP/2 multiplexing: Multiple streams over single TCP connection reduces overhead Binary protocol: Faster parsing than JSON/XML text protocols Throughput Bidirectional streaming: Can handle thousands of messages per second Flow control: Built-in backpressure prevents overwhelming slower clients Compression: gzip compression available for larger payloads Connection reuse: HTTP/2 persistent connections reduce handshake overhead Resource Overhead Memory: Protocol Buffer deserialization is memory-efficient CPU: Binary encoding/decoding faster than JSON parsing Network: Compact binary format reduces bandwidth usage by 30-50% vs JSON Benchmarks Industry benchmarks show gRPC typically delivers:\n7-10x better throughput than REST/JSON 20-30% lower latency for streaming scenarios 40-50% smaller payload sizes vs JSON Streaming Capabilities Unary RPC (Request-Response) rpc SubmitAction(BotAction) returns (ActionResult) {} Single request, single response Suitable for discrete bot actions (move, attack) Similar to REST API calls Server Streaming rpc WatchBattleState(BattleId) returns (stream GameState) {} Single request, stream of responses Ideal for pushing game state updates to bots Server controls message flow Client Streaming rpc BatchActions(stream BotAction) returns (BatchResult) {} Stream of requests, single response Useful for queuing multiple actions Less common in game networking Bidirectional Streaming rpc Battle(stream BotAction) returns (stream GameEvent) {} Both client and server send streams independently Perfect for real-time game interaction Bots send actions, receive continuous state/events Natural fit for turn-based and real-time battles Flow Control and Backpressure HTTP/2 flow control prevents fast senders from overwhelming receivers Application-level backpressure via streaming APIs Configurable window sizes for buffering Language \u0026 Platform Support Official Language Support gRPC has official support for:\nGo - Excellent, idiomatic integration (ideal for game server) Python - Mature, widely used for ML-based bots Java/Kotlin - Production-ready JavaScript/TypeScript - Node.js and browser support C++ - High-performance native implementation C# - Full .NET integration Rust - tonic library, growing ecosystem Ruby, PHP, Dart, Objective-C - Community support Code Generation protoc compiler generates client/server stubs Language-specific plugins for idiomatic code Type-safe interfaces reduce runtime errors Automatic serialization/deserialization Developer Experience Pros:\nStrong typing catches errors at compile time Self-documenting .proto schema files Consistent API across all languages Rich ecosystem of tools and libraries Cons:\nLearning curve for Protocol Buffers syntax Code generation step in build process Less human-readable than JSON (debugging) Cross-Platform Compatibility Works on Linux, macOS, Windows Container-friendly (Docker, Podman) Mobile platform support (iOS, Android) Browser support via grpc-web (requires proxy) OpenTelemetry Integration Native Instrumentation Support gRPC has excellent OpenTelemetry integration:\nAutomatic Instrumentation:\nTrace spans automatically created for each RPC Context propagation via gRPC metadata No manual span creation required for basic telemetry Metrics Collection:\nRPC duration (client and server-side) Request/response sizes Success/error rates Active connections and streams Language-specific OTEL libraries provide auto-instrumentation Example (Go):\nimport ( \"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\" ) server := grpc.NewServer( grpc.UnaryInterceptor(otelgrpc.UnaryServerInterceptor()), grpc.StreamInterceptor(otelgrpc.StreamServerInterceptor()), ) Integration with Battle Bots OTLP Stack From ADR-0002 (OpenTelemetry SDK) and ADR-0003 (Observability Stack):\nTraces → Tempo (via OTLP) Metrics → Mimir (via OTLP) Logs → Loki (via OTLP) All integrate seamlessly with gRPC’s OTEL instrumentation Battle-specific Telemetry:\nSpan attributes for bot IDs, action types Custom metrics for game-specific events Distributed tracing across bot → server → storage Correlation of battle state with RPC calls Container Networking HTTP/2 Compatibility Excellent container support: HTTP/2 works natively in Docker/Podman Port mapping: Single port for all RPC methods (e.g., 50051) No special container configuration required Service Discovery Docker networks: Bots can resolve server by service name Kubernetes: Native Service discovery integration podman-compose: DNS-based service names work out of box Connection Management Connection pooling: gRPC clients maintain persistent connections Keepalive: Configurable HTTP/2 keepalive pings Reconnection: Automatic retry and backoff strategies Health checking: gRPC health check protocol Load Balancing Client-side load balancing: Built-in support Proxy load balancing: Works with Envoy, nginx, HAProxy Round-robin, least-request algorithms: Configurable NAT Traversal for P2P Challenges:\nHTTP/2 expects client-initiated connections P2P requires bots to act as both client AND server NAT hole-punching more complex with TCP vs UDP Solutions:\nUse rendezvous server for initial handshake Bots expose gRPC server on known ports STUN/TURN-like relay for unreachable bots Or leverage gRPC’s bidirectional streaming (one connection, two-way communication) Development Experience Tooling Ecosystem Protocol Development:\nprotoc - Protocol Buffer compiler buf - Modern protobuf toolchain (linting, breaking change detection) IDE plugins for .proto syntax highlighting and validation Testing:\ngrpcurl - curl-like tool for gRPC (manual testing) ghz - Benchmarking and load testing tool Built-in reflection for service discovery Mock server generation for unit tests Debugging:\ngRPC reflection for runtime introspection Interceptors for logging, debugging Wireshark protocol dissector for packet analysis Browser DevTools via grpc-web Documentation:\nProtocol Buffers are self-documenting Tools like protoc-gen-doc generate HTML/Markdown docs OpenAPI can be generated from .proto files Learning Curve For Bot Developers:\nLow barrier: Install client library, import generated code Moderate protobuf learning: Understanding .proto syntax Language familiarity: Use gRPC in familiar programming language For Platform Developers:\nModerate setup: Learning protoc, code generation Service design: Defining effective RPC interfaces Streaming patterns: Understanding bidirectional communication Implementation Complexity Client/Server Mode:\nSimple: Bots are gRPC clients, server is gRPC server Battle server implements service interface Clients use generated stubs P2P Mode:\nModerate complexity: Each bot runs gRPC server Bots act as both client and server Connection management more complex Discovery and coordination needed Pros and Cons Summary Advantages ✅ Performance: Fast binary protocol with low latency and high throughput\n✅ Bidirectional streaming: Natural fit for real-time battle communication\n✅ Strong typing: Protocol Buffers provide type safety and validation\n✅ Language-agnostic: Excellent support across all major languages\n✅ OpenTelemetry integration: Native instrumentation, seamless OTLP stack integration\n✅ Container-friendly: HTTP/2 works excellently in Docker/Podman\n✅ Versioning: Built-in backward/forward compatibility in protobuf\n✅ Tooling: Rich ecosystem for testing, debugging, documentation\n✅ Industry adoption: Proven at scale by Google, Netflix, Square\n✅ Code generation: Reduces boilerplate, enforces contracts\nDisadvantages ❌ Learning curve: Requires understanding Protocol Buffers\n❌ Build complexity: Code generation step in build pipeline\n❌ Browser support: Requires grpc-web proxy (not native browser)\n❌ Debugging: Binary format less human-readable than JSON\n❌ P2P complexity: NAT traversal more challenging with HTTP/2\n❌ Firewall traversal: Some networks block non-80/443 ports\n❌ Less flexible: Schema changes require .proto updates and recompilation\nSuitability for Battle Bots Client/Server Architecture: ⭐⭐⭐⭐⭐ (Excellent)\nBidirectional streaming perfect for real-time battles Strong OTEL integration meets observability requirements Language support enables diverse bot ecosystem P2P Architecture: ⭐⭐⭐⭐☆ (Good with caveats)\nBots can run as gRPC servers NAT traversal requires additional coordination Connection complexity manageable with proper design References gRPC Official Documentation Protocol Buffers Documentation gRPC Performance Best Practices OpenTelemetry gRPC Instrumentation gRPC Go Examples HTTP/2 Specification (RFC 7540) grpcurl - gRPC curl tool ghz - gRPC benchmarking tool ","categories":"","description":"Analysis of gRPC as a communication protocol for bot-to-server interface\n","excerpt":"Analysis of gRPC as a communication protocol for bot-to-server …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/protocols/grpc/","tags":"","title":"gRPC Protocol Analysis"},{"body":"Idea This proof of concept demonstrates containerized bots battling each other in a 1v1 match within a 2-dimensional space. To evaluate the optimal architecture for the final BattleBots platform, both a client/server implementation and a peer-to-peer implementation will be completed and compared.\nRequirements Client/Server Architecture Implement a client/server architecture to evaluate its suitability for the final BattleBots platform.\nPeer-to-Peer Architecture Implement a peer-to-peer architecture to compare against the client/server approach.\n1v1 Battle Bots compete in a 1v1 battle within a 2-dimensional space.\nContainerized Bots Each bot should be a container to ensure isolation and portability.\nLanguage-Agnostic Bot Implementation The game logic should be independent of each bot so that any programming language can be used to implement a bot.\nObservability Observability signals should be captured so the battle can be monitored in real-time.\nBattle Visualization A battle visualization should be implemented to display the battle state and actions.\nPending ADRs Game Mechanics ADRs (0005-0009) The core game mechanics for 1v1 battles are defined across multiple ADRs:\nADR-0005: BattleBot Universe Topological Properties: Mathematical and topological foundation defining the spatial structure (2D Euclidean space, Cartesian coordinates, rectangular boundaries) ADR-0006: Bot Characteristics System: Four-stat system (Health, Speed, Defense, Mass) defining bot capabilities ADR-0007: Equipment and Loadout System: Equipment-based customization enabling diverse bot builds ADR-0008: Bot Actions and Resource Management: Dual-constraint action system with energy costs and cooldowns See also Game Mechanics Analysis for detailed technical specifications.\nADR-NNNN: Client/Server Architecture This ADR will document the design decisions for the client/server implementation, including the server’s responsibilities for game state management, turn coordination, and validation of bot actions.\nADR-NNNN: Peer-to-Peer Architecture This ADR will document the design decisions for the peer-to-peer implementation, including consensus mechanisms for game state, conflict resolution, and how bots communicate directly with each other.\nADR-NNNN: Game Runtime Architecture This ADR will define the “game loop” and core game mechanics, including turn-based vs. real-time gameplay, tick rates, state updates, and the overall flow of battle execution.\nADR-NNNN: Bot to Battle Server Interface This ADR will define the communication protocol between bots and the battle server or between bots in P2P mode, evaluating options such as gRPC, HTTP, or custom TCP/UDP packets.\nADR-NNNN: Observability Stack This ADR will document the observability architecture, including metrics collection, logging, tracing, and how battle telemetry is captured and exposed for monitoring and analysis.\nADR-NNNN: Battle Visualization This ADR will document the design of the battle visualization system, including the rendering approach, real-time updates, and how observability data is translated into visual representations.\n","categories":"","description":"Documents the proof of concept for running a 1v1 battle between two containerized bots using podman-compose\n","excerpt":"Documents the proof of concept for running a 1v1 battle between two …","ref":"/battlebots/pr-preview/pr-153/research_and_development/user-journeys/0001-poc/","tags":"","title":"[0001] Proof of Concept - 1v1 Battle"},{"body":" Context and Problem Statement As the project grows, architectural decisions are made that have long-term impacts on the system’s design, maintainability, and scalability. Without a structured way to document these decisions, we risk losing the context and rationale behind important choices, making it difficult for current and future team members to understand why certain approaches were taken.\nHow should we document architectural decisions in a way that is accessible, maintainable, and provides sufficient context for future reference?\nDecision Drivers Need for clear documentation of architectural decisions and their rationale Easy accessibility and searchability of past decisions Low barrier to entry for creating and maintaining decision records Integration with existing documentation workflow Version control friendly format Industry-standard approach that team members may already be familiar with Considered Options MADR (Markdown Architectural Decision Records) ADR using custom format Wiki-based documentation No formal ADR process Decision Outcome Chosen option: “MADR (Markdown Architectural Decision Records)”, because it provides a well-established, standardized format that is lightweight, version-controlled, and integrates seamlessly with our existing documentation structure. MADR 4.0.0 offers a clear template that captures all necessary information while remaining flexible enough for different types of decisions.\nConsequences Good, because MADR is a widely adopted standard with clear documentation and examples Good, because markdown files are easy to create, edit, and review through pull requests Good, because ADRs will be version-controlled alongside code, maintaining historical context Good, because the format is flexible enough to accommodate strategic, user-journey, and API design decisions Good, because team members can easily search and reference past decisions Neutral, because requires discipline to maintain and update ADR status as decisions evolve Bad, because team members need to learn and follow the MADR format conventions Confirmation Compliance will be confirmed through:\nCode reviews ensuring new architectural decisions are documented as ADRs ADRs are stored in docs/content/r\u0026d/adrs/ following the naming convention NNNN-title-with-dashes.md Regular reviews during architecture discussions to reference and update existing ADRs Pros and Cons of the Options MADR (Markdown Architectural Decision Records) MADR 4.0.0 is a standardized format for documenting architectural decisions using markdown.\nGood, because it’s a well-established standard with extensive documentation Good, because markdown is simple, portable, and version-control friendly Good, because it provides a clear structure while remaining flexible Good, because it integrates with static site generators and documentation tools Good, because it’s lightweight and doesn’t require special tools Neutral, because it requires some initial learning of the format Neutral, because maintaining consistency requires discipline ADR using custom format Create our own custom format for architectural decision records.\nGood, because we can tailor it exactly to our needs Bad, because it requires defining and maintaining our own standard Bad, because new team members won’t be familiar with the format Bad, because we lose the benefits of community knowledge and tooling Bad, because it may evolve inconsistently over time Wiki-based documentation Use a wiki system (like Confluence, Notion, or GitHub Wiki) to document decisions.\nGood, because wikis provide easy editing and hyperlinking Good, because some team members may be familiar with wiki tools Neutral, because it may or may not integrate with version control Bad, because content may not be version-controlled alongside code Bad, because it creates a separate system to maintain Bad, because it’s harder to review changes through standard PR process Bad, because portability and long-term accessibility may be concerns No formal ADR process Continue without a structured approach to documenting architectural decisions.\nGood, because it requires no additional overhead Bad, because context and rationale for decisions are lost over time Bad, because new team members struggle to understand why decisions were made Bad, because it leads to repeated discussions of previously settled questions Bad, because it makes it difficult to track when decisions should be revisited More Information MADR 4.0.0 specification: https://adr.github.io/madr/ ADRs will be categorized as: strategic, user-journey, or api-design ADR status values: proposed | accepted | rejected | deprecated | superseded by ADR-XXXX All ADRs are stored in docs/content/r\u0026d/adrs/ directory ","categories":"","description":"Adopt Markdown Architectural Decision Records (MADR) as the standard format for documenting architectural decisions in the project.\n","excerpt":"Adopt Markdown Architectural Decision Records (MADR) as the standard …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0001-use-madr-for-architecture-decision-records/","tags":"","title":"[0001] Use MADR for Architecture Decision Records"},{"body":"Context and Problem Statement Battle Bots requires comprehensive observability to monitor game server performance, track bot behavior, debug issues, and visualize battle state in real-time. We need to select an observability SDK that will provide instrumentation for metrics, traces, and logs across our system. The choice will impact vendor lock-in, cost, flexibility, and integration complexity.\nWhich observability SDK should we adopt for instrumenting the Battle Bots platform?\nDecision Drivers Vendor neutrality and ability to switch backends Support for metrics, traces, and logs (unified observability) Language support (especially for our game server implementation) Integration with container environments Community adoption and long-term sustainability Cost implications (SDK licensing, vendor fees) Ease of integration and developer experience Performance overhead Support for custom attributes and semantic conventions Considered Options OpenTelemetry Datadog SDK Sentry SDK Decision Outcome Chosen option: OpenTelemetry, because it is an open standard that prevents vendor lock-in and aligns with the open-source nature of the Battle Bots project. As a CNCF-graduated project, it provides long-term sustainability and broad industry adoption while maintaining flexibility to switch observability backends without re-instrumentation.\nConsequences Good, because we maintain complete vendor neutrality and can switch backends (Prometheus, Jaeger, Loki, etc.) without changing instrumentation code Good, because we adopt an industry-standard approach to observability that is widely supported and documented Good, because the open-source SDK has no licensing costs and aligns with project philosophy Good, because semantic conventions will ensure consistent telemetry across all Battle Bots components Good, because comprehensive language support enables bot developers to use OpenTelemetry in their preferred languages Bad, because we need to deploy and manage separate backend infrastructure for metrics, traces, and logs Bad, because initial setup requires more configuration compared to all-in-one commercial solutions Bad, because debugging telemetry pipelines may require deeper understanding of the OpenTelemetry architecture Confirmation This decision will be considered successful when:\nGame server successfully exports metrics, traces, and logs via OpenTelemetry SDK Observability backends (selected via future ADRs) receive and display telemetry data correctly Developer experience for adding custom instrumentation is straightforward Performance overhead of instrumentation is acceptable (\u003c 5% CPU/memory impact) We can successfully switch between different backend providers without code changes Pros and Cons of the Options OpenTelemetry OpenTelemetry is an open-source observability framework providing vendor-neutral APIs, SDKs, and tools for generating and collecting telemetry data (metrics, logs, and traces).\nGood, because it is vendor-neutral and prevents lock-in to any specific observability backend Good, because it supports all three pillars of observability (metrics, traces, logs) Good, because it has broad language support including Go, Java, Python, JavaScript, and many others Good, because it is CNCF-graduated with strong industry adoption and long-term sustainability Good, because it allows flexibility to switch backends (Prometheus, Jaeger, Loki, commercial vendors) without changing instrumentation Good, because it has native Kubernetes and container environment support Good, because it defines semantic conventions for consistent telemetry across services Good, because it is free and open-source with no licensing costs Neutral, because it requires separate backend infrastructure (exporters to Prometheus, Jaeger, etc.) Bad, because initial setup complexity is higher than vendor-specific SDKs Bad, because some advanced features may require vendor-specific extensions Datadog SDK Datadog provides proprietary SDKs for instrumenting applications and sending telemetry to the Datadog platform.\nGood, because it offers a unified, fully-managed observability platform Good, because it provides excellent out-of-the-box dashboards and visualizations Good, because it has automatic instrumentation for many frameworks and libraries Good, because it offers strong APM (Application Performance Monitoring) features Good, because it has excellent documentation and developer experience Good, because it includes alerting, incident management, and collaboration features Neutral, because it requires a Datadog account and subscription Bad, because it creates vendor lock-in - switching away requires significant re-instrumentation Bad, because it has ongoing per-host/container costs that scale with usage Bad, because telemetry data is only compatible with Datadog’s backend Bad, because it may not align with open-source philosophy of the project Sentry SDK Sentry is primarily an error tracking and performance monitoring platform with SDKs for multiple languages.\nGood, because it excels at error tracking and exception reporting Good, because it has good performance monitoring capabilities Good, because it offers distributed tracing Good, because it has strong developer experience and debugging tools Good, because it provides issue grouping and notification features Good, because it has a generous free tier for open-source projects Neutral, because it focuses more on errors/exceptions than comprehensive observability Bad, because metrics support is less mature compared to dedicated observability platforms Bad, because it creates vendor lock-in similar to Datadog Bad, because log aggregation is not a primary feature Bad, because it’s more specialized for error tracking than full observability More Information Related Research Analysis documents on observability backends are available in docs/content/research_and_development/analysis/observability/ The POC user journey (ADR-0000 reference pending) identifies observability as a critical requirement Implementation Considerations If OpenTelemetry is chosen, separate ADRs will be needed for: Metrics backend selection (e.g., Prometheus, Mimir) Tracing backend selection (e.g., Jaeger, Tempo) Log aggregation backend selection (e.g., Loki) OpenTelemetry Collector deployment strategy If Datadog or Sentry is chosen, consider OpenTelemetry compatibility for future flexibility Consider hybrid approaches (e.g., OpenTelemetry for instrumentation + commercial backend) Questions to Resolve What is the expected scale of telemetry data (events/sec, log volume)? What is the budget for observability infrastructure and services? Is vendor neutrality a hard requirement or nice-to-have? What are the specific visualization and alerting requirements? Will bots require observability instrumentation, or only game server? ","categories":"","description":"Select OpenTelemetry as the observability SDK for instrumenting metrics, traces, and logs across the Battle Bots platform.\n","excerpt":"Select OpenTelemetry as the observability SDK for instrumenting …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0002-observability-sdk-selection/","tags":"","title":"[0002] Observability SDK Selection"},{"body":"Overview HTTP-based protocols represent the most widely-adopted communication patterns on the web. This analysis evaluates three approaches for Battle Bots bot-to-server communication:\nHTTP/REST - Request/response pattern with polling or long-polling WebSockets - Full-duplex bidirectional communication over TCP Server-Sent Events (SSE) - Unidirectional server-to-client streaming Each approach offers different trade-offs in complexity, performance, and developer familiarity.\nHTTP/REST Analysis Architecture Pattern Traditional Request/Response:\nBot → POST /battles/{id}/actions → Server Bot ← 200 OK {result} ← Server Bot → GET /battles/{id}/state → Server Bot ← 200 OK {gameState} ← Server RESTful API Design:\nPOST /battles/{id}/actions - Submit bot action GET /battles/{id}/state - Poll for current game state GET /battles/{id}/events?since={timestamp} - Fetch events since last poll DELETE /battles/{id}/connections/{botId} - Disconnect from battle State Updates Polling:\nBot repeatedly requests /state endpoint (e.g., every 100ms) Simple to implement High latency (up to poll interval) Wasted requests when no state changes Long-Polling:\nServer holds request open until state changes Immediately returns new state when available Better latency than polling, worse than streaming Reconnection overhead between state changes Performance Characteristics Latency:\nPolling: 50-500ms depending on interval Long-polling: 10-100ms with reconnection overhead HTTP/1.1: Head-of-line blocking, connection limits HTTP/2: Multiplexing improves concurrent requests Throughput:\nLimited by request/response cycle time High overhead for frequent small updates Better for discrete actions than continuous state Resource Usage:\nConnection churning with polling/long-polling Server load from repeated requests Client CPU from polling loops Advantages ✅ Universal language support (every language has HTTP client)\n✅ Simple mental model - stateless request/response\n✅ Easy debugging with curl, browser DevTools\n✅ RESTful conventions well-understood\n✅ HTTP caching for read-heavy scenarios\n✅ Firewall-friendly (port 80/443)\nDisadvantages ❌ Poor real-time performance without long-polling\n❌ Polling wastes bandwidth and CPU\n❌ High latency for state updates\n❌ No server push (without long-polling hacks)\n❌ Inefficient for streaming game state\nSuitability for Battle Bots ⭐⭐☆☆☆ (Marginal) - Only viable for very slow turn-based games with infrequent updates.\nWebSockets Analysis Architecture Pattern Full-Duplex Communication:\nBot → WS Connect ws://server/battle/123 → Server ← WS Upgrade (101 Switching Protocols) ← Bot → {\"action\": \"move\", \"x\": 10} → ← {\"event\": \"state_update\", \"positions\": [...]} ← Bot → {\"action\": \"attack\", \"target\": \"bot2\"} → ← {\"event\": \"damage\", \"target\": \"bot2\", \"hp\": 50} ← Persistent Connection:\nSingle TCP connection for bidirectional messages No HTTP overhead after handshake Both sides can send messages anytime Connection stays open for battle duration Message Framing Text Frames:\nJSON messages (human-readable) Easy debugging and development Larger payload sizes Binary Frames:\nMessagePack, Protocol Buffers, CBOR Efficient encoding/decoding Smaller payload sizes (30-50% reduction) Performance Characteristics Latency:\n5-20ms for message round-trip (local network) Near-zero after connection establishment No polling overhead Comparable to gRPC streaming Throughput:\nThousands of messages per second per connection Limited by TCP bandwidth, not protocol overhead Efficient for rapid state updates Resource Usage:\nOne TCP connection per bot (persistent) Lower CPU than polling (no repeated handshakes) Memory for message buffering Connection scaling considerations (1000s of concurrent bots) Connection Management Lifecycle:\nHTTP upgrade handshake Persistent WebSocket connection Keepalive pings/pongs Graceful or abrupt close Reconnection:\nNot automatic - client must implement State synchronization after reconnect Resume semantics (message IDs, sequence numbers) Heartbeats:\nPing/Pong frames for keepalive Detect half-open connections Configurable timeout Language \u0026 Platform Support Excellent support across languages:\nGo: gorilla/websocket, nhooyr/websocket Python: websockets, aiohttp JavaScript: Native WebSocket API (browser + Node.js) Java: Java WebSocket API (JSR 356), Spring WebSocket Rust: tokio-tungstenite C#: ASP.NET Core SignalR Browser Native:\nWebSockets work in all modern browsers Enables browser-based bot development Web-based battle visualization OpenTelemetry Integration Challenges:\nNo standard instrumentation like gRPC Manual span creation for each message Context propagation requires custom headers/metadata Implementation Approach:\n// Manual tracing ctx, span := tracer.Start(ctx, \"websocket.message\") defer span.End() span.SetAttributes( attribute.String(\"message.type\", msg.Type), attribute.String(\"bot.id\", botID), ) Metrics:\nMessage count, size, latency Connection duration, errors Custom battle-specific metrics Trace Correlation:\nEmbed trace context in JSON messages Reconstruct distributed trace manually More complex than gRPC auto-instrumentation Container Networking Proxy Considerations:\nSticky sessions required: Connection affinity Nginx: ip_hash or sticky directive HAProxy: stick-table for session persistence Envoy: Consistent hashing HTTP/1.1 Limitation:\nWebSocket upgrade uses HTTP/1.1 One WebSocket per TCP connection No multiplexing like HTTP/2 Kubernetes:\nService type LoadBalancer with session affinity Ingress controllers support WebSocket Readiness probes need special handling Development Experience Tooling:\nwscat: Command-line WebSocket client Browser DevTools: Inspect WebSocket frames Postman: WebSocket request support websocat: Netcat-like WebSocket tool Debugging:\nMessages visible in browser DevTools Text frames easy to inspect Binary frames require decoder Testing:\nUnit test with mock WebSocket Integration test with test server Load testing with ws-bench Advantages ✅ True bidirectional communication (no polling)\n✅ Low latency for real-time updates\n✅ Efficient - single persistent connection\n✅ Universal support across languages and browsers\n✅ Familiar to web developers\n✅ Flexible - text or binary messages\n✅ Firewall-friendly (works on port 80/443)\nDisadvantages ❌ No automatic reconnection (client must implement)\n❌ Scaling challenges (sticky sessions, connection limits)\n❌ OpenTelemetry integration requires manual instrumentation\n❌ No built-in backpressure mechanism\n❌ HTTP/1.1 only (no HTTP/2 multiplexing)\n❌ Connection state complicates load balancing\nSuitability for Battle Bots Client/Server Architecture: ⭐⭐⭐⭐☆ (Very Good)\nExcellent real-time performance Manual OTEL integration is manageable Sticky sessions solvable with proper load balancing P2P Architecture: ⭐⭐⭐☆☆ (Moderate)\nBots must run WebSocket servers NAT traversal challenges (similar to gRPC) Discovery and connection complexity Server-Sent Events (SSE) Analysis Architecture Pattern Unidirectional Server → Client:\nBot → GET /battles/{id}/events → Server ← HTTP/1.1 200 OK ← Content-Type: text/event-stream ← data: {\"event\": \"state_update\", \"positions\": [...]} ← data: {\"event\": \"damage\", \"target\": \"bot2\", \"hp\": 50} Bot → POST /battles/{id}/actions → Server (separate connection) Hybrid Approach:\nSSE for server → bot (game state, events) HTTP POST for bot → server (actions) Protocol Details Event Stream Format:\nevent: gameStateUpdate data: {\"positions\": [...], \"tick\": 42} id: 42 event: botDamaged data: {\"botId\": \"bot2\", \"damage\": 10} id: 43 Text-based format Named events for filtering Auto-incrementing IDs for resume Automatic Reconnection:\nBrowser automatically reconnects on disconnect Last-Event-ID header for resuming from last event Built-in retry with exponential backoff Performance Characteristics Latency:\n10-30ms for server → bot messages Similar to WebSocket for downstream POST latency for bot → server actions (20-50ms) Throughput:\nExcellent for server → bot (streaming) Limited by HTTP POST for bot → server Asymmetric performance Resource Usage:\nOne long-lived connection per bot (SSE) Short-lived connections for actions (POST) Lower than polling, higher than WebSocket Advantages ✅ Automatic reconnection with event ID resume\n✅ Simple - just HTTP GET, no upgrade handshake\n✅ Browser native - EventSource API\n✅ Text-based - easy debugging\n✅ Firewall-friendly (HTTP)\n✅ Built-in event types and filtering\nDisadvantages ❌ Unidirectional only (server → client)\n❌ Requires separate channel for client → server (HTTP POST)\n❌ Text-only (no binary without base64 encoding)\n❌ HTTP/1.1 connection limits (6 per domain in browsers)\n❌ Less efficient than WebSocket for bidirectional\n❌ Not widely adopted outside browser contexts\nSuitability for Battle Bots ⭐⭐⭐☆☆ (Moderate) - Viable for slow-paced battles, but WebSocket is simpler for bidirectional needs.\nSerialization Format Comparison JSON (JavaScript Object Notation) Characteristics:\nText-based, human-readable Language-agnostic Schema-optional (self-describing) Pros:\nEasy debugging (read with eyes) Universal support Browser-native parsing No build step Cons:\nLarger payload size (2-3x vs binary) Slower parsing than binary No built-in versioning No schema validation Example:\n{ \"action\": \"move\", \"botId\": \"bot-123\", \"position\": {\"x\": 10, \"y\": 20}, \"timestamp\": 1701820800 } Size: ~120 bytes\nMessagePack Characteristics:\nBinary JSON-like format Schema-optional Faster and smaller than JSON Pros:\n30-50% smaller than JSON 2-5x faster encoding/decoding Language support: Go, Python, JS, Java, Rust Drop-in JSON replacement Cons:\nNot human-readable Less ubiquitous than JSON No schema enforcement Example (binary):\n\\x84\\xa6action\\xa4move\\xa5botId\\xa7bot-123\\xa8position\\x82... Size: ~70 bytes\nProtocol Buffers over HTTP Characteristics:\nBinary protocol with schema (.proto) Same as gRPC but over HTTP/WebSocket Strong typing and versioning Pros:\nSmallest payload (50-70% smaller than JSON) Type safety and validation Forward/backward compatibility Code generation Cons:\nRequires .proto files and code generation Not human-readable Build complexity Size: ~50 bytes\nCBOR (Concise Binary Object Representation) Characteristics:\nBinary JSON alternative (RFC 8949) Self-describing like JSON Designed for IoT/constrained environments Pros:\nSmaller than JSON More data types (binary, dates) IETF standard Cons:\nLess adoption than MessagePack Not as compact as Protocol Buffers Recommendation for Battle Bots Development/Debugging: JSON (easy to inspect)\nProduction: Protocol Buffers (best performance + type safety)\nAlternative: MessagePack (good middle ground)\nOpenTelemetry Integration Summary Protocol OTEL Support Implementation Effort Trace Propagation Auto-Instrumentation HTTP/REST ⭐⭐⭐⭐⭐ Easy W3C Trace Context headers Yes (most languages) WebSockets ⭐⭐⭐☆☆ Moderate Manual (custom metadata) No SSE ⭐⭐⭐☆☆ Moderate W3C headers on GET Partial Best OTEL Integration: HTTP/REST (but worst real-time performance)\nCompromise: WebSocket with manual instrumentation\nContainer Networking Summary Protocol Container Support Load Balancing Session Affinity NAT Traversal HTTP/REST Excellent Easy Not required N/A WebSockets Good Moderate Required Challenging SSE Good Moderate Required Moderate Development Experience Summary Protocol Learning Curve Tooling Debugging Language Support HTTP/REST Low Excellent Easy Universal WebSockets Low-Moderate Good Moderate Excellent SSE Low Moderate Easy Good Pros and Cons Summary Overall Assessment HTTP/REST:\n✅ Simple, universal, well-tooled ❌ Poor real-time performance Use case: Admin APIs, non-real-time operations WebSockets:\n✅ Excellent real-time performance, bidirectional ❌ Manual OTEL, scaling complexity Use case: Real-time battle communication (strong candidate) SSE:\n✅ Auto-reconnect, simple server-push ❌ Unidirectional, requires POST for bot actions Use case: Asymmetric scenarios (server-heavy updates) References WebSocket Protocol (RFC 6455) Server-Sent Events Specification HTTP/2 Specification (RFC 7540) MessagePack Specification CBOR (RFC 8949) OpenTelemetry HTTP Instrumentation WebSocket vs Server-Sent Events gorilla/websocket - Go WebSocket library ","categories":"","description":"Analysis of HTTP/REST, WebSockets, and SSE for bot-to-server communication\n","excerpt":"Analysis of HTTP/REST, WebSockets, and SSE for bot-to-server …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/protocols/http/","tags":"","title":"HTTP-based Protocols Analysis"},{"body":"Context and Problem Statement Following the decision to adopt OpenTelemetry as our observability SDK (ADR-0002), we need to select the backend components for our observability stack. The stack must handle three distinct observability signals: traces, metrics, and logs. We need to decide on the ingestion mechanism for telemetry data, the storage backends for each signal type, and the visualization layer for unified observability.\nWhich combination of ingestion, backend, and visualization components should we deploy for the Battle Bots observability stack?\nDecision Drivers Vendor neutrality and avoiding lock-in Open-source preference to align with project philosophy Resource efficiency (memory, CPU, storage) Ease of deployment and operational overhead Query performance and scalability Long-term storage capabilities and retention policies Unified visualization across all observability signals Community support and maturity Cost of infrastructure and licensing Feature completeness for each signal type Flexibility to swap components independently Considered Options Ingestion Option I1: OpenTelemetry Collector Option I2: Direct export to signal-specific systems Traces Option T1: Tempo Option T2: Jaeger Option T3: Zipkin Metrics Option M1: Prometheus Option M2: Mimir Logs Option L1: OpenSearch Option L2: Loki Visualization Option V1: Grafana Option V2: Datadog Option V3: Dynatrace Option V4: New Relic Decision Outcome Chosen option: Open-Source Observability Stack with Grafana Ecosystem\nIngestion: OpenTelemetry Collector (Option I1) Visualization: Grafana (Option V1) Logs: Loki (Option L2) Traces: Tempo (Option T1) Metrics: Mimir (Option M2) This combination provides a fully open-source, vendor-neutral observability stack that leverages the Grafana ecosystem for seamless integration across all three signals. The stack prioritizes cost-effectiveness (object storage for all backends), operational simplicity (unified Grafana Labs components), and maintains flexibility to swap individual components as needs evolve.\nArchitecture Diagram architecture-beta service gameserver(server)[Game Server] service botruntime(server)[Bot Runtime] service battleviz(server)[Battle Visualization] service otelcollector(server)[OpenTelemetry Collector] service tempo(database)[Tempo] service mimir(database)[Mimir] service loki(database)[Loki] service grafana(internet)[Grafana] junction ingest junction export junction juncTempo1 junction juncTempo2 junction juncMimir junction juncLoki1 junction juncLoki2 gameserver:B -- T:ingest botruntime:R -- L:ingest battleviz:T -- B:ingest ingest:R --\u003e L:otelcollector otelcollector:R -- L:export export:T --\u003e B:tempo export:R --\u003e L:mimir export:B --\u003e T:loki tempo:R \u003c-- L:juncTempo1 mimir:R \u003c-- L:juncMimir loki:R \u003c-- L:juncLoki1 juncTempo1:R -- L:juncTempo2 juncLoki1:R -- L:juncLoki2 grafana:T -- B:juncTempo2 grafana:L -- R:juncMimir grafana:B -- T:juncLoki2 Consequences Good, because all components are open-source, avoiding vendor lock-in and licensing costs Good, because Grafana ecosystem provides unified, seamless integration across all signals Good, because Tempo, Loki, and Mimir all use object storage, minimizing infrastructure costs Good, because OpenTelemetry Collector decouples applications from backend systems Good, because we maintain flexibility to swap individual components independently Good, because resource efficiency is optimized (Loki and Tempo avoid expensive indexing) Good, because the stack aligns with the project’s open-source philosophy Good, because PromQL, LogQL, and TraceQL provide consistent query language patterns Neutral, because we need to self-host and manage all components Neutral, because Grafana requires learning three query languages (PromQL, LogQL, TraceQL) Bad, because operational overhead is higher than managed commercial solutions Bad, because Mimir is more complex to deploy than single-instance Prometheus Bad, because Loki requires careful label design to avoid high cardinality issues Bad, because we lack some enterprise features (advanced RBAC, managed services) Confirmation This decision will be considered successful when:\nAll three observability signals (traces, metrics, logs) are collected and queryable Visualization layer provides unified view across all signals Performance overhead of the stack is acceptable (\u003c 10% infrastructure cost) Operators can effectively debug issues using the observability data Retention policies maintain historical data for at least 30 days The stack can scale to handle expected production load Costs (infrastructure + licensing) fit within budget constraints Pros and Cons of the Options Ingestion Option I1: OpenTelemetry Collector OpenTelemetry Collector is a vendor-agnostic service for receiving, processing, and exporting telemetry data.\nGood, because it provides a unified ingestion point for all observability signals Good, because it decouples applications from backend systems Good, because it supports protocol translation (OTLP, Jaeger, Zipkin, Prometheus, etc.) Good, because it enables sending data to multiple backends simultaneously Good, because it provides data processing (filtering, sampling, enrichment) Good, because it allows backend changes without application redeployment Good, because it can aggregate telemetry from multiple sources Good, because it reduces load on applications by offloading export logic Good, because it supports tail-based sampling for traces Neutral, because it adds another component to deploy and manage Bad, because it introduces an additional hop in the telemetry pipeline Bad, because it becomes a critical point of failure if not properly configured Bad, because it requires additional infrastructure resources Option I2: Direct Export to Signal-Specific Systems Direct export means OpenTelemetry SDK sends telemetry directly to backend systems without an intermediary collector.\nGood, because it reduces architectural complexity (fewer moving parts) Good, because it eliminates an additional network hop Good, because it reduces infrastructure requirements (no collector to deploy) Good, because it has lower operational overhead Good, because latency is reduced for telemetry delivery Neutral, because applications must be configured with backend endpoints Bad, because changing backends requires application reconfiguration and redeployment Bad, because applications must handle export logic and retries Bad, because it couples applications to specific backend protocols Bad, because sending to multiple backends requires multiple exporters in each application Bad, because advanced processing (sampling, filtering) must be done in applications Bad, because it increases load on applications Traces Option T1: Tempo Grafana Tempo is a high-volume, cost-effective distributed tracing backend that requires only object storage.\nGood, because it uses object storage (S3, GCS, local disk), making it extremely cost-effective Good, because it has minimal operational overhead (no complex indexing) Good, because it integrates seamlessly with Grafana Good, because it scales horizontally and handles high trace volumes Good, because it supports multiple trace formats (Jaeger, Zipkin, OpenTelemetry) Good, because it has native OpenTelemetry Collector support Good, because it uses TraceQL for powerful trace queries Good, because resource usage is low compared to full-text indexing solutions Neutral, because trace discovery relies on trace IDs or service metadata Bad, because it lacks full-text search across all trace data Bad, because it is relatively newer compared to Jaeger/Zipkin Option T2: Jaeger Jaeger is a CNCF-graduated distributed tracing system originally developed by Uber.\nGood, because it is mature and battle-tested in production environments Good, because it has comprehensive trace search and filtering capabilities Good, because it provides its own UI in addition to Grafana integration Good, because it is CNCF-graduated with strong community support Good, because it supports multiple storage backends (Cassandra, Elasticsearch, Badger) Good, because it has native OpenTelemetry Collector support Neutral, because it requires additional storage infrastructure (database) Bad, because operational overhead is higher than Tempo Bad, because storage costs can be significant with Elasticsearch/Cassandra Bad, because resource consumption is higher due to indexing Option T3: Zipkin Zipkin is a distributed tracing system originally created by Twitter.\nGood, because it is very mature and widely adopted Good, because it has a simple architecture and deployment model Good, because it provides its own UI for trace visualization Good, because it supports multiple storage backends (MySQL, Cassandra, Elasticsearch) Good, because it has low resource requirements for small deployments Neutral, because OpenTelemetry Collector support is available but less emphasized Bad, because it has less active development compared to Tempo/Jaeger Bad, because Grafana integration is not as seamless Bad, because it lacks some modern features found in Tempo/Jaeger Bad, because community momentum has shifted toward newer solutions Metrics Option M1: Prometheus Prometheus is a CNCF-graduated monitoring and alerting toolkit designed for reliability and simplicity.\nGood, because it is the industry standard for metrics collection Good, because it has excellent Grafana integration Good, because it is CNCF-graduated with massive community support Good, because it has a powerful query language (PromQL) Good, because it uses efficient time-series storage Good, because it has extensive ecosystem of exporters and integrations Good, because it has native OpenTelemetry Collector support Good, because it is simple to deploy and operate for small-to-medium scale Good, because it has built-in alerting capabilities Neutral, because it is designed for single-server deployments Bad, because it has limited long-term storage capabilities (local disk only) Bad, because high availability requires complex federation setups Bad, because it doesn’t scale horizontally for writes Option M2: Mimir Grafana Mimir is a horizontally scalable, long-term storage for Prometheus metrics.\nGood, because it provides unlimited scalability for metrics storage Good, because it is fully compatible with Prometheus (PromQL, remote write) Good, because it uses object storage for cost-effective long-term retention Good, because it has seamless Grafana integration Good, because it supports multi-tenancy out of the box Good, because it provides high availability natively Good, because it has native OpenTelemetry Collector support Good, because it can act as a Prometheus replacement with better scalability Neutral, because it is more complex to deploy than single-instance Prometheus Bad, because operational overhead is higher than Prometheus Bad, because it requires more infrastructure (object storage, multiple components) Bad, because it may be over-engineered for small deployments Logs Option L1: OpenSearch OpenSearch is an open-source fork of Elasticsearch, providing search and analytics capabilities.\nGood, because it offers powerful full-text search across all log data Good, because it has rich query capabilities (SQL, DSL) Good, because it has strong data visualization tools (OpenSearch Dashboards) Good, because it integrates with Grafana Good, because it is mature and well-documented Good, because it supports structured and unstructured log data Good, because it has native OpenTelemetry Collector support Neutral, because it requires significant infrastructure (cluster setup) Bad, because resource consumption is high (CPU, memory, storage) Bad, because operational complexity is significant (cluster management, shard optimization) Bad, because storage costs can be expensive at scale Bad, because it may be over-engineered if simple grep-style queries suffice Option L2: Loki Grafana Loki is a log aggregation system designed to be cost-effective and easy to operate.\nGood, because it is extremely resource-efficient compared to full-text search solutions Good, because it uses object storage, making it cost-effective Good, because it has seamless Grafana integration (native support) Good, because operational overhead is minimal Good, because it scales horizontally Good, because it indexes only metadata (labels), not full log content Good, because it has native OpenTelemetry Collector support via OTLP Good, because it uses LogQL, which is similar to PromQL for consistency Neutral, because it requires labels to be well-structured for efficient queries Bad, because it doesn’t support full-text search across log content Bad, because complex queries across unlabeled data are slow Bad, because it requires careful label design to avoid high cardinality Visualization Option V1: Grafana Grafana is an open-source observability platform for visualizing metrics, logs, and traces.\nGood, because it is open-source and free, avoiding vendor lock-in Good, because it provides unified visualization for all three signals Good, because it has native integrations with Prometheus, Tempo, Loki, Jaeger, and many others Good, because it supports custom dashboards with powerful query editors Good, because it has a large community and extensive plugin ecosystem Good, because it supports alerting capabilities Good, because it can correlate data across different data sources Good, because it aligns with open-source philosophy of the project Good, because it has no per-user or per-host licensing costs Neutral, because it requires self-hosting and operational management Bad, because creating effective dashboards requires learning different query languages (PromQL, LogQL, TraceQL) Bad, because advanced features may require additional plugins or configuration Bad, because it lacks some enterprise features of commercial platforms (advanced RBAC, audit logs) Option V2: Datadog Datadog is a commercial observability platform providing unified monitoring, logging, and tracing.\nGood, because it provides a fully-managed, unified observability platform Good, because it has excellent out-of-the-box dashboards and visualizations Good, because it offers advanced analytics and correlation across signals Good, because it provides APM, RUM (Real User Monitoring), and many integrated features Good, because it has strong alerting, incident management, and collaboration tools Good, because operational overhead is minimal (fully managed) Good, because it has comprehensive documentation and support Neutral, because it can ingest OpenTelemetry data via OTLP Bad, because it creates significant vendor lock-in Bad, because it has substantial per-host/container costs that scale with usage Bad, because it contradicts open-source philosophy Bad, because costs can become prohibitive at scale Bad, because data is stored in Datadog’s infrastructure only Option V3: Dynatrace Dynatrace is an enterprise observability and AIOps platform.\nGood, because it provides AI-powered automatic root cause analysis Good, because it has comprehensive full-stack monitoring capabilities Good, because it offers automatic discovery and dependency mapping Good, because it excels at application performance monitoring Good, because operational overhead is minimal (fully managed) Good, because it has strong enterprise features and support Neutral, because it supports OpenTelemetry ingestion Bad, because it creates significant vendor lock-in Bad, because it has premium pricing model (enterprise-focused) Bad, because it contradicts open-source philosophy Bad, because it may be over-engineered for Battle Bots scale Bad, because costs are typically higher than other commercial solutions Option V4: New Relic New Relic is a commercial observability platform with full-stack monitoring capabilities.\nGood, because it provides unified observability for metrics, logs, and traces Good, because it has a generous free tier for small projects Good, because it offers good out-of-the-box visualizations and dashboards Good, because operational overhead is minimal (fully managed) Good, because it has native OpenTelemetry support Good, because it provides powerful query language (NRQL) Good, because it has decent community and documentation Neutral, because pricing is based on data ingestion rather than hosts Bad, because it creates vendor lock-in Bad, because costs scale with data volume Bad, because it contradicts open-source philosophy Bad, because advanced features require paid tiers Bad, because data is stored in New Relic’s infrastructure only More Information Related Decisions ADR-0002: Observability SDK Selection - establishes OpenTelemetry as the instrumentation layer Related Research Analysis documents available in docs/content/research_and_development/analysis/observability/ Architecture Overview The selected observability stack architecture:\n[Battle Bots Services] → [OpenTelemetry SDKs] → [OpenTelemetry Collector] → [Tempo (Traces)] → [Mimir (Metrics)] → [Loki (Logs)] → [Grafana (Visualization)] All three backends (Tempo, Mimir, Loki) use object storage (S3, GCS, or local disk) for cost-effective, scalable storage. Grafana provides the unified visualization layer with native support for all three data sources.\nImplementation Considerations OpenTelemetry Collector Configuration: Deploy as a sidecar or gateway to receive OTLP data and export to Tempo, Mimir, and Loki Object Storage: Configure S3-compatible object storage (or local disk for development) for all three backends Label Design: Carefully design log labels for Loki to balance query performance and cardinality Mimir vs Prometheus: Start with Mimir for long-term storage capabilities, though Prometheus could be used initially if simpler deployment is preferred Data Retention: Configure retention policies per signal type (e.g., traces: 7 days, metrics: 30 days with downsampling, logs: 14 days) Deployment Order: Deploy in order: object storage → Tempo + Mimir + Loki → OTel Collector → Grafana → configure data sources Grafana Data Sources: Configure three data sources in Grafana (Tempo, Mimir/Prometheus, Loki) for unified querying Correlation: Leverage trace IDs in logs and metrics to enable correlation across all three signals in Grafana Resource Planning: Monitor resource usage in POC environment to right-size deployments for production Migration Path: The OpenTelemetry Collector allows future migration to alternative backends without changing application instrumentation Rationale for Selection This specific combination was selected based on the following factors:\nUnified Ecosystem: Tempo, Loki, and Mimir are all Grafana Labs projects, ensuring consistent design patterns and seamless integration Cost Optimization: All three backends use object storage, significantly reducing storage costs compared to indexed solutions Operational Simplicity: Managing components from the same ecosystem reduces operational complexity versus mixing vendors Vendor Neutrality: All components are open-source, avoiding commercial lock-in while maintaining professional quality Scalability: Each component scales horizontally and is designed for high-volume production use Query Languages: PromQL (Mimir), LogQL (Loki), and TraceQL (Tempo) share similar syntax patterns, reducing learning curve OpenTelemetry Native: All components have native OpenTelemetry support via OTLP protocol Resource Efficiency: Loki and Tempo avoid expensive full-text indexing, keeping resource usage low Project Alignment: Open-source approach aligns with Battle Bots project philosophy Migration Flexibility: OpenTelemetry Collector allows swapping backends without re-instrumenting applications Next Steps POC Deployment: Deploy the full stack (OTel Collector + Tempo + Mimir + Loki + Grafana) in development environment Instrumentation: Instrument Battle Bots services with OpenTelemetry SDK (per ADR-0002) and verify all three signals are collected Object Storage Setup: Configure S3-compatible storage or local disk for backend storage Grafana Dashboards: Create initial dashboards for key metrics, traces, and logs Label Strategy: Define and document label naming conventions for Loki and Mimir Retention Policies: Configure appropriate retention for each signal type based on storage constraints Performance Testing: Generate load and measure resource usage, query performance, and storage costs Documentation: Document deployment procedures, configuration, and operational runbooks Production Readiness: Evaluate high availability, backup, and disaster recovery requirements before production deployment Open Questions What object storage provider should be used (S3, GCS, MinIO, local disk)? Should Mimir be deployed immediately, or start with Prometheus and migrate later? What are the specific retention requirements for each signal type? Do we need multi-tenancy support for isolating different battle arenas? What high availability requirements exist for the observability stack? ","categories":"","description":"Deploy an OpenTelemetry Collector with Tempo for traces, Prometheus for metrics, and Loki for logs, unified by Grafana for visualization.\n","excerpt":"Deploy an OpenTelemetry Collector with Tempo for traces, Prometheus …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0003-observability-stack-architecture/","tags":"","title":"[0003] Observability Stack Architecture"},{"body":"Overview Custom TCP and UDP protocols offer maximum control and potential performance optimization for game networking. This analysis evaluates whether the complexity and development cost of custom protocols is justified for Battle Bots, drawing on lessons from game industry networking.\nTCP Custom Protocol Analysis Protocol Design Message Format Example:\n[4 bytes: magic number 0x42544C00] [2 bytes: protocol version] [2 bytes: message type] [4 bytes: message length] [4 bytes: sequence number] [N bytes: payload (JSON, MessagePack, Protobuf)] [4 bytes: CRC32 checksum] Connection-Oriented Characteristics:\nReliable, ordered delivery (TCP guarantees) Stream-based (requires message framing) Connection handshake and teardown Flow control and congestion avoidance built-in Message Framing Strategies Length-Prefixed:\n[4-byte length][payload] Simple to parse Requires buffering until full message received Delimiter-Based:\n[payload]\\n Easy to implement Inefficient if payload contains delimiters Fixed-Length Headers:\nHeader describes payload structure Best practice for custom protocols Allows for protocol evolution Performance Characteristics Latency:\n5-15ms round-trip (local network) Similar to gRPC/WebSocket (all use TCP) No inherent advantage over HTTP/2 or WebSocket Throughput:\nLimited by TCP, not protocol overhead Custom framing slightly more efficient than HTTP Marginal gain (5-10%) vs WebSocket CPU Overhead:\nCustom parsing faster than HTTP header parsing Binary protocols minimize serialization cost Negligible difference in practice for Battle Bots scale Advantages ✅ Maximum control over message format\n✅ Minimal overhead (no HTTP headers)\n✅ Custom flow control if needed\n✅ Protocol optimizations for specific use case\nDisadvantages ❌ High implementation cost (build, test, debug)\n❌ Language-specific implementations (no code generation)\n❌ Limited tooling (manual packet inspection)\n❌ No standard libraries (roll your own)\n❌ Bug-prone (protocol state machines, edge cases)\n❌ No advantage over WebSocket/gRPC for this use case\nComparison with WebSocket Both use TCP, so performance is nearly identical:\nWebSocket has ~6 bytes per frame overhead Custom protocol saves ~6 bytes per message Conclusion: Negligible benefit, massive implementation cost UDP Custom Protocol Analysis Protocol Design Unreliable Datagram Characteristics:\nNo connection (connectionless) Packets can be lost, duplicated, reordered Low latency (no TCP handshake/ACK overhead) Manual reliability layer required Custom Reliability Layer Example:\n[4 bytes: sequence number] [4 bytes: ack bitfield] [1 byte: reliability flags] [N bytes: payload] Reliability Modes:\nUnreliable: Send and forget (position updates) Reliable: Retransmit until ACK (critical events) Ordered: Sequence numbers, discard out-of-order Sequenced: Latest only, discard older packets Performance Characteristics Latency:\n2-10ms round-trip (no TCP handshake) 30-50% lower latency than TCP in ideal conditions Worse under packet loss (retransmissions) Throughput:\nHigher ceiling than TCP (no congestion control) Prone to network congestion without custom control Better for bursty traffic Packet Loss Handling:\n1-5% packet loss is common on Internet Game engines interpolate/extrapolate positions Critical events need reliability layer Use Cases in Game Networking Good for UDP:\nPlayer position updates (frequent, latest is best) Input state (redundant, lose older packets) Non-critical effects (particle systems, audio) High tick-rate simulations (60+ updates/sec) Bad for UDP:\nPlayer actions (attack, use item - must be reliable) Inventory changes (critical state) Chat messages (must arrive) Battle Bots actions (move, attack decisions are critical) Game Networking Patterns Client-Side Prediction:\nClient predicts local movement → Send input to server Server authoritative simulation → Send correction Client reconciles prediction with server state Masks network latency Requires complex reconciliation logic Server Reconciliation:\nServer is authoritative Clients show approximate state Server corrects divergence Snapshot Interpolation:\nServer sends state snapshots Client interpolates between snapshots Smooth animation despite packet loss Delta Compression:\nSend only changed fields Reduces bandwidth for large state objects Requires baseline tracking Are these needed for Battle Bots?\nTurn-based or low tick-rate: No Real-time FPS-style: Maybe Current POC requirements: No OpenTelemetry Integration Challenges Major Issues:\nNo standard trace propagation (no HTTP headers) Custom metadata format required Manual span creation for every packet Sampling complexity (trace 1/1000 packets?) Implementation Approach:\n// Custom trace context in UDP packet type PacketHeader struct { TraceID [16]byte SpanID [8]byte Flags byte // ... rest of packet } Metrics Collection:\nPacket send/receive counters Loss rate calculation Latency histogram (manual timing) Battle-specific metrics Complexity vs gRPC/WebSocket:\n10x more complex to instrument No automatic integration with OTLP High maintenance burden Container Networking Port Mapping:\nUDP ports must be explicitly mapped Each bot needs unique port (or shared port with routing) Firewall Traversal:\nMany networks block UDP Corporate firewalls often UDP-hostile NAT64 environments problematic Load Balancing:\nStateless load balancing possible Consistent hashing needed for session affinity More complex than TCP NAT Traversal for P2P:\nHole Punching:\nBoth bots connect to rendezvous server Server shares IP:port of each bot Bots send UDP packets to each other NAT creates temporary mappings Direct P2P communication established STUN/TURN:\nSTUN: Discover public IP:port TURN: Relay server for firewall traversal TURN required when symmetric NAT prevents hole-punching Complexity:\nSignificantly more complex than TCP Failure rate: 5-20% of connections may require TURN Additional infrastructure cost Development \u0026 Tooling Implementation Cost:\nHigh: Build protocol from scratch in each language Testing: Unit tests, integration tests, fuzz testing Edge cases: Connection state, packet loss, reordering Debugging Tools:\ntcpdump: Capture packets Wireshark: Decode custom protocol (requires dissector plugin) Custom tooling: Packet replayer, simulator Logging: Extensive logging required Language Support:\nManual implementation per language No code generation (unlike gRPC/Protobuf) Consistency challenges across languages Maintenance:\nProtocol versioning complexity Backward compatibility testing Documentation burden for bot developers Industry Examples \u0026 Lessons Unreal Engine Networking Architecture:\nUDP-based with custom reliability layer RPCs for critical events (reliable) Property replication for state (unreliable with delta compression) Client-side prediction and server reconciliation Why UDP:\nFast-paced FPS games (60-120 tick rate) Hundreds of position updates per second Packet loss acceptable for intermediate positions Applicability to Battle Bots:\nUnclear if Battle Bots needs 60+ tick rate If turn-based, UDP advantage disappears Unity DOTS NetCode Architecture:\nUDP with custom reliable channel Delta compression for snapshot synchronization Ghost entities (networked objects) Client-side prediction Why UDP:\nReal-time multiplayer games Low latency critical High entity count Valve Source Engine Architecture:\nUDP with separate reliable/unreliable channels Lag compensation for hit detection Client-side prediction Command acknowledgment Lessons:\nUDP justified for \u003c 50ms latency requirements Complex to implement correctly Years of refinement What Battle Bots Can Learn Question: Does Battle Bots need \u003c 50ms latency?\nTurn-based or slow tick-rate (\u003c 10/sec):\nTCP is sufficient (WebSocket, gRPC) UDP complexity not justified Real-time fast-paced (\u003e 30/sec tick rate):\nUDP may provide benefits Requires client-side prediction High development cost POC Requirement:\nNo specific tick rate mentioned Suggests UDP is premature optimization Protocol Design Considerations Message Format Design Efficiency:\nBinary \u003e Text for bandwidth Protocol Buffers balance efficiency + maintainability Versioning:\nProtocol version field in header Feature negotiation during handshake Backward compatibility strategy Extensibility:\nReserve fields for future use TLV (Type-Length-Value) encoding Protocol Buffers automatically extensible Error Handling Connection Errors:\nTimeout handling Reconnection logic State recovery Protocol Errors:\nInvalid message format Unexpected message type Version mismatch Application Errors:\nInvalid bot actions Game rule violations Rate limiting Authentication \u0026 Security Connection Authentication:\nToken-based (JWT in handshake) Mutual TLS (mTLS) API keys Message Integrity:\nHMAC signatures CRC/checksum for corruption Encryption (TLS for TCP, DTLS for UDP) Denial of Service:\nRate limiting per bot Connection limits Packet flood protection Development Complexity Comparison Aspect Custom TCP Custom UDP gRPC WebSocket Implementation High Very High Low Low Language Support Manual Manual Generated Libraries Tooling Minimal Minimal Excellent Good OTEL Integration Hard Very Hard Native Moderate Maintenance High Very High Low Low Debugging Hard Very Hard Easy Moderate Effort Estimate:\nCustom TCP: 4-6 weeks per language Custom UDP: 8-12 weeks per language (+ reliability layer) gRPC: 1-2 days (define .proto, generate code) WebSocket: 1-3 days (use library) Pros and Cons Summary Custom TCP ✅ Maximum protocol control\n✅ Minimal wire overhead\n❌ High implementation cost (weeks per language)\n❌ No tooling ecosystem\n❌ Difficult OTEL integration\n❌ No performance advantage over WebSocket\nVerdict: ❌ Not justified for Battle Bots\nCustom UDP ✅ Lowest latency (2-10ms)\n✅ No TCP overhead\n✅ Good for high tick-rate (60+/sec)\n❌ Very high implementation cost (months)\n❌ Reliability layer complexity\n❌ Poor firewall/NAT traversal\n❌ Very difficult OTEL integration\n❌ Only beneficial if \u003c 50ms latency critical\nVerdict: ❌ Premature optimization for POC\nWhen Custom Protocols Make Sense Justified:\nExtreme performance requirements (\u003c 10ms latency) Specialized hardware or protocols Unique constraints not met by standards Not Justified (Battle Bots POC):\nUnknown tick rate requirements Observability is priority (ADR-0002) Language-agnostic bot interface Development velocity matters Suitability for Battle Bots Custom TCP: ⭐☆☆☆☆ (Not Recommended)\nNo performance benefit over WebSocket Massive implementation cost Poor observability integration Custom UDP: ⭐☆☆☆☆ (Not Recommended for POC)\nUnclear if latency requirements justify complexity OTEL integration extremely difficult Firewall/NAT traversal issues Defer until POC proves need Recommendation: Use standard protocols (gRPC/WebSocket) for POC. Re-evaluate custom UDP only if profiling shows \u003c 50ms latency is critical and unachievable with TCP-based protocols.\nReferences Game Networking Patterns (Gaffer On Games) Source Multiplayer Networking Unreal Engine Network Architecture Unity DOTS NetCode Fast-Paced Multiplayer (Gabriel Gambetta) NAT Traversal Techniques STUN Protocol (RFC 5389) TURN Protocol (RFC 5766) TCP vs UDP for Games (Glenn Fiedler) ","categories":"","description":"Analysis of custom TCP and UDP protocols for bot-to-server communication\n","excerpt":"Analysis of custom TCP and UDP protocols for bot-to-server …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/protocols/custom/","tags":"","title":"Custom TCP/UDP Protocol Analysis"},{"body":" Context and Problem Statement The Battle Bots platform requires a communication protocol for bots to interact with the battle server (client/server architecture) or with each other (peer-to-peer architecture). This protocol must support real-time communication while remaining language-agnostic to enable bot development in any programming language.\nRelated to User Journey [0001] Proof of Concept - 1v1 Battle, which identifies bot-to-server interface as a pending ADR required for POC implementation.\nKey Requirements:\nReal-time communication capabilities Language-agnostic interface (Python, Go, Java, JavaScript, Rust, etc.) Low-latency performance Support for both client/server and P2P architectural modes Integration with OpenTelemetry observability stack (ADR-0002, ADR-0003) Container-friendly networking (Docker/Podman) Which communication protocol should Battle Bots adopt for the bot-to-server and bot-to-bot interface?\nDecision Drivers Performance: Low latency and high throughput for real-time battle interaction Language Support: Must enable bot development in diverse programming languages Observability: Seamless integration with OpenTelemetry (ADR-0002) and OTLP stack (ADR-0003) Developer Experience: Ease of implementation for bot authors Communication Flexibility: Support for various communication patterns Type Safety: Schema validation and versioning for protocol evolution Container Networking: Compatibility with Docker/Podman environments Dual Architecture Support: Viability for both client/server and P2P modes Tooling Ecosystem: Availability of debugging, testing, and development tools Implementation Complexity: Development and maintenance cost Industry Maturity: Proven technology with active community support Considered Options Option 1: gRPC Option 2: WebSockets with JSON/MessagePack Option 3: HTTP/REST with Server-Sent Events (SSE) Option 4: Custom TCP protocol Option 5: Custom UDP protocol Decision Outcome Chosen option: “gRPC”, because it provides the optimal balance of performance, developer experience, and observability integration while meeting all functional requirements for both client/server and P2P architectures.\ngRPC delivers excellent performance with superior type safety (Protocol Buffers), native OpenTelemetry instrumentation, and excellent language support through code generation. The protocol provides flexible communication patterns suitable for real-time battle interactions.\nConsequences Positive:\n✅ Excellent OpenTelemetry integration - Native auto-instrumentation for traces, metrics, and logs with zero manual setup aligns perfectly with ADR-0002 and ADR-0003 ✅ Language-agnostic via Protocol Buffers - Code generation for all major languages (Go, Python, Java, JavaScript, Rust, C++, C#) enables diverse bot ecosystem ✅ Type safety and versioning - .proto schema enforces contracts and provides forward/backward compatibility ✅ Flexible communication patterns - Supports unary, server streaming, client streaming, and bidirectional streaming for various use cases ✅ Performance - Binary protocol with 7-10x better throughput than REST/JSON and 30-50% smaller payloads ✅ Container-friendly - HTTP/2 works natively in Docker/Podman with simple port mapping ✅ Rich tooling - grpcurl for testing, ghz for benchmarking, reflection for discovery ✅ Industry proven - Used at scale by Google, Netflix, Square for production systems ✅ Self-documenting - .proto files serve as both implementation and documentation Negative:\n❌ Learning curve - Bot developers must understand Protocol Buffers syntax (mitigated by examples and generated code) ❌ Build complexity - Requires protoc compiler and code generation step in build pipeline ❌ Binary debugging - Not human-readable like JSON (mitigated by grpcurl and reflection) ❌ Browser support - Requires grpc-web proxy for browser-based bots (not native WebSocket) ❌ P2P NAT traversal - More complex than UDP for peer-to-peer due to HTTP/2 over TCP Neutral:\n⚪ HTTP/2 requirement - Benefits from multiplexing but limited to HTTP/2 capabilities ⚪ Port considerations - Non-standard ports (e.g., 50051) may require firewall rules in some environments Confirmation Implementation compliance will be verified through:\nProtocol Definition: All bot-to-server communication defined in .proto files in repository OTEL Instrumentation: Automated tests verify traces/metrics are emitted to Tempo/Mimir Language SDK Examples: Reference bot implementations in at least 3 languages (Go, Python, JavaScript) Integration Tests: Client/server and P2P battle scenarios with containerized bots Performance Benchmarks: Latency and throughput meet \u003c 50ms round-trip target Pros and Cons of the Options gRPC Description: HTTP/2-based RPC framework with Protocol Buffers for serialization and support for multiple communication patterns.\nDetailed Analysis: See gRPC Protocol Analysis\nPros:\n✅ Native OpenTelemetry support - Automatic trace propagation, span creation, and metrics collection integrate seamlessly with ADR-0002 observability stack ✅ Protocol Buffers - Strong typing, schema validation, and versioning prevent runtime errors and enable protocol evolution ✅ Flexible communication patterns - Supports unary, server streaming, client streaming, and bidirectional streaming ✅ Code generation - Eliminates boilerplate, enforces API contracts across all languages ✅ Performance - Binary serialization achieves 5-20ms latency, thousands of messages/sec throughput ✅ Language coverage - Official support for Go, Python, Java, JavaScript, C++, C#, Rust, and more ✅ Container networking - HTTP/2 works natively in Docker/Podman without special configuration ✅ Tooling ecosystem - grpcurl, ghz, reflection, IDE plugins provide excellent developer experience ✅ Dual architecture support - Client/server: bots as clients. P2P: bots expose gRPC servers and connect to each other Cons:\n❌ Learning curve - Developers must learn .proto syntax (estimated 1-2 hours for basics) ❌ Build step - Code generation adds complexity to build pipelines ❌ Binary format - Debugging requires tools like grpcurl vs simple curl for JSON ❌ Browser limitation - Requires grpc-web proxy, not native browser support ❌ P2P NAT - TCP-based NAT traversal requires rendezvous server or relay for some networks Suitability:\nClient/Server: ⭐⭐⭐⭐⭐ (Excellent) - Ideal fit for all requirements P2P: ⭐⭐⭐⭐☆ (Very Good) - NAT traversal manageable with coordination server WebSockets with JSON/MessagePack Description: Full-duplex bidirectional communication over persistent TCP connection with JSON or binary serialization.\nDetailed Analysis: See HTTP-based Protocols Analysis\nPros:\n✅ True bidirectional - Full-duplex communication without polling ✅ Low latency - 5-20ms, comparable to gRPC ✅ Universal support - Native in browsers and all major languages ✅ Flexible serialization - JSON for debugging, MessagePack for efficiency ✅ Familiar to web developers - Lower barrier than learning gRPC/protobuf ✅ Simple protocol - No code generation or build complexity Cons:\n❌ Manual OpenTelemetry instrumentation - No automatic trace propagation or span creation ❌ No schema enforcement - JSON lacks type safety, versioning requires custom logic ❌ Sticky sessions required - Load balancing complexity for stateful connections ❌ No code generation - Manual client/server implementation in each language ❌ HTTP/1.1 only - No HTTP/2 multiplexing benefits ❌ Reconnection logic - Client must implement reconnect and state recovery Suitability:\nClient/Server: ⭐⭐⭐⭐☆ (Very Good) - Viable, but lacks gRPC’s OTEL integration P2P: ⭐⭐⭐☆☆ (Moderate) - Similar NAT challenges as gRPC HTTP/REST with Server-Sent Events Description: Hybrid approach using HTTP POST for bot actions and SSE for server-to-bot event streaming.\nDetailed Analysis: See HTTP-based Protocols Analysis\nPros:\n✅ Automatic reconnection - SSE handles reconnect with Last-Event-ID resume ✅ Simple model - Standard HTTP requests plus event stream ✅ Excellent OTEL support - HTTP instrumentation is mature ✅ Text-based - Easy debugging with curl and browser DevTools ✅ Browser native - EventSource API built into browsers Cons:\n❌ Unidirectional streaming - Requires separate HTTP POST for bot → server ❌ Asymmetric performance - Different latency for upstream vs downstream ❌ HTTP/1.1 connection limits - Browser constraint (6 per domain) ❌ Text-only - No native binary support (requires base64 encoding) ❌ Less common - Fewer production examples than WebSocket or gRPC ❌ Two separate channels - More complex than single bidirectional stream Suitability:\nClient/Server: ⭐⭐⭐☆☆ (Moderate) - Workable but awkward bidirectional pattern P2P: ⭐⭐☆☆☆ (Marginal) - SSE not designed for P2P scenarios Custom TCP Protocol Description: Application-specific binary protocol over raw TCP sockets with custom message framing.\nDetailed Analysis: See Custom TCP/UDP Protocol Analysis\nPros:\n✅ Maximum control - Full control over wire format and protocol behavior ✅ Minimal overhead - No HTTP headers or protocol baggage Cons:\n❌ Very high implementation cost - 4-6 weeks per language for production-quality code ❌ No performance advantage - TCP latency same as WebSocket/gRPC (both use TCP) ❌ Manual OTEL integration - No automatic instrumentation, requires custom trace context ❌ No tooling - Must build custom debugging and testing tools ❌ Maintenance burden - Protocol bugs, versioning, cross-platform issues ❌ Language fragmentation - Manual implementation in each language, consistency challenges Suitability:\nClient/Server: ⭐☆☆☆☆ (Not Recommended) - Cost far exceeds marginal benefit P2P: ⭐☆☆☆☆ (Not Recommended) - Same TCP limitations as gRPC/WebSocket Verdict: ❌ Rejected - No measurable performance benefit over gRPC/WebSocket while requiring 10-20x more development effort.\nCustom UDP Protocol Description: Connectionless packet-based protocol with custom reliability layer for critical messages.\nDetailed Analysis: See Custom TCP/UDP Protocol Analysis\nPros:\n✅ Lowest latency - 2-10ms potential vs 5-20ms for TCP ✅ No head-of-line blocking - Lost packets don’t block subsequent packets ✅ Suitable for high tick-rate - Games running at 60+ updates/sec benefit Cons:\n❌ Very high complexity - 8-12 weeks per language plus reliability layer ❌ Unclear requirement - Battle Bots tick rate and latency needs undefined ❌ Firewall hostile - Many networks block UDP, requires fallback ❌ NAT traversal complexity - Hole punching, STUN/TURN infrastructure needed for P2P ❌ Extreme OTEL difficulty - No standard trace propagation, manual packet-level instrumentation ❌ Premature optimization - POC should prove \u003c 50ms latency is insufficient before custom UDP Suitability:\nClient/Server: ⭐☆☆☆☆ (Not Recommended for POC) - Defer until profiling proves TCP inadequate P2P: ⭐⭐☆☆☆ (Marginal) - NAT traversal challenges significant Verdict: ❌ Rejected for POC - Re-evaluate only if client/server POC demonstrates that TCP-based protocols cannot meet latency requirements (evidence currently absent).\nMore Information Related ADRs ADR-0002: OpenTelemetry SDK Selection - Establishes OTEL as observability standard ADR-0003: Observability Stack Architecture - Defines Tempo, Mimir, Loki as backends Future ADR: Game Runtime Architecture - Will define tick rate and game loop mechanics Future ADR: Client/Server vs P2P Architecture - Will choose primary architecture mode Related Requirements From User Journey [0001] Proof of Concept - 1v1 Battle:\nLanguage-agnostic bot implementation Containerized bots (Docker/Podman) Observability signals capture Real-time battle visualization Open Questions Tick Rate: What is the target game tick rate? (Informs whether gRPC performance is sufficient) P2P Consensus: How will bots reach consensus in P2P mode? (Future ADR) Browser Bots: Should we support browser-based bots via grpc-web? Rate Limiting: What are the action rate limits per bot? Further Reading gRPC Protocol Analysis - Detailed gRPC evaluation HTTP-based Protocols Analysis - WebSocket and SSE analysis Custom Protocols Analysis - TCP/UDP evaluation gRPC Official Documentation Protocol Buffers Language Guide OpenTelemetry gRPC Instrumentation ","categories":"","description":"Selection of gRPC as the communication protocol for bot-to-server and bot-to-bot interfaces\n","excerpt":"Selection of gRPC as the communication protocol for bot-to-server and …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0004-bot-battle-server-interface/","tags":"","title":"[0004] Bot to Battle Server Communication Protocol"},{"body":" Context and Problem Statement Battle Bots requires a rigorous mathematical foundation for the spatial environment where battles occur. We need to define the topological and geometric properties of the “BattleBot Universe” that govern all spatial interactions, movement mechanics, collision detection, and distance calculations.\nThis decision defines four fundamental topological properties that characterize the battle space:\nDimensionality: The number of spatial dimensions (2D vs 3D) Vector Space: The mathematical space structure (continuous Euclidean space vs discrete lattice) Coordinate Chart: The coordinate system for position representation (Cartesian vs Polar) Boundary: The manifold topology (unbounded vs bounded space) These properties form the mathematical foundation upon which all spatial mechanics, bot characteristics, equipment systems, and action mechanics are built. The choice of topological structure has cascading implications for implementation complexity, computational efficiency, tactical depth, and accessibility.\nWithout well-defined topological properties, we cannot:\nImplement consistent position and movement mechanics Establish arena boundaries and out-of-bounds handling Provide a predictable spatial framework for bot developers Design algorithms that work with the spatial structure Support visualization and rendering systems Create a coherent foundation for all spatial game mechanics Decision Drivers Mathematical Rigor - Spatial system should have well-defined mathematical properties Implementation Complexity - Simpler topologies reduce development and computational cost Tactical Depth - Spatial structure should enable meaningful strategic positioning Accessibility - Bot developers should have accessible algorithms (pathfinding, movement) Computational Efficiency - Spatial calculations should be performant in real-time Predictability - Physics and movement should be deterministic and understandable Visualization Clarity - Positions must map cleanly to visual representation Engagement Guarantee - Topology should ensure bots cannot avoid combat indefinitely Protocol Integration - Must integrate with gRPC protocol for position updates (ADR-0004) Extensibility - Should support future enhancements (obstacles, terrain, variable arenas) Standard Tooling - Should leverage standard mathematical libraries and algorithms Decision Outcome Property 1: Dimensionality Chosen: Option 1.1 - 2D\nRationale:\nSimplicity First: 2D pathfinding (A*, Dijkstra) and collision detection (2D circle intersection) are well-documented and accessible Lower Barrier to Entry: Bot developers familiar with 2D algorithms from games, robotics simulations, and common CS education Computational Efficiency: 2D physics orders of magnitude less expensive than 3D (O(n²) vs O(n³) for many operations) Visualization Simplicity: Direct 2D rendering without camera controls, 3D projection, or depth perception complexity Sufficient Strategic Depth: 2D space provides adequate complexity for flanking, positioning, range control, and tactical movement Aligns with Other Decisions: Complements bounded rectangular arena and Cartesian coordinates Alternative Considered: Option 1.2 - 3D would add significant complexity to pathfinding (3D A*), collision detection (3D physics), and visualization (3D rendering, camera controls) without proportional gameplay benefit for 1v1 battles. Can be reconsidered for future game modes if aerial combat or vertical positioning becomes strategically important.\nProperty 2: Vector Space Chosen: Option 2.1 - R^n (Continuous Euclidean Space)\nRationale:\nSmooth Continuous Movement: Enables fluid, realistic movement that integrates naturally with real-time gameplay model Infinite Precision: Floating-point coordinates allow sub-unit positioning (no grid snapping artifacts) Standard Physics: Continuous space supports standard physics (velocity, acceleration, friction) using well-established formulas Tactical Positioning Depth: Precise positioning enables fine-grained strategy (optimal range, exact angles) Real-time Protocol Integration: gRPC can stream continuous position updates without discrete grid jumps Extensibility: Supports future terrain effects, obstacles, dynamic boundaries without discretization constraints Alternative Considered: Option 2.2 - n-dimensional Lattice would simplify collision detection (grid occupancy) and eliminate floating-point precision issues, but would sacrifice movement fluidity and create discrete grid-to-grid jump artifacts that feel unnatural in real-time gameplay. Grid-based pathfinding is simpler, but requires discrete approximation that limits tactical positioning depth.\nProperty 3: Coordinate Chart Chosen: Option 3.1 - Cartesian\nRationale:\nUniversal Familiarity: Cartesian (x, y) coordinates are universally taught and understood Algorithmic Simplicity: Distance, angle, and vector calculations use standard formulas Library Support: Every programming language has extensive Cartesian math libraries Rectangular Arena Alignment: Cartesian coordinates naturally align with rectangular boundaries Grid Visualization: Maps directly to pixel grids for rendering (x → screen x, y → screen y) Pathfinding Compatibility: A* and pathfinding algorithms designed for Cartesian grids Alternative Considered: Option 3.2 - Polar (r, θ) coordinates would be more natural for rotational mechanics but require trigonometric conversions for most operations, have less library support, and align poorly with rectangular boundaries. Polar coordinates are better suited for radial-specific mechanics (turrets, circular arenas) which are not part of the current design.\nProperty 4: Boundary Chosen: Option 4.2 - Rectangular Boundary\nRationale:\nEngagement Guarantee: Fixed boundaries prevent indefinite evasion, forcing interaction Predictable Strategy: Bots can rely on constant arena size for pathfinding and tactical planning Cartesian Alignment: Rectangular boundaries align perfectly with Cartesian coordinates Simple Collision Detection: Boundary checks are trivial comparisons (x \u003c min, x \u003e max) Fair and Balanced: Equal access to arena space for both bots throughout battle Visualization Clarity: Rectangular arena displays naturally on screen without distortion Complements Timeout: Fixed boundaries work with timeout mechanism to ensure conclusion Alternative Considered: Option 4.3 - Circular Boundary would provide uniform distance from center but requires distance calculations for boundary checks (more expensive than rectangular comparisons) and aligns poorly with Cartesian coordinates. Circular boundaries also have no corners, which eliminates certain positional strategies.\nAlternative Rejected: Option 4.1 - Unbounded would allow indefinite evasion, potentially creating stalemate scenarios even with timeout. Unbounded space also complicates pathfinding (no clear boundaries for navigation) and visualization (camera must follow bots across potentially large distances).\nSpatial System Implementation Specification The four topological properties define the following concrete spatial system implementation:\nCoordinate System The battle space uses a 2D Cartesian coordinate system with the following properties:\nDimensionality: Two-dimensional space (x, y coordinates only; no z-axis or vertical elevation) Origin (0, 0): Located at the center of the arena X-axis: Horizontal axis, with positive values extending to the right and negative values to the left Y-axis: Vertical axis, with positive values extending upward and negative values downward Units: Abstract spatial units (not meters, pixels, or other real-world measurements) Precision: Floating-point coordinates allow for sub-unit positioning accuracy This centered origin simplifies calculations for distance, angle, and relative positioning between bots. It also provides symmetry for balanced starting positions in various battle configurations.\nBoundaries The battle space is defined by rectangular boundaries:\nArena Size: 100 x 100 units (TBD - subject to tuning based on playtesting) X-axis Range: -50 to +50 units Y-axis Range: -50 to +50 units Out-of-Bounds Handling:\nMovement Blocking: Any movement command that would place a bot outside the boundaries is clamped to the nearest valid position at the boundary edge No Wrapping: Coordinates do not wrap around (i.e., exiting the right side does not place a bot on the left side) Boundary Contact: Bots may be positioned exactly on the boundary line Force Effects: External forces (knockback, explosions, etc.) that would push a bot out-of-bounds will stop at the boundary Movement Constraints The following basic movement constraints apply to the battle space:\nContinuous Movement: Bots cannot instantly teleport from one position to another; all movement follows continuous paths through the 2D Euclidean space Boundary Enforcement: Any movement that would place a bot outside the rectangular boundaries is prevented (specific collision mechanics will be defined in a separate ADR) No Coordinate Wrapping: The space does not wrap around (i.e., exiting one side does not place a bot on the opposite side) Deterministic Physics: All spatial calculations use deterministic algorithms to ensure consistent behavior across platforms Consequences Dimensionality Decision (2D) Good, because simplest spatial implementation (2D algorithms, 2D visualization) Good, because lower barrier to entry for bot developers Good, because sufficient strategic depth for positioning and tactics Good, because reduces computational requirements vs 3D Good, because integrates seamlessly with rectangular boundaries and Cartesian coordinates Neutral, because limits future aerial or vertical combat mechanics Bad, because eliminates vertical positioning as strategic dimension Bad, because may feel limiting if users expect 3D movement Vector Space Decision (R^n Continuous) Good, because enables smooth, realistic movement Good, because supports standard continuous physics formulas Good, because infinite precision for tactical positioning Good, because integrates naturally with real-time gRPC protocol Good, because extensible to terrain effects and obstacles Neutral, because requires careful floating-point handling Bad, because floating-point edge cases more complex than integer lattice Bad, because pathfinding requires discretization step Coordinate Chart Decision (Cartesian) Good, because universally familiar coordinate system Good, because extensive library and tooling support Good, because aligns naturally with rectangular boundaries Good, because direct mapping to pixel rendering Good, because standard distance and angle formulas Neutral, because polar coordinates may be more natural for some rotational mechanics Bad, because bot developers must calculate angles for directional actions Boundary Decision (Rectangular) Good, because guarantees engagement (no indefinite evasion) Good, because simple collision detection (4 comparisons) Good, because aligns perfectly with Cartesian coordinates Good, because predictable arena for pathfinding Good, because fair and balanced (symmetric access) Good, because complements timeout mechanism for battle conclusion Neutral, because could support variable sizes in future Bad, because corners enable defensive camping strategies Bad, because lacks additional pressure (no shrinking) Overall Integration Good, because all four properties create coherent mathematical foundation Good, because choices are mutually reinforcing (Cartesian + Rectangular, 2D + R^n) Good, because extensible framework supports future enhancements Good, because spatial system implementation follows naturally from topological properties Good, because creates foundation for ADR-0006 (characteristics), ADR-0007 (equipment), ADR-0008 (actions) Good, because property-based decision structure allows independent tuning and future modifications Good, because balances mathematical rigor with practical accessibility Confirmation The decision will be confirmed through:\nTopological Consistency: Verify mathematical properties are correctly implemented\n2D coordinate representation in all spatial data structures Continuous floating-point position values (no grid snapping) Cartesian coordinate system throughout codebase Rectangular boundary enforcement Spatial Mechanics Validation:\nBoundary enforcement working correctly (clamping at edges) Continuous movement paths validated Coordinate calculations accurate and deterministic Developer Accessibility:\nBot SDK exposes Cartesian (x, y) coordinates Movement API uses familiar vector representations Documentation includes standard formulas (distance, angle) Sample bots demonstrate pathfinding in continuous 2D space Performance Testing:\n2D collision detection meets real-time tick rate requirements Floating-point calculations deterministic across platforms Spatial queries (nearest bot, line of sight) performant Protocol Integration:\ngRPC messages correctly encode 2D positions Position updates stream smoothly in continuous space Boundary violations detected and communicated Visualization Testing:\n2D Cartesian coordinates map correctly to screen pixels Rectangular arena renders clearly Bot positions and movements display accurately Extensibility Validation:\nSystem can support future obstacles and terrain Variable rectangular arena sizes possible Spatial queries abstracted for future enhancements Playtesting:\nArena size (100x100) provides appropriate tactical space Spatial dimensions support engaging battles Future Consideration:\nDocument path to 3D extension if needed Evaluate polar coordinates for specific mechanics (turrets, sensors) Consider circular boundaries for alternative game modes Pros and Cons of the Options Property 1: Dimensionality Option 1.1: 2D (CHOSEN) Battles occur in two-dimensional space (x, y coordinates).\nGood, because simplest spatial implementation (2D pathfinding, collision detection, physics) Good, because lower computational requirements compared to 3D Good, because easier visualization (direct 2D display, no camera controls needed) Good, because lower barrier to entry for bot developers (2D algorithms more accessible) Good, because well-documented algorithms (A*, 2D vector math, 2D physics) Good, because sufficient strategic depth for positioning and movement tactics Neutral, because appropriate for ground-based combat scenarios Neutral, because may be extended to 3D in future if needed Bad, because eliminates vertical positioning as strategic dimension Bad, because no aerial combat or flying units Bad, because may feel limiting if users expect 3D movement Option 1.2: 3D Battles occur in three-dimensional space (x, y, z coordinates).\nGood, because enables vertical positioning strategy (height advantage) Good, because supports aerial combat and flying units Good, because additional strategic dimension (above/below positioning) Good, because familiar from many modern games Neutral, because enables jump mechanics or flight equipment Neutral, because may be more engaging for some users Bad, because significantly more complex implementation (3D pathfinding, collision, physics) Bad, because much higher computational requirements (3D calculations expensive) Bad, because complex visualization (3D rendering, camera controls, depth perception) Bad, because higher barrier to entry (3D algorithms more complex) Bad, because more difficult to debug and visualize battles Bad, because may be unnecessary complexity for 1v1 ground combat Property 2: Vector Space Option 2.1: R^n - Continuous Euclidean Space (CHOSEN) Positions represented as continuous floating-point coordinates in standard n-dimensional Euclidean space.\nGood, because enables smooth, continuous movement Good, because supports standard continuous physics formulas (velocity, acceleration, friction) Good, because infinite precision for tactical positioning (no grid snapping) Good, because integrates naturally with real-time gRPC protocol Good, because extensible to terrain effects, obstacles, dynamic boundaries Good, because familiar from most modern games and simulations Neutral, because requires careful floating-point handling for determinism Neutral, because pathfinding requires discretization for algorithms like A* Bad, because floating-point precision can introduce edge cases Bad, because collision detection more complex than grid occupancy Bad, because requires careful handling of floating-point comparisons Option 2.2: n-dimensional Lattice of R^n Positions represented as discrete points on an integer lattice (grid).\nGood, because eliminates floating-point precision issues Good, because simpler collision detection (grid cell occupancy) Good, because natural fit for grid-based pathfinding (A*, BFS) Good, because deterministic integer mathematics Good, because easier to reason about and debug Neutral, because grid resolution determines precision (finer grids approach continuous) Bad, because discrete movement creates grid-to-grid jump artifacts Bad, because feels unnatural and less fluid in real-time gameplay Bad, because limits tactical positioning precision (can’t be “between” grid cells) Bad, because requires special handling for diagonal movement distances Bad, because poor integration with real-time streaming protocol (discrete jumps) Property 3: Coordinate Chart Option 3.1: Cartesian (CHOSEN) Positions represented using Cartesian coordinates (x, y).\nGood, because universally familiar coordinate system Good, because algorithmic simplicity (standard distance and angle formulas) Good, because extensive library support in every programming language Good, because aligns naturally with rectangular boundaries Good, because direct mapping to pixel grids for rendering Good, because pathfinding algorithms designed for Cartesian grids Good, because simple boundary checks (x \u003c min, x \u003e max) Neutral, because appropriate for most spatial scenarios Bad, because requires trigonometry for angle calculations Bad, because polar coordinates may be more natural for rotational mechanics Option 3.2: Polar Positions represented using polar coordinates (r, θ) - distance and angle from origin.\nGood, because natural for rotational and radial mechanics Good, because distance from center is explicit (r coordinate) Good, because angle of position is explicit (θ coordinate) Neutral, because appropriate for circular arenas or radial gameplay Neutral, because familiar from mathematics and physics Bad, because requires trigonometric conversions for most operations Bad, because less universal familiarity (more complex than Cartesian) Bad, because limited library support (often converted to Cartesian internally) Bad, because aligns poorly with rectangular boundaries Bad, because distance between two polar points requires conversion Bad, because visualization requires conversion to screen coordinates Option 3.3: Discrete Cartesian (for Lattice) Positions represented using discrete Cartesian coordinates on a lattice (integer x, y).\nGood, because familiar Cartesian coordinate system Good, because simple integer arithmetic Good, because natural for grid-based pathfinding Good, because deterministic (no floating-point issues) Neutral, because only viable option for lattice vector space Bad, because limited to discrete grid positions Bad, because creates grid-to-grid jump artifacts Bad, because not chosen due to vector space decision (R^n continuous chosen) Property 4: Boundary Option 4.1: Unbounded No arena boundaries (infinite or very large playable area).\nGood, because enables unlimited strategic space Good, because no artificial boundaries constraining movement Good, because may feel more “realistic” for some scenarios Neutral, because appropriate for exploration-focused gameplay Neutral, because eliminates boundary collision checks Bad, because allows indefinite evasion (bots can run away forever) Bad, because enables stalemate even with timeout (avoid combat entire battle) Bad, because complicates pathfinding (no clear navigation boundaries) Bad, because difficult visualization (camera must track across large distances) Bad, because may create boring gameplay (chasing fleeing opponents) Bad, because timeout becomes only victory condition (no engagement guarantee) Bad, because eliminates positional strategy (no boundaries to control) Bad, because fundamentally incompatible with engagement-focused 1v1 gameplay Option 4.2: Rectangular Boundary (CHOSEN) Fixed rectangular arena boundaries that remain constant throughout the battle.\nGood, because guarantees engagement (bots cannot escape indefinitely) Good, because predictable arena enables reliable pathfinding Good, because simplest implementation (fixed boundary checks) Good, because fair and balanced (equal arena access) Good, because enables positional strategy (corner control, center dominance) Good, because complements timeout mechanism for battle conclusion Good, because aligns perfectly with Cartesian coordinates Good, because clear visualization (fixed arena visible throughout) Neutral, because could be enhanced with shrinking in future modes Neutral, because requires boundary collision detection Bad, because may enable corner camping defensive strategies Bad, because lacks additional engagement pressure Bad, because fixed size may feel static compared to dynamic boundaries Option 4.3: Circular Boundary Fixed circular arena boundary centered at origin.\nGood, because provides uniform distance from center for all boundary points Good, because guarantees engagement (bots cannot escape) Good, because symmetric and aesthetically pleasing Good, because no corners to enable camping strategies Neutral, because appropriate for radial or rotational gameplay Neutral, because requires distance calculation for boundary checks Bad, because boundary checks more expensive than rectangular (distance vs comparison) Bad, because aligns poorly with Cartesian coordinates (requires distance calculations) Bad, because more complex collision detection than rectangular boundaries Bad, because visualization may require circular clipping Bad, because pathfinding must account for curved boundaries More Information Related Documentation ADR-0004: Bot to Battle Server Interface: gRPC protocol for streaming position updates in continuous 2D space\nADR-0006: Bot Characteristics System: Speed and Mass characteristics that govern movement in this spatial system\nADR-0007: Equipment and Loadout System: Equipment that modifies movement capabilities within this spatial framework\nADR-0008: Bot Actions and Resource Management: Movement and combat actions that operate within this spatial system\nPOC User Journey: Proof of concept implementation using this spatial foundation\nNote: This ADR supersedes and integrates the former ADR-0006 (Battle Space Spatial System), which is now deprecated.\nFuture ADRs: The following topics were removed from this ADR and will be addressed in separate architectural decisions:\nCollision Detection and Bot Positioning (bot size, collision mechanics, collision resolution) Friction and Movement Physics (friction coefficients, velocity decay, variable friction zones) Line of Sight (visibility calculations, obstacle blocking) Implementation Notes Mathematical Foundation:\nThe BattleBot Universe is mathematically defined as:\nTopological Space: 2-dimensional Euclidean space R² Manifold: Closed rectangular region [−50, 50] × [−50, 50] ⊂ R² Metric: Standard Euclidean metric d(p,q) = √((x₂−x₁)² + (y₂−y₁)²) Coordinate Chart: Cartesian coordinates φ: R² → R² where φ(p) = (x, y) Boundary: ∂M = {(x,y) : x=±50 or y=±50} Numeric Value Refinement:\nThe arena size (100x100 units) is marked TBD and will be refined through:\nPlaytesting to tune arena size for engagement frequency Visualization testing for rendering clarity Timeout scenario frequency analysis to tune arena size appropriately Equipment balance testing to ensure stat-based equipment choices remain meaningful Key Design Insights:\n2D + R^n continuous + Cartesian + Rectangular boundaries create mutually reinforcing topological framework Mathematical rigor provides clear foundation for implementation Simplicity enables focus on core battle mechanics and accessibility Complete spatial framework enables users to implement sophisticated pathfinding and AI solutions Extensibility allows future enhancements (3D, fog of war, variable arenas) without disrupting foundation Future Considerations:\nVariable Arena Sizes: Different battle modes (quick match vs. tournament) may have different dimensions 3D Spatial System: If aerial combat or vertical positioning becomes strategically important, extend to R³ with (x, y, z) coordinates Circular Boundaries: Could be added as optional game mode for radial gameplay Dynamic Boundaries: Shrinking arena boundaries could be added as optional game mode for aggressive pacing Obstacles and Terrain: Spatial system designed to support static/dynamic obstacles and variable terrain effects Polar Coordinate Option: For specific mechanics (turret rotation, sensor sweeps) polar coordinates could supplement Cartesian system Design Principles The BattleBot Universe topological properties follow these principles:\nMathematical Rigor: Well-defined topological and geometric properties provide clear foundation Simplicity First: 2D continuous Euclidean space with Cartesian coordinates and rectangular boundaries reduce complexity Accessibility: Bot developers have familiar coordinate systems and well-documented algorithms Guaranteed Engagement: Bounded rectangular arena ensures battles conclude with interaction Predictability: Deterministic physics and movement with clear boundary rules Fairness: Symmetric arena and equal access to space for all bots Integration: Spatial framework seamlessly combines with characteristics, equipment, and actions Extensibility: Property-based structure allows future enhancements without disrupting core mechanics ","categories":"","description":"Mathematical and topological foundation defining the spatial structure of the battle space\n","excerpt":"Mathematical and topological foundation defining the spatial structure …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0005-battlebot-universe-topological-properties/","tags":"","title":"[0005] BattleBot Universe Topological Properties"},{"body":" Context and Problem Statement Battle Bots requires an attribute system that defines bot capabilities and creates strategic differentiation between different bot builds. We need to determine how many stats, what they represent, how they interact, and how they integrate with equipment customization. The characteristic system must support diverse playstyles while remaining accessible and understandable to bot developers.\nWithout a well-defined characteristic system, we cannot:\nDefine bot capabilities and limitations Create meaningful equipment modifications Balance different bot builds and strategies Design combat calculations and damage systems Enable strategic depth through stat optimization Provide clear feedback to bot developers about their bot’s strengths Decision Drivers Strategic Depth - System should create meaningful build choices and optimization paths Equipment Integration - Stats must be modifiable by equipment to enable customization Build Diversity - Multiple viable stat allocations should exist (no single dominant build) Developer Accessibility - Stats should be understandable without being overwhelming Calculation Simplicity - Stat interactions should be calculable and predictable Playstyle Support - Should enable distinct archetypes (tank, DPS, mobile, balanced) Spatial Integration - Must work with continuous 2D Euclidean space (ADR-0005) Decision Outcome The bot characteristics system consists of three core attributes that create strategic depth through meaningful interactions while remaining accessible to bot developers:\nHealth - Bot’s survivability pool; total damage a bot can sustain before destruction Defense - Damage mitigation capability; reduces effective damage from enemy attacks Mass - Equipment-derived weight; calculated from equipped items and impacts effective thrust-to-movement conversion This three-attribute system creates strategic depth through stat interactions (Effective HP, thrust-based mobility), enables equipment-driven tradeoffs via Mass, balances complexity with accessibility, and provides diverse optimization paths without overwhelming developers. Movement is governed by a thrust-based system (ADR-0009) where bots apply continuous thrust force to overcome friction (ADR-0005), with Mass affecting how efficiently thrust translates into velocity. The inclusion of equipment-derived Mass creates natural mobility-power tradeoffs that emerge from loadout choices.\nBot Characteristics Specification Health Health (HP) represents a bot’s survivability in combat. This is the primary resource that determines whether a bot remains operational in battle.\nKey Properties:\nHP Pool: Total damage a bot can sustain before destruction (TBD: placeholder 100-500 range) Damage Resistance: Works in conjunction with Defense stat to reduce incoming damage Destruction Threshold: Bot is eliminated when Health reaches 0 No Regeneration: Health does not regenerate during battle (current design) Gameplay Impact:\nHigher Health allows bots to sustain longer engagements Critical for aggressive playstyles that prioritize direct confrontation Must be balanced against offensive capabilities to ensure threat viability Low Health bots must rely on mobility (via thrust actions) and tactical positioning to survive Defense Defense represents a bot’s ability to mitigate incoming damage. This stat reduces the effective damage from enemy attacks.\nKey Properties:\nDamage Reduction: Percentage or flat reduction applied to incoming damage (TBD: placeholder 1-10) All Damage Types: Applies to all incoming damage sources (current design) Multiplicative with Health: Effective HP = Health × Defense multiplier No Evasion Component: Defense reduces damage taken, not hit chance Gameplay Impact:\nHigh Defense enables tank strategies and prolonged engagements Multiplicatively increases effective Health pool Critical for front-line and damage-absorbing playstyles Low Defense bots must rely on mobility (via thrust actions) for damage avoidance Defense vs. Health allocation creates build optimization choices Mass Mass represents the total physical weight of a bot, consisting of an intrinsic base mass plus the weight of equipment and components it carries. Unlike other characteristics, Mass is not directly allocated but is determined by the bot’s base mass and loadout choices.\nKey Properties:\nBase Mass: Every bot has an intrinsic starting mass (TBD: placeholder value) Equipment-Derived: Total Mass = Base Mass + sum of all equipped items Dynamic Value: Changes based on equipped weapons, armor, and modules Movement Impact: Higher Mass reduces acceleration from thrust actions (more force needed to overcome inertia and friction) Momentum Effects: Mass affects collision physics and knockback resistance (TBD) No Direct Damage Scaling: Mass affects mobility, not offensive capability Gameplay Impact:\nHeavy equipment loadouts reduce mobility through increased Mass (requiring more thrust to achieve same velocity) Creates natural tradeoff between firepower/protection and maneuverability Light bots sacrifice durability for superior acceleration and agility Mass cannot be optimized independently - it’s a consequence of equipment choices Forces strategic decisions between powerful equipment and tactical mobility Higher Mass requires sustained thrust to overcome friction and maintain velocity (ADR-0005) May affect collision mechanics and position displacement (future mechanics) Equipment Examples (TBD):\nWeapons: Heavy weapons (high Mass) vs. light weapons (low Mass) Armor: Heavy plating (high Mass) vs. light armor (low Mass) Modules: Power cores, sensors, and systems each contribute Mass Loadout variety creates diverse Mass profiles across bot builds Stat Interactions Bot characteristics don’t operate in isolation - they create complex interactions that define combat dynamics:\nEffective Durability: Health and Defense combine to determine true survivability:\nEffective HP Formula: Health × (1 + Defense modifier) High Defense multiplies the value of each Health point Balanced allocation is more efficient than single-stat stacking Example: 100 Health + 50% Defense = 150 Effective HP Mass and Mobility: Mass directly impacts effective movement capability:\nThrust-to-Velocity Relationship: Acceleration = Thrust Force / Mass (influenced by friction from ADR-0005) Heavy equipment increases Mass, reducing acceleration from thrust actions and requiring more sustained thrust to overcome friction Light loadouts maximize agility and responsiveness at the cost of offensive/defensive power Equipment choices fundamentally alter tactical capabilities through Mass-based mobility tradeoffs Combat Positioning: Thrust actions (ADR-0008) enable tactical positioning and engagement control:\nHigh thrust capacity allows kiting, pursuit, and disengagement Low-Mass bots have positioning advantage through superior acceleration Mass penalties from heavy equipment reduce positioning flexibility and increase thrust requirements Lightweight builds gain tactical mobility at the cost of durability Survivability Tradeoffs: Defensive investment creates complex build choices:\nHigh Health + Low Defense = vulnerable to sustained damage Low Health + High Defense = vulnerable to burst damage Mass from defensive equipment reduces mobility and increases thrust requirements for evasion Optimal defensive strategy depends on threat profile Loadout Optimization: Equipment choices (ADR-0007) create cascading effects across all characteristics:\nHeavy weapons increase offensive capability but increase Mass, reducing acceleration and requiring more thrust Armor improves Defense but adds Mass that limits mobility and increases friction effects Lightweight builds sacrifice protection for superior acceleration and lower thrust requirements No equipment configuration dominates all scenarios (intended design goal) Consequences Good, because three-stat system creates strategic depth without overwhelming developers Good, because stat interactions (Effective HP, thrust-based mobility) enable emergent complexity from simple rules Good, because equipment-derived Mass creates natural mobility-firepower tradeoffs through thrust mechanics Good, because multiple playstyles are viable (tank, DPS, mobile, balanced) through different stat profiles Good, because Defense × Health interaction rewards balanced allocation over single-stat stacking Good, because thrust-based movement (ADR-0008) with Mass and friction (ADR-0005) enables tactical gameplay through physics Good, because Mass cannot be optimized independently, forcing meaningful equipment tradeoffs Good, because stats map cleanly to combat calculations and physics-based movement Good, because movement physics create natural skill expression through thrust management Neutral, because stat values (Health range, Defense values) require extensive playtesting Neutral, because Mass modifier formulas and thrust-to-acceleration ratios need tuning to balance equipment weight penalties Neutral, because three stats create a minimal but sufficient foundation for strategic depth Bad, because thrust-based movement adds complexity compared to simple speed-based systems Bad, because stat interactions (especially Effective HP and thrust-Mass-friction relationships) add complexity to build optimization Bad, because equipment-derived Mass means loadout choices have cascading effects on movement that may confuse new developers Confirmation The decision will be confirmed through:\nImplementation of characteristic system in game server with stat calculation formulas Equipment system implementation (ADR-0007) that modifies stats and contributes Mass Playtesting with diverse bot builds (tank, DPS, balanced, mobile) to validate viability Balance analysis ensuring no single stat profile dominates all scenarios Developer feedback on stat system accessibility and understandability Combat simulation confirming stat interactions create meaningful tactical choices More Information Related Documentation ADR-0005: BattleBot Universe Topological Properties: Mathematical foundation and spatial system with friction mechanics that govern movement physics\nADR-0007: Equipment and Loadout System: Equipment that modifies stats and contributes to Mass\nADR-0008: Bot Actions and Resource Management: Actions that consume resources and leverage bot characteristics\nBot Characteristics Analysis: Detailed technical specifications for the stat system\nADR-0005: BattleBot Universe Topological Properties: Mathematical foundation and spatial system that these characteristics integrate with\nImplementation Notes All numeric values in this ADR are marked TBD (To Be Determined) and serve as placeholder values to establish the framework structure. These values will be refined through:\nCombat simulation to model stat interactions and balance implications Playtesting with diverse bot builds across different stat profiles Equipment balance analysis to ensure Mass penalties are meaningful but not punishing Health and Defense tuning to create appropriate effective HP ranges Thrust-to-Mass ratio balancing to ensure mobility advantages are significant but not overwhelming Friction coefficient tuning (ADR-0005) to balance movement physics Competitive meta analysis to identify dominant builds and adjust accordingly Key Design Insight: Mass consists of intrinsic base mass plus equipment weight, not directly allocated as a stat. This creates emergent tradeoffs where powerful equipment inherently reduces mobility (through increased thrust requirements and friction effects), forcing strategic loadout decisions without requiring explicit stat allocation. Movement is governed by thrust actions (ADR-0008) that must overcome both Mass-based inertia and friction forces (ADR-0005).\nFuture Considerations:\nAdditional derived stats (e.g., Effective HP, effective acceleration) may be exposed to developers Attack stat may be added if weapon damage needs per-bot customization Energy stat may become a direct characteristic if resource management complexity increases Evasion or Accuracy stats may be added if hit-chance mechanics are introduced Thrust capacity may become a direct characteristic if thrust-based movement requires per-bot customization beyond equipment Design Principles The characteristic system follows these principles:\nInteractions over Isolation: Stats combine to create emergent complexity Tradeoffs over Power: Equipment choices involve costs and benefits through Mass Diversity over Dominance: Multiple stat profiles should be competitively viable Clarity over Complexity: Four stats balance depth with accessibility Equipment Integration: Stats are modified by loadout choices (ADR-0007) ","categories":"","description":"Attribute system defining bot capabilities and creating strategic differentiation\n","excerpt":"Attribute system defining bot capabilities and creating strategic …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0006-bot-characteristics-system/","tags":"","title":"[0006] Bot Characteristics System"},{"body":" Context and Problem Statement Battle Bots requires a customization system that enables players to differentiate their bots before battle begins. We need to determine how bots customize their capabilities, what equipment types exist, how equipment affects performance, and how loadout constraints create strategic tradeoffs. The equipment system must create meaningful build diversity while integrating with the characteristics system (ADR-0006) and action system (ADR-0008).\nWithout a well-defined equipment system, we cannot:\nCreate distinct bot builds and playstyles Enable pre-battle strategic decisions Provide stat customization and optimization paths Define which actions are available to bots Balance power vs. mobility tradeoffs Support future equipment additions and expansions Decision Drivers Build Diversity - Multiple viable loadouts should exist with distinct strengths and weaknesses Pre-Battle Strategic Decisions - Equipment choices should matter and create differentiation Stat Modification Clarity - Equipment effects on characteristics (ADR-0006) should be understandable Equipment-Action Coupling - Equipment should enable/disable specific actions (ADR-0008) Tradeoff Mechanics - Equipment should involve costs and benefits (no strictly superior choices) Extensibility - System should support future equipment additions without redesign Protocol Integration - Equipment selection must map to gRPC protocol (ADR-0004) Developer Accessibility - Loadout configuration should be straightforward for bot developers Decision Outcome The equipment system consists of two equipment categories (Weapons, Armor) with the following initial equipment options:\nWeapons: Rifle, Shotgun Armor: Light Armor, Medium Armor, Heavy Armor Each bot equips a loadout with 1 weapon and 1 armor. Equipment modifies bot characteristics (ADR-0006) and determines available actions (ADR-0008), creating distinct tactical options for different builds.\nConsequences Good, because equipment loadout system creates meaningful build diversity (DPS, Tank, Balanced, Mobile/Skirmisher) Good, because pre-battle equipment selection enables strategic planning and counter-play Good, because stat modifications are clear and calculable (simple additive formula) Good, because equipment-action coupling (ADR-0008) creates distinct tactical options for different builds Good, because Defense vs. Speed tradeoffs in armor create natural power-mobility choices Good, because loadout constraints force meaningful choices Good, because modular structure (each equipment type and option in dedicated sections) makes equipment additions straightforward Good, because initial equipment set provides foundation for expansion Good, because example builds demonstrate viable diversity and counter-play options Neutral, because stat modification values (all TBD) require extensive playtesting Neutral, because loadout slot counts (1 weapon, 1 armor) need validation through testing Neutral, because equipment balance requires tuning to ensure no dominant loadout Bad, because equipment system adds complexity for bot developers to understand Bad, because stat modifications and action requirements create dependencies to track Bad, because equipment balance is critical - single dominant loadout would eliminate diversity Confirmation The decision will be confirmed through:\nImplementation of equipment system in game server with stat modification calculations Bot SDK exposing loadout configuration and equipment selection interface Playtesting with all four example builds (DPS, Tank, Balanced, Stealth) to validate viability Competitive balance analysis ensuring no single loadout dominates all matchups Counter-play validation confirming viable counter-builds exist for each archetype Developer feedback on equipment configuration clarity and accessibility Protocol integration testing for equipment selection and validation Weapons Weapons enable combat actions and determine offensive capabilities. Each weapon provides a unique attack action with different energy costs, damage patterns, and tactical applications. All weapons contribute to bot Mass (ADR-0006), with heavier weapons providing greater firepower at the cost of mobility.\nRifle Standard precision weapon enabling reliable ranged attacks.\nStat Effects:\nNo modifications (baseline weapon) Mass Contribution: TBD (baseline weapon mass) Tactical Profile:\nReliable ranged damage output Versatile baseline option suitable for any playstyle No stat penalties, maintains mobility Effective at medium to long range Shotgun Close-range weapon enabling devastating burst damage with damage falloff based on distance.\nStat Effects:\n-1 Speed (weight penalty) (TBD) -1 Range (close-range weapon) (TBD) Mass Contribution: TBD (higher than Rifle) Tactical Profile:\nHigh close-range burst damage Requires positioning to maximize effectiveness Weight penalty reduces mobility Ineffective at long range due to damage falloff Armor Armor provides defensive bonuses and damage mitigation. Armor directly modifies Defense and Speed characteristics (ADR-0006), creating tradeoffs between survivability and mobility. All armor contributes to bot Mass, with heavier armor providing greater protection at the cost of reduced Speed.\nLight Armor Minimal protection that maintains mobility for evasive playstyles.\nStat Effects:\n+1 Defense (TBD) +0 Speed (no speed penalty) (TBD) Mass Contribution: TBD (minimal) Tactical Profile:\nMinimal defense bonus maintains baseline survivability No speed penalty preserves mobility Optimal for evasion-based and high-mobility builds Relies on Speed rather than damage absorption Medium Armor Balanced protection with moderate defensive bonus and minor speed penalty.\nStat Effects:\n+2 Defense (TBD) -1 Speed (TBD) Mass Contribution: TBD (moderate) Tactical Profile:\nReasonable defense without severe mobility cost Versatile option for balanced builds Moderate survivability increase with manageable speed reduction Suitable for all-around playstyles Heavy Armor Maximum protection with significant defensive bonus and substantial speed penalty.\nStat Effects:\n+3 Defense (TBD) -2 Speed (TBD) Mass Contribution: TBD (high) Tactical Profile:\nMaximum damage reduction for tank builds Significant speed penalty limits mobility Enables prolonged engagements and damage absorption Requires positional awareness due to low mobility Example Loadouts The following example loadouts demonstrate the range of viable bot configurations and playstyle diversity using the equipment options defined above:\nDPS (Damage Per Second) Build\nPhilosophy: Maximize offensive capability and damage output. Accept lower survivability for high burst damage potential.\nEquipment:\nWeapon: Shotgun (high damage spray at close range) Armor: Light Armor (minimal defense, maintain mobility) Stat Profile (TBD values):\nAttack: 12 (high) Defense: 6 (low) Speed: 10 (good) Energy: 10 (average) Strategy: Close distance quickly, deliver devastating shotgun blasts at close range. High risk, high reward playstyle focused on burst damage.\nWeaknesses: Vulnerable to sustained fire, limited survivability if caught in poor position, ineffective at long range.\nTank Build\nPhilosophy: Maximum survivability and staying power. Control space through defensive presence and outlast opponents.\nEquipment:\nWeapon: Rifle (reliable baseline offense) Armor: Heavy Armor (maximum damage reduction) Stat Profile (TBD values):\nAttack: 8 (moderate) Defense: 13 (very high) Speed: 8 (low) Energy: 10 (average) Strategy: Hold key positions, absorb damage with heavy armor, maintain reliable ranged offense with rifle. Win through attrition rather than burst damage.\nWeaknesses: Low mobility makes positioning critical, vulnerable to kiting strategies, limited offensive pressure.\nBalanced Build\nPhilosophy: Versatile configuration capable of adapting to various situations. No extreme weaknesses, no extreme strengths.\nEquipment:\nWeapon: Rifle (reliable ranged damage) Armor: Medium Armor (reasonable defense without severe speed penalty) Stat Profile (TBD values):\nAttack: 10 (above average) Defense: 10 (above average) Speed: 9 (average) Energy: 10 (average) Strategy: Maintain optimal engagement distance with rifle, rely on well-rounded stats to handle unexpected situations. Adaptable to opponent strategies.\nWeaknesses: Lacks specialization, may be outperformed by specialized builds in their areas of strength.\nMobile/Skirmisher Build\nPhilosophy: Control battlefield through mobility and positioning rather than direct combat superiority.\nEquipment:\nWeapon: Rifle (precise long-range attacks) Armor: Light Armor (maintain mobility) Stat Profile (TBD values):\nAttack: 8 (moderate) Defense: 6 (low) Speed: 12 (very high) Energy: 10 (average) Strategy: Strike from optimal angles with rifle shots, rely on superior mobility to control engagement distance and disengage when needed. Win through tactical advantage and superior positioning rather than sustained combat.\nWeaknesses: Ineffective in forced direct combat, vulnerable if caught in poor position, relies on maintaining optimal range.\nMore Information Related Documentation ADR-0006: Bot Characteristics System: Stats that equipment modifies (Health, Speed, Defense, Mass)\nADR-0008: Bot Actions and Resource Management: Actions that equipment enables or requires\nEquipment System Analysis: Detailed technical specifications for equipment types and loadouts\nADR-0005: BattleBot Universe Topological Properties: Mathematical foundation and spatial system that equipment integrates with\nADR-0004: Bot to Battle Server Interface: gRPC protocol for equipment selection and validation\nImplementation Notes Modular Structure and Extensibility:\nThis ADR defines the initial equipment options for each category (2 weapons, 3 armor types). The modular structure—with each equipment type and option in dedicated sections—enables straightforward expansion:\nAdding New Equipment Options: New equipment can be added by simply adding new subsections under the appropriate category (Weapons or Armor). For example, “Laser Rifle” could be added as a new subsection under Weapons. Adding New Equipment Categories: New equipment categories (e.g., “Chassis Types” or “Power Cores”) can be introduced by adding new top-level sections to the Equipment System Specification. Modifying Existing Equipment: Specific equipment subsections can be superseded to rebalance or redesign individual items without affecting other equipment. Stat Value Refinement:\nAll numeric values in this ADR are marked TBD (To Be Determined) and serve as placeholder values to establish the framework structure. These values will be refined through:\nPlaytesting with all four example builds to validate competitive viability Balance modeling to ensure no single loadout dominates all matchups Counter-play analysis confirming viable counter-builds exist for each archetype Stat tuning to create meaningful tradeoffs (Defense vs. Speed, Power vs. Mobility) Weapon balance to ensure Rifle and Shotgun are situationally viable Armor balance to ensure Light/Medium/Heavy each have optimal use cases Key Design Insights:\nEquipment-action coupling ensures loadout choices directly affect tactical options Defense vs. Speed tradeoffs in armor create natural tank vs. mobile playstyle spectrum Example builds demonstrate diversity while providing optimization starting points Modular structure (dedicated sections per equipment) simplifies expansion and balance changes Design Principles The equipment system follows these principles:\nTradeoffs over Power: All equipment involves costs and benefits Diversity over Dominance: Multiple loadouts should be competitively viable Coupling over Independence: Equipment determines both stats and actions Constraints over Freedom: Slot limits force meaningful choices Extensibility over Finality: Framework supports future equipment additions ","categories":"","description":"Bot customization through weapons and armor enabling strategic diversity\n","excerpt":"Bot customization through weapons and armor enabling strategic …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0007-equipment-loadout-system/","tags":"","title":"[0007] Equipment and Loadout System"},{"body":" Context and Problem Statement Battle Bots requires an action system that governs how bots behave in real-time battles. We need to define what actions bots can perform, how actions are constrained (costs, cooldowns, equipment requirements), and how the resource management system prevents action spam while maintaining fluid gameplay. The action system must integrate with the real-time continuous gameplay model and support the equipment-based customization system (ADR-0006).\nWithout a well-defined action system, we cannot:\nDesign the game server tick processing and action resolution Create bot SDK action interfaces and methods Define protocol messages for action submission and validation Implement resource management (energy) and timing constraints (cooldowns) Balance tactical decision-making and action variety Prevent action spam or create action starvation scenarios Decision Drivers Real-time Gameplay Support - Must work with continuous tick-based game loop (not turn-based) Resource Management Depth - Should create strategic decisions about when to use actions Tactical Timing Decisions - Cooldowns should create timing choices beyond raw resource availability Equipment Integration - Actions must respect equipment requirements (ADR-0007) Balance Flexibility - Energy costs and cooldowns provide tuning knobs for game balance Action Spam Prevention - System must prevent trivial spamming of powerful actions Gameplay Fluidity - Basic actions (Move) should remain fluid and responsive Considered Options Option 1: Cooldown-Only System - Actions limited by time-based cooldowns only (no energy) Option 2: Energy-Only System - Actions limited by energy pool regeneration only (no cooldowns) Option 3: Dual-Constraint System - Both energy costs AND cooldowns required Option 4: Action Points Per Turn - Traditional turn-based action economy Decision Outcome Chosen option: “Option 3: Dual-Constraint System (energy costs AND cooldowns)”, because it creates both resource management decisions (energy) and tactical timing decisions (cooldowns), prevents action spam while maintaining fluidity for basic actions, enables diverse action costs for balance tuning, and integrates naturally with equipment system and real-time gameplay.\nBot Actions and Resource Management Specification Resource Management System Energy Pool (from ADR-0006: Bot Characteristics):\nAll actions consume energy from a limited pool Energy regenerates over time at a fixed rate (TBD) Insufficient energy prevents action execution Energy creates strategic resource management (choosing when to spend energy) Equipment can modify energy capacity (e.g., Boost Engine: -1 Energy Capacity) Cooldown System:\nActions have cooldown periods measured in game ticks Cooldowns prevent immediate re-use of actions Cooldowns create tactical timing decisions (when to use powerful abilities) Independent of energy (both must be satisfied to perform action) Dual Constraints: Both energy availability AND cooldown status must be satisfied to perform actions:\nBasic actions (Move) have low energy cost and no cooldown for fluid movement Powerful actions (Shield, ShotgunBlast) have higher energy costs and longer cooldowns for balance This dual system prevents both energy-based spam and cooldown-based spam Action Categories Actions are organized into four categories based on their primary purpose:\nMovement Actions: Navigate the 2D Euclidean battle space (ADR-0005)\nCombat Actions: Deal damage to opponent bots\nDefensive Actions: Mitigate incoming damage or avoid attacks\nUtility Actions: Provide information or modify bot state\nAction Catalog Movement Actions\nMove: Moves the bot in a specified direction within the battle space\nEnergy Cost: 5 (TBD) Cooldown: 0 ticks (TBD) Equipment Required: None (universal action) Parameters: Direction vector or target coordinates Constraints: Cannot move through obstacles or other bots (ADR-0005 collision rules) Tactical Use: Positioning, pursuit, evasion, range control Combat Actions\nRifleShot: Single-shot, precise ranged attack\nEnergy Cost: 15 (TBD) Cooldown: 1 tick (TBD) Equipment Required: Rifle (ADR-0007) Damage: Moderate (TBD) Range: Long (TBD) Constraints: Requires line of sight (ADR-0005), rifle must be equipped Tactical Use: Consistent ranged damage, reliable baseline offense ShotgunBlast: Spray of projectiles effective at close range\nEnergy Cost: 20 (TBD) Cooldown: 2 ticks (TBD) Equipment Required: Shotgun (ADR-0007) Damage: High at close range with falloff based on distance (TBD) Range: Short to medium (TBD) Constraints: Shotgun must be equipped, damage decreases with distance Tactical Use: High burst damage at close range, requires positioning Defensive Actions\nBlock: Reduces damage from incoming attacks\nEnergy Cost: 10 (TBD) Cooldown: 2 ticks (TBD) Equipment Required: None (universal action) Effect: Reduces damage by percentage (TBD) Duration: Active for current tick only Constraints: Must be activated before damage is received Tactical Use: Mitigate predicted incoming damage, reduce burst damage Evade: Attempts to dodge incoming attacks\nEnergy Cost: 15 (TBD) Cooldown: Variable (TBD) Equipment Required: None (universal action) Effect: Chance to completely avoid attack (TBD) Duration: Active for current tick only Constraints: Success rate may depend on bot Speed stat (ADR-0006) Tactical Use: Avoid high-damage attacks, risky but high-reward defense Shield: Activates energy shield that absorbs damage over multiple ticks\nEnergy Cost: 20 initial + ongoing drain (TBD) Cooldown: Variable (TBD) Equipment Required: None (universal action) Effect: Absorbs damage up to threshold (TBD) Duration: Sustained (multiple ticks with ongoing energy drain - TBD) Constraints: Ongoing energy cost while active may limit other actions Tactical Use: Extended damage mitigation, control space through invulnerability window Utility Actions\nScan: Gathers information about environment and nearby bots\nEnergy Cost: 5 (TBD) Cooldown: Variable (TBD) Equipment Required: Sensor Array module recommended (ADR-0007) Effect: Returns information about nearby entities (positions, health, etc.) Range: Limited detection radius, enhanced by Sensor Array (TBD) Constraints: May only reveal information within range Tactical Use: Information gathering, enemy tracking, battlefield awareness Charge: Increases energy regeneration rate temporarily\nEnergy Cost: Variable (may cost initial energy or pause regen - TBD) Cooldown: Variable (TBD) Equipment Required: None (universal action) Effect: Boosts energy regeneration for duration (TBD) Duration: Multiple ticks (TBD) Constraints: Bot may be vulnerable while charging (cannot perform other actions - TBD) Tactical Use: Energy economy management, prepare for energy-intensive action sequences Equipment-Dependent Actions (from ADR-0007)\nBoost: Temporary speed increase for repositioning\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Boost Engine module Effect: Increases Speed temporarily (TBD) Tactical Use: Close distance for shotgun attacks, escape dangerous positions Repair: Limited self-repair during combat\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Repair Kit module Effect: Restores HP (TBD amount, usage limits) Tactical Use: Extend combat effectiveness, sustain tank builds Cloak: Reduces detection range by enemies\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Stealth Module Effect: Reduces enemy detection range temporarily Tactical Use: Avoid detection, enable surprise attacks, stealth positioning Action Execution and Processing Action Submission:\nBots submit actions via the bot-to-server communication protocol Actions are queued and processed during the next game tick Multiple actions may be submitted if resources permit (parallel actions TBD) Action Validation:\nServer validates energy availability before execution Server validates cooldown status before execution Server validates equipment requirements before execution Invalid actions are rejected with error feedback to bot Action Resolution:\nActions are resolved during the game tick cycle Action outcomes are broadcast to all bots via the communication protocol State changes (damage, position, energy) are updated and synchronized Consequences Good, because dual-constraint system (energy + cooldowns) creates both resource management and tactical timing layers Good, because basic actions (Move) have low cost and no cooldown for fluid gameplay Good, because powerful actions (Shield, ShotgunBlast) have high costs and cooldowns preventing spam Good, because equipment-dependent actions integrate naturally with loadout system (ADR-0007) Good, because energy costs and cooldowns provide independent tuning knobs for balance Good, because action variety (Movement, Combat, Defensive, Utility) enables diverse playstyles Good, because universal actions (Move, Block, Evade) are always available regardless of equipment Good, because real-time action submission enables responsive gameplay Neutral, because all energy costs and cooldown values are TBD requiring extensive playtesting Neutral, because dual resource system (energy + cooldowns) is more complex than single-constraint systems Neutral, because energy regeneration rate requires careful tuning to avoid starvation or abundance Bad, because dual-constraint system adds complexity for bot developers to manage Bad, because energy management adds cognitive load beyond simple cooldown tracking Bad, because equipment-dependent actions require validation overhead in protocol Bad, because action spam prevention must be tuned carefully to avoid frustrating resource starvation Confirmation The decision will be confirmed through:\nImplementation of action system in game server with energy and cooldown tracking Bot SDK exposing action methods with energy/cooldown visibility Playtesting action economy to validate energy costs and cooldowns feel balanced Action spam testing to confirm dual constraints prevent trivial ability spam Equipment-action integration testing to validate equipment requirements work correctly Balance tuning of energy costs, cooldowns, and regeneration rates through competitive gameplay Protocol integration testing for action submission, validation, and feedback Pros and Cons of the Options Option 1: Cooldown-Only System Actions limited by time-based cooldowns only, no energy resource.\nGood, because simpler than dual-constraint system Good, because eliminates energy tracking and management complexity Good, because easier for developers to understand (just wait for cooldown) Good, because predictable action availability based on time Neutral, because may be sufficient for simple action economies Bad, because lacks resource management strategic layer Bad, because no cost for spamming available actions (just wait for cooldowns) Bad, because difficult to balance rapid low-cooldown actions vs. slow high-impact actions Bad, because no energy scarcity creates no opportunity cost for action use Bad, because equipment cannot modify energy capacity for strategic effect Option 2: Energy-Only System Actions limited by energy pool regeneration only, no cooldowns.\nGood, because creates resource management decisions Good, because energy scarcity forces strategic action choices Good, because simpler than dual-constraint system Good, because equipment can modify energy capacity for build diversity Neutral, because single resource may be sufficient for action economy Bad, because powerful actions can be spammed rapidly if energy is available Bad, because no timing constraints beyond energy availability Bad, because burst damage strategies may dominate (spend all energy immediately) Bad, because difficult to prevent action spam without introducing effective cooldowns anyway Bad, because regeneration rate becomes sole balancing mechanism Option 3: Dual-Constraint System (energy + cooldowns) Both energy costs AND cooldowns required (CHOSEN).\nGood, because creates resource management (energy) and tactical timing (cooldowns) Good, because prevents action spam through dual constraints Good, because basic actions can have low cost and no cooldown for fluidity Good, because powerful actions can have high cost and long cooldown for balance Good, because provides two independent tuning knobs for balance adjustments Good, because equipment can modify energy capacity for strategic effect Good, because cooldowns prevent burst-spam even with high energy Good, because energy scarcity creates opportunity cost for action use Neutral, because requires careful tuning to avoid frustrating starvation or trivial abundance Neutral, because dual constraints require understanding both systems Bad, because more complex than single-constraint systems Bad, because adds cognitive load for bot developers to track both resources Bad, because requires tuning two systems instead of one Option 4: Action Points Per Turn Traditional turn-based action point economy.\nGood, because familiar system from many turn-based games Good, because deterministic action availability per turn Good, because easy to balance (X actions per turn) Good, because clear constraints on action usage Neutral, because works well for turn-based gameplay Bad, because poor fit for real-time continuous gameplay model Bad, because turn boundaries create artificial rhythm contrary to real-time design Bad, because gRPC protocol designed for continuous action submission, not turns Bad, because eliminates timing decisions within turns Bad, because rigid action economy may feel restrictive in real-time battles More Information Related Documentation ADR-0006: Bot Characteristics System: Energy pool characteristic that actions consume\nADR-0007: Equipment and Loadout System: Equipment that enables/disables actions and modifies energy\nADR-0005: BattleBot Universe Topological Properties: Mathematical foundation and spatial system where movement and combat actions occur\nBot Actions Analysis: Detailed technical specifications for action catalog\nADR-0004: Bot to Battle Server Communication Protocol: Communication protocol choice (gRPC) for bot-to-server interface\nImplementation Notes All numeric values in this ADR are marked TBD (To Be Determined) and serve as placeholder values to establish the framework structure. These values will be refined through:\nEnergy economy modeling to determine regeneration rates and pool sizes Playtesting action costs to validate energy expenditure feels balanced Cooldown tuning to prevent action spam without creating frustrating delays Balance analysis of powerful vs. basic actions (Shield vs. Move) Equipment-action integration testing to validate equipment requirements Competitive gameplay to identify dominant action patterns and adjust accordingly Developer feedback on action system complexity and usability Key Design Insights:\nBasic actions (Move: 5 energy, 0 cooldown) enable fluid gameplay Powerful actions (Shield: 20 energy, variable cooldown) are constrained to prevent spam Equipment-dependent actions create loadout-specific tactical options Universal actions ensure all bots have baseline capabilities Energy System Tuning Considerations:\nEnergy regeneration rate determines action frequency baseline Energy pool size determines burst action capability Energy costs should scale with action power (Move: 5, RifleShot: 15, Shield: 20) Equipment energy modifiers enable build optimization (Boost Engine: -1 capacity) Cooldown System Tuning Considerations:\nCooldowns prevent rapid-fire use of powerful abilities Cooldown duration should reflect action impact (Shield longer than RifleShot) Zero-cooldown actions (Move) enable continuous use if energy permits Cooldowns create timing windows for counter-play Future Enhancements:\nAdditional action types (area-of-effect, status effects, debuffs) Combo actions that trigger based on action sequences Interrupt actions that counter opponent actions Charged actions that increase in power with longer preparation Resource conversion actions (health for energy, etc.) Design Principles The action and resource system follows these principles:\nDual Constraints: Energy and cooldowns create layered decision-making Fluidity for Basics: Low-cost, no-cooldown basic actions maintain gameplay flow Constraints for Power: High-cost, long-cooldown powerful actions prevent spam Equipment Integration: Actions respect equipment requirements and modifications Real-time Fit: System designed for continuous gameplay, not turn-based ","categories":"","description":"Action system governing bot behavior and creating tactical decision-making in real-time battles\n","excerpt":"Action system governing bot behavior and creating tactical …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/0008-bot-actions-resource-management/","tags":"","title":"[0008] Bot Actions and Resource Management"},{"body":"User Journeys This section contains detailed user journey documentation that defines how users interact with the Battlebots platform. Each journey document includes:\nUser personas and their goals Step-by-step flow diagrams Technical requirements (access control, analytics, etc.) Success metrics These documents serve as the foundation for feature development and help ensure a consistent, user-centered experience.\n","categories":"","description":"Documentation of user flows and experiences for the Battlebots platform\n","excerpt":"Documentation of user flows and experiences for the Battlebots …","ref":"/battlebots/pr-preview/pr-153/research_and_development/user-journeys/","tags":"","title":"User Journeys"},{"body":"In 1v1 battles, two bots face off in direct combat within a bounded 2D arena. Victory goes to the bot that reduces its opponent’s health to zero or has more health when the timeout expires.\nWhat You’ll Find Here This section contains complete mechanics documentation for 1v1 battles:\nGetting Started - Quick start guide with a minimal bot example Arena - The 2D battle space, coordinates, boundaries, collision, and physics Bot Characteristics - Health, Defense, and Mass stats that define your bot Equipment - Weapons and armor that customize your bot’s capabilities Actions - All available actions, energy costs, and cooldowns Key Concepts Real-Time Gameplay 1v1 battles operate in real-time with a continuous tick-based game loop. Your bot receives state updates each tick and can submit actions to perform.\nWin Conditions Elimination: Reduce opponent’s Health to 0 Timeout: Have more Health than opponent when time expires Bot Customization Before battle, you configure your bot’s equipment loadout (1 weapon + 1 armor). During battle, your bot performs actions that consume energy and have cooldown periods.\nProgramming Challenge This documentation explains the mechanics and rules. Developing effective bot logic, decision-making algorithms, and winning strategies is your challenge as a programmer!\nGet Started New to 1v1 battles? Start with the Getting Started guide to create your first bot, then explore the mechanics pages to understand the full system.\n","categories":"","description":"Complete mechanics guide for 1v1 battles - arena, bot characteristics, equipment, and actions\n","excerpt":"Complete mechanics guide for 1v1 battles - arena, bot characteristics, …","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/","tags":"","title":"1v1 Battles"},{"body":"This guide will help you create your first bot for 1v1 battles. We’ll cover the basics of bot structure, equipment configuration, and connecting to the battle server.\nPrerequisites Programming experience in any language Battle Bots SDK installed for your chosen language Development environment set up Bot Structure A Battle Bots bot consists of:\nEquipment Configuration - Choose your weapon and armor before battle State Processing - Receive and process battle state updates each tick Action Submission - Decide which actions to perform based on current state Communication - Connect to the battle server via gRPC Minimal Bot Example Here’s a minimal bot structure to get you started:\nStep 1: Configure Equipment Before battle, configure your bot’s loadout with 1 weapon and 1 armor:\n# Example: Simple balanced loadout loadout = { \"weapon\": \"Rifle\", # Baseline precision weapon \"armor\": \"Light Armor\" # Minimal defense, maintains mobility } See the Equipment page for all available weapons and armor options.\nStep 2: Connect to Battle Server Your bot connects to the battle server using the SDK:\nfrom battlebots_sdk import BattleBot bot = BattleBot(loadout=loadout) bot.connect() Step 3: Process State Updates Each tick, your bot receives a state update with:\nYour bot’s position, health, energy Opponent bot’s position (if visible) Arena boundaries Available actions def process_state(state): my_position = state.my_bot.position # (x, y) coordinates my_health = state.my_bot.health my_energy = state.my_bot.energy opponent = state.opponent_bot # May be None if out of sight if opponent: opponent_position = opponent.position opponent_health = opponent.health return decide_action(state) Step 4: Submit Actions Based on the state, your bot decides which action to perform:\ndef decide_action(state): # Example: Simple logic if state.opponent_bot: # Opponent visible - check if in range distance = calculate_distance(state.my_bot.position, state.opponent_bot.position) if distance \u003c RIFLE_RANGE and state.my_bot.energy \u003e= 15: return Action.RifleShot(target=state.opponent_bot.position) # Move toward center if opponent not visible return Action.Move(target=(0, 0)) Step 5: Run Your Bot # Main bot loop while bot.is_battle_active(): state = bot.get_current_state() action = process_state(state) bot.submit_action(action) Next Steps Now that you have a basic bot structure, learn about the game mechanics:\nArena - Understand the battle space, coordinates, and collision rules Bot Characteristics - Learn about Health, Defense, and Mass stats Equipment - Explore different weapons and armor options Actions - Discover all available actions, their costs, and constraints Important Notes SDK Abstracts Complexity: Your SDK handles gRPC communication, coordinate calculations, and state management Tick-Based: Battles run in real-time ticks; your bot should process state and respond quickly Energy Management: Monitor your energy pool; actions cost energy to perform Cooldowns: Some actions have cooldown periods before they can be used again Start simple, test your bot in battles, and iterate on your approach. The mechanics documentation will help you understand the full system, but figuring out effective bot logic is your challenge!\n","categories":"","description":"Quick start guide for creating your first 1v1 battle bot\n","excerpt":"Quick start guide for creating your first 1v1 battle bot\n","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/getting-started/","tags":"","title":"Getting Started"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/","tags":"","title":"Analysis"},{"body":"1v1 battles take place in a 2D rectangular arena with defined boundaries. The arena uses a Cartesian coordinate system and applies physics rules for movement, collision, and friction.\nCoordinate System The arena uses a 2D Cartesian coordinate system - the same (x, y) coordinates you learned in math class.\ngraph TD subgraph Arena[\"Arena: 100x100 units\"] origin[\"Origin (0, 0)\u003cbr/\u003eCenter of arena\"] topRight[\"+50, +50\u003cbr/\u003eTop-Right Corner\"] topLeft[\"-50, +50\u003cbr/\u003eTop-Left Corner\"] bottomRight[\"+50, -50\u003cbr/\u003eBottom-Right Corner\"] bottomLeft[\"-50, -50\u003cbr/\u003eBottom-Left Corner\"] end style origin fill:#4CAF50 style topRight fill:#f9f style topLeft fill:#f9f style bottomRight fill:#f9f style bottomLeft fill:#f9f Key Properties Origin (0, 0): Located at the center of the arena X-axis: Horizontal axis Positive values extend to the right Negative values extend to the left Range: -50 to +50 Y-axis: Vertical axis Positive values extend upward Negative values extend downward Range: -50 to +50 Units: Abstract spatial units (not meters or pixels) Precision: Floating-point coordinates allow sub-unit positioning Distance Calculation Distance between two points uses the Pythagorean theorem:\ndistance = √((x₂ - x₁)² + (y₂ - y₁)²) Your SDK provides helper functions for distance calculations.\nArena Boundaries The battle space is bounded by a 100 × 100 unit rectangular arena.\nBoundary Rules X-axis boundaries: x = -50 (left wall) and x = +50 (right wall) Y-axis boundaries: y = -50 (bottom wall) and y = +50 (top wall) Movement clamping: Bots cannot move outside boundaries; position is clamped to the edge No wrapping: Coordinates do not wrap around (leaving right side doesn’t place you on left) No wall damage: Colliding with walls does not cause damage Elastic collisions: Bots stop at the wall without bouncing Bot Positioning and Collision Each bot occupies a circular area in the arena.\nBot Footprint Bot radius: 2 units Center position: Bot coordinates (x, y) represent the center of its circular footprint No overlap: Bots cannot overlap; their circular areas must not intersect graph LR subgraph BotCollision[\"Bot Collision Detection\"] A[\"Bot A\u003cbr/\u003ePosition: (10, 20)\u003cbr/\u003eRadius: 2\"] B[\"Bot B\u003cbr/\u003ePosition: (14, 20)\u003cbr/\u003eRadius: 2\"] distance[\"Distance = 4 units\u003cbr/\u003eSum of radii = 4 units\u003cbr/\u003eStatus: Touching\"] end A -.Distance = 4.-\u003e B style A fill:#4CAF50 style B fill:#2196F3 Collision Detection Bot-to-Bot Collision:\nTwo bots collide when the distance between their centers is less than the sum of their radii For identical bots: collision occurs when distance \u003c 4 units (2 + 2) Collision prevents movement through the other bot Bot-to-Wall Collision:\nLeft wall: x - radius \u003c -50 Right wall: x + radius \u003e 50 Bottom wall: y - radius \u003c -50 Top wall: y + radius \u003e 50 Collision Resolution When collision is detected:\nMovement is adjusted to place the bot at the point of contact Bots stop at the obstacle without pushing or displacement No damage is applied from collision Physics Friction The arena applies friction to all moving bots, affecting their velocity over time.\ngraph LR A[\"Tick 1:\u003cbr/\u003eApply Move action\u003cbr/\u003eVelocity increases\"] --\u003e B[\"Tick 2:\u003cbr/\u003eNo Move action\u003cbr/\u003eFriction reduces velocity\"] B --\u003e C[\"Tick 3:\u003cbr/\u003eNo Move action\u003cbr/\u003eVelocity continues decreasing\"] C --\u003e D[\"Tick 4+:\u003cbr/\u003eEventually stops\u003cbr/\u003eVelocity reaches 0\"] style A fill:#4CAF50 style B fill:#FFC107 style C fill:#FF9800 style D fill:#F44336 Friction Mechanics Friction coefficient (μ): 0.1 (subject to balance tuning) Velocity decay: Each tick, friction reduces your bot’s velocity Natural stopping: Without continuous thrust, your bot will gradually slow to a stop Continuous movement: Maintaining velocity requires repeated Move actions Friction Formula friction_force = -μ × velocity new_velocity = velocity + friction_force The negative sign indicates friction opposes the direction of movement.\nVariable Friction Zones Uniform friction: By default, the entire arena has uniform friction (μ = 0.1) Future feature: Specific areas may have different friction values (slippery or rough surfaces) Line of Sight Line of sight determines whether your bot can “see” or target the opponent bot.\ngraph LR subgraph LineOfSight[\"Line of Sight\"] A[\"Bot A\u003cbr/\u003eAttacker\"] B[\"Bot B\u003cbr/\u003eTarget\"] C[\"Bot C\u003cbr/\u003eBlocking\"] A -.Clear line of sight.-\u003e B A -.Blocked by Bot C.-\u003e C end style A fill:#4CAF50 style B fill:#2196F3 style C fill:#FF5722 Line of Sight Rules Direct path: Line of sight exists if an unobstructed straight line can be drawn between bot centers Obstacle blocking: Other bots block line of sight if the line passes through their circular area Walls don’t block: Boundary walls do not block line of sight (though weapons may have their own rules) Line of Sight Calculation To determine if Bot A has line of sight to Bot B:\nDraw a line segment from A’s center to B’s center For each other bot C in the arena: Calculate the perpendicular distance from C’s center to the line segment If this distance is less than C’s radius, line of sight is blocked Movement Constraints No teleportation: Bots cannot instantly jump to new positions; all movement follows continuous paths Speed limits: Bots have maximum movement speeds (defined by bot characteristics and equipment) Collision blocking: Cannot move through other bots or walls Friction decay: Velocity naturally decreases without continuous thrust Summary The 1v1 battle arena is a 100×100 unit rectangular space with:\n2D Cartesian coordinates centered at origin (0, 0) Bounded by walls at x=±50 and y=±50 Bots as circular footprints with 2-unit radius Collision detection preventing overlap Friction (μ=0.1) causing velocity decay Line of sight for targeting and detection Understanding these spatial mechanics is essential for implementing effective bot movement, positioning, and targeting logic.\n","categories":"","description":"The 2D battle space - coordinates, boundaries, collision detection, and physics\n","excerpt":"The 2D battle space - coordinates, boundaries, collision detection, …","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/arena/","tags":"","title":"Arena"},{"body":"Welcome to the Battle Bots gameplay documentation! This section provides comprehensive mechanics guides for different battle types available in Battle Bots.\nEach battle type has its own self-contained documentation covering the specific rules, mechanics, and systems that apply to that mode. This organization allows each battle type to have unique mechanics while maintaining clear, focused documentation.\nAvailable Battle Types 1v1 Battles Two bots face off in a direct confrontation. Learn about the arena, bot characteristics, equipment systems, and available actions for 1v1 combat.\nWhat is Battle Bots? Battle Bots is a PVP game platform where you implement autonomous bots that battle each other in real-time. Each bot is independent software that reacts to game state and performs actions in a battle space. Your bot’s success depends on your programming skills and the algorithms you implement.\nGetting Started To start building your bot:\nChoose a battle type to learn about (start with 1v1 Battles) Read the mechanics documentation for that battle type Check out the bot SDK for your preferred programming language Implement your bot’s logic and decision-making Test your bot in battles and iterate on your approach Battle Bots is a programming challenge - the documentation explains the mechanics, but figuring out effective strategies is up to you!\n","categories":"","description":"Documentation for different battle types and game mechanics\n","excerpt":"Documentation for different battle types and game mechanics\n","ref":"/battlebots/pr-preview/pr-153/gameplay/","tags":"","title":"Gameplay"},{"body":"Architecture Decision Records (ADRs) This section contains architectural decision records that document the key design choices made for the Battlebots platform. Each ADR follows the MADR 4.0.0 format and includes:\nContext and problem statement Decision drivers and constraints Considered options with pros and cons Decision outcome and rationale Consequences (positive and negative) Confirmation methods ADR Categories ADRs are classified into three categories:\nStrategic - High-level architectural decisions affecting the entire system (frameworks, authentication strategies, cross-cutting patterns). Use for foundational technology choices. User Journey - Decisions solving specific user journey problems. More tactical than strategic, but still architectural. Use when evaluating approaches to implement user-facing features. API Design - API endpoint implementation decisions (pagination, filtering, bulk operations). Use for significant API design trade-offs that warrant documentation. Status Values Each ADR has a status that reflects its current state:\nproposed - Decision is under consideration accepted - Decision has been approved and should be implemented rejected - Decision was considered but not approved deprecated - Decision is no longer relevant or has been superseded superseded by ADR-XXXX - Decision has been replaced by a newer ADR These records provide historical context for architectural decisions and help ensure consistency across the platform.\n","categories":"","description":"Documentation of architectural decisions made in the Battlebots platform using MADR 4.0.0 standard\n","excerpt":"Documentation of architectural decisions made in the Battlebots …","ref":"/battlebots/pr-preview/pr-153/research_and_development/adrs/","tags":"","title":"Architecture Decision Records"},{"body":"Your bot’s capabilities are defined by three core characteristics: Health, Defense, and Mass. These stats determine your survivability, damage mitigation, and movement properties.\nHealth Health (HP) is your bot’s survivability pool - the total amount of damage your bot can sustain before being eliminated.\nKey Properties HP Pool: Total damage your bot can take before destruction Range: 100-500 HP (placeholder values, subject to balance tuning) Destruction: Bot is eliminated when Health reaches 0 No Regeneration: Health does not regenerate during battle (current design) Gameplay Impact Higher Health allows your bot to stay in battles longer Low-Health bots must rely on damage avoidance through mobility or defensive actions Health works multiplicatively with Defense to create Effective HP (see below) Defense Defense represents your bot’s ability to mitigate incoming damage. It reduces the effective damage from enemy attacks.\nKey Properties Damage Reduction: Reduces incoming damage by a percentage or flat amount Range: 1-10 Defense (placeholder values, subject to balance tuning) Applies to All Damage: Affects all incoming damage sources (current design) No Evasion: Defense reduces damage taken, not hit chance Gameplay Impact Each point of Defense makes your Health more valuable Defense and Health combine multiplicatively to increase effective survivability Higher Defense enables sustained engagements Effective HP Health and Defense work together to determine your true survivability:\nflowchart LR A[\"Health: 100\"] --\u003e C[\"Effective HP: 150\"] B[\"Defense: 50% reduction\"] --\u003e C C --\u003e D[\"Takes 150 damage\u003cbr/\u003eto eliminate\"] style C fill:#4CAF50 Effective HP Formula Effective HP = Health × (1 + Defense modifier) Example: A bot with 100 Health and 50% Defense has 150 Effective HP - it takes 150 points of damage to destroy.\nWhy This Matters Balanced allocation of Health and Defense is more effective than stacking one stat Example: 100 Health + 50% Defense (150 Effective HP) provides better burst damage resistance than 150 Health + 0% Defense Mass Mass represents your bot’s total weight, determined by your equipment loadout. Mass is not directly allocated - it’s calculated from your equipped items.\nKey Properties Equipment-Derived: Total Mass = Base Mass + Equipment Mass Dynamic Value: Changes based on equipped weapons, armor, and modules Movement Impact: Higher Mass reduces acceleration (more force needed to overcome inertia and friction) No Direct Damage Scaling: Mass affects mobility, not offensive capability flowchart TD A[\"Choose Equipment\"] --\u003e B[\"Light Armor:\u003cbr/\u003eMinimal Mass\"] A --\u003e C[\"Heavy Armor:\u003cbr/\u003eHigh Mass\"] B --\u003e D[\"Faster Acceleration\u003cbr/\u003eMore Responsive\"] C --\u003e E[\"Slower Acceleration\u003cbr/\u003eLess Maneuverable\"] style A fill:#2196F3 style B fill:#4CAF50 style C fill:#FF5722 style D fill:#8BC34A style E fill:#FF9800 Gameplay Impact Heavy equipment (powerful weapons, heavy armor) increases Mass, reducing mobility Light equipment maintains mobility but sacrifices offensive/defensive power Mass cannot be optimized independently - it’s a consequence of equipment choices Higher Mass requires sustained thrust to overcome friction and maintain velocity Equipment Examples Weapons: Heavy weapons (high Mass) vs. light weapons (low Mass) Armor: Heavy plating (high Mass) vs. light armor (low Mass) Each equipment choice contributes to your total Mass profile Stat Interactions Bot characteristics don’t operate in isolation - they combine to create complex gameplay dynamics.\nEffective Durability Health and Defense multiply together to determine true survivability:\nFormula: Effective HP = Health × (1 + Defense modifier) Optimization: Balanced allocation is more efficient than single-stat stacking Example: 100 Health + 50% Defense (150 Effective HP) is more effective against burst damage than 150 Health + 0% Defense Mass and Mobility Mass directly impacts movement capability:\nThrust Relationship: Acceleration = Thrust Force / Mass Friction Impact: Higher Mass requires more sustained thrust to overcome arena friction (μ=0.1) Equipment Tradeoff: Heavy equipment increases power but reduces positioning flexibility Survivability Tradeoffs Defensive investment creates build choices:\nHigh Health + Low Defense: Vulnerable to sustained damage Low Health + High Defense: Vulnerable to burst damage High Mass: Defense from heavy armor reduces mobility, increasing thrust requirements Summary Bot characteristics create a three-stat system:\nHealth: Your survivability pool (how much damage you can take) Defense: Damage mitigation (makes each Health point more valuable) Mass: Equipment weight (affects acceleration and mobility) These stats interact to create diverse bot profiles. Your equipment choices (covered in Equipment) determine your final stat allocation and Mass.\nUnderstanding these characteristics is essential for configuring effective bot loadouts and implementing smart combat logic.\n","categories":"","description":"Health, Defense, and Mass stats that define your bot's capabilities\n","excerpt":"Health, Defense, and Mass stats that define your bot's capabilities\n","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/bot-characteristics/","tags":"","title":"Bot Characteristics"},{"body":" Subject to Change The equipment system described here is currently in the PROPOSED stage and may change based on playtesting and balance analysis. Check back for updates! Before battle, you configure your bot’s loadout by selecting equipment. Equipment modifies your bot’s characteristics and determines which actions are available during battle.\nLoadout Slots Each bot equips:\n1 Weapon - Determines available combat actions 1 Armor - Modifies Defense and Speed characteristics All equipment contributes to your bot’s total Mass, which affects movement acceleration.\nWeapons Weapons enable combat actions and determine offensive capabilities. Each weapon provides unique attack actions with different energy costs, damage patterns, and range characteristics.\nRifle Standard precision weapon enabling reliable ranged attacks.\nStat Effects:\nNo stat modifications (baseline weapon) Mass Contribution: TBD (baseline weapon mass) Characteristics:\nEnables: RifleShot action Damage: Moderate (TBD) Range: Long (TBD) Energy Cost: 15 per shot (TBD) Cooldown: 1 tick (TBD) Profile: Versatile baseline option suitable for various loadouts. No stat penalties, maintains mobility. Effective at medium to long range with consistent damage output.\nShotgun Close-range weapon enabling devastating burst damage with distance-based damage falloff.\nStat Effects:\nSpeed Penalty: -1 (TBD) Range Penalty: -1 (TBD) Mass Contribution: TBD (higher than Rifle) Characteristics:\nEnables: ShotgunBlast action Damage: High at close range, decreases with distance (TBD) Range: Short to medium (TBD) Energy Cost: 20 per shot (TBD) Cooldown: 2 ticks (TBD) Profile: High burst damage at close range. Weight penalty reduces mobility. Requires positioning to maximize effectiveness. Ineffective at long range due to damage falloff.\nArmor Armor provides defensive bonuses and damage mitigation. Armor modifies Defense and Speed characteristics, creating tradeoffs between survivability and mobility. All armor contributes to bot Mass.\nLight Armor Minimal protection that maintains mobility.\nStat Effects:\nDefense: +1 (TBD) Speed: No penalty (TBD) Mass Contribution: TBD (minimal) Profile: Minimal defense bonus with no speed penalty. Preserves mobility for positioning-focused loadouts. Relies on movement rather than damage absorption.\nMedium Armor Balanced protection with moderate defensive bonus and minor speed penalty.\nStat Effects:\nDefense: +2 (TBD) Speed: -1 (TBD) Mass Contribution: TBD (moderate) Profile: Reasonable defense without severe mobility cost. Versatile option for balanced loadouts. Moderate survivability increase with manageable speed reduction.\nHeavy Armor Maximum protection with significant defensive bonus and substantial speed penalty.\nStat Effects:\nDefense: +3 (TBD) Speed: -2 (TBD) Mass Contribution: TBD (high) Profile: Maximum damage reduction. Significant speed penalty limits mobility. Enables sustained engagements and damage absorption. Requires positional awareness due to low mobility.\nEquipment Combinations Here are example equipment combinations showing how weapons and armor create different bot profiles. These are presented as factual loadout options, not as recommendations.\ngraph TD subgraph \"Equipment Loadout Options\" A[\"Rifle + Light Armor\"] --\u003e A1[\"Moderate offense\u003cbr/\u003eLow defense\u003cbr/\u003eHigh mobility\u003cbr/\u003eLow Mass\"] B[\"Rifle + Medium Armor\"] --\u003e B1[\"Moderate offense\u003cbr/\u003eModerate defense\u003cbr/\u003eAverage mobility\u003cbr/\u003eModerate Mass\"] C[\"Rifle + Heavy Armor\"] --\u003e C1[\"Moderate offense\u003cbr/\u003eHigh defense\u003cbr/\u003eLow mobility\u003cbr/\u003eHigh Mass\"] D[\"Shotgun + Light Armor\"] --\u003e D1[\"High close-range offense\u003cbr/\u003eLow defense\u003cbr/\u003eGood mobility (with penalty)\u003cbr/\u003eModerate Mass\"] end style A fill:#4CAF50 style B fill:#2196F3 style C fill:#FF5722 style D fill:#FFC107 Rifle + Light Armor Stat Profile:\nOffense: Moderate (long-range precision) Defense: Low (+1 from Light Armor) Mobility: High (no speed penalties) Mass: Low (minimal equipment weight) Characteristics: Maintains mobility with consistent ranged damage. No speed penalties. Low defensive capability. Relies on positioning and movement.\nRifle + Medium Armor Stat Profile:\nOffense: Moderate (long-range precision) Defense: Moderate (+2 from Medium Armor) Mobility: Average (-1 from Medium Armor) Mass: Moderate Characteristics: Balanced offensive and defensive capabilities. Slight speed reduction. Versatile loadout with no extreme strengths or weaknesses.\nRifle + Heavy Armor Stat Profile:\nOffense: Moderate (long-range precision) Defense: High (+3 from Heavy Armor) Mobility: Low (-2 from Heavy Armor) Mass: High Characteristics: High damage reduction with sustained survivability. Significant mobility penalty. Reliable ranged offense. Positioning is critical due to low mobility.\nShotgun + Light Armor Stat Profile:\nOffense: High at close range (burst damage) Defense: Low (+1 from Light Armor) Mobility: Good (Shotgun -1 penalty, no armor penalty) Mass: Moderate (Shotgun has higher mass than Rifle) Characteristics: High burst damage potential at close range. Reduced effectiveness at distance. Relatively mobile despite shotgun weight. Low survivability requires damage avoidance.\nEquipment and Actions Equipment determines which actions are available:\nRifle: Enables RifleShot action Shotgun: Enables ShotgunBlast action Universal Actions: Move, Block, Evade, Shield, Scan, Charge are available regardless of equipment See the Actions page for complete action details.\nEquipment and Mass All equipment contributes to your bot’s total Mass:\nTotal Mass = Base Mass + Weapon Mass + Armor Mass Higher Mass reduces acceleration (you need more force to move the same speed). This creates a natural tradeoff where powerful equipment inherently reduces mobility.\nExample Mass Impacts:\nLight loadout (Rifle + Light Armor): Fast acceleration, responsive movement Heavy loadout (Shotgun + Heavy Armor): Slow acceleration, requires sustained thrust to overcome friction Mass calculations and specific values are TBD pending balance testing.\nConfiguration Equipment is configured before battle begins:\nloadout = { \"weapon\": \"Rifle\", # or \"Shotgun\" \"armor\": \"Light Armor\" # or \"Medium Armor\" or \"Heavy Armor\" } Equipment cannot be changed during battle.\nSummary The equipment system provides:\n2 Weapon Options: Rifle (baseline precision), Shotgun (close-range burst) 3 Armor Options: Light (mobility), Medium (balanced), Heavy (survivability) 4 Example Combinations: Demonstrating different stat profiles Mass Tradeoffs: Heavy equipment reduces mobility through increased Mass Action Enablement: Equipment determines available combat actions Equipment choices define your bot’s capabilities before battle. Understanding these tradeoffs is essential for effective loadout configuration.\n","categories":"","description":"Weapons and armor that customize your bot's capabilities and enable different actions\n","excerpt":"Weapons and armor that customize your bot's capabilities and enable …","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/equipment/","tags":"","title":"Equipment"},{"body":" Subject to Change The action system described here is currently in the PROPOSED stage and may change based on playtesting and balance analysis. Energy costs, cooldowns, and action availability may be adjusted! During battle, your bot performs actions each tick. Actions allow your bot to move, attack, defend, and gather information. All actions are constrained by a dual-resource system: energy costs and cooldowns.\nResource Management System Actions are governed by two independent constraint systems that work together to prevent action spam while maintaining gameplay fluidity.\nEnergy Pool Capacity: Limited pool of energy (TBD, can be modified by equipment) Regeneration: Energy regenerates over time at a fixed rate (TBD) Consumption: All actions consume energy Constraint: Insufficient energy prevents action execution Cooldown System Time-Based: Actions have cooldown periods measured in game ticks Independent: Cooldowns are separate from energy (both must be satisfied) Prevents Re-Use: Actions cannot be used again until cooldown expires Varies by Action: Powerful actions have longer cooldowns Dual-Constraint System To perform an action, both conditions must be met:\nSufficient energy available Action cooldown has expired flowchart TD A[\"Bot wants to\u003cbr/\u003eperform action\"] --\u003e B{Energy\u003cbr/\u003eavailable?} B --\u003e|No| F[\"Action Rejected\"] B --\u003e|Yes| C{Cooldown\u003cbr/\u003eready?} C --\u003e|No| F C --\u003e|Yes| D{Equipment\u003cbr/\u003erequirement met?} D --\u003e|No| F D --\u003e|Yes| E[\"Action Executed\"] style E fill:#4CAF50 style F fill:#F44336 Why Both?\nEnergy creates resource management decisions Cooldowns prevent burst spam even with high energy Basic actions (Move) have low cost and no cooldown for fluid movement Powerful actions (Shield) have high cost and long cooldown for balance Action Categories Actions are organized into four categories:\nMovement - Navigate the arena Combat - Deal damage to opponents Defensive - Mitigate or avoid damage Utility - Information gathering and state modification Movement Actions Move Moves your bot in a specified direction within the battle space.\nCosts and Constraints:\nEnergy Cost: 5 (TBD) Cooldown: 0 ticks (TBD) Equipment Required: None (universal action) Parameters:\nDirection vector or target coordinates Constraints:\nCannot move through obstacles or other bots (collision detection applies) Movement affected by arena friction (velocity decays without continuous thrust) Position clamped to arena boundaries Description: Move is the most fundamental action, enabling positioning, pursuit, evasion, and range control. Zero cooldown and low energy cost make movement fluid and responsive.\nCombat Actions Combat actions deal damage to opponent bots. Available combat actions depend on your equipped weapon.\nRifleShot Single-shot, precise ranged attack.\nCosts and Constraints:\nEnergy Cost: 15 (TBD) Cooldown: 1 tick (TBD) Equipment Required: Rifle Parameters:\nTarget position or bot Effects:\nDamage: Moderate (TBD) Range: Long (TBD) Constraints:\nRequires line of sight to target Rifle must be equipped Description: Consistent ranged damage with moderate energy cost and short cooldown. Reliable baseline offense for Rifle-equipped bots.\nShotgunBlast Spray of projectiles effective at close range with distance-based damage falloff.\nsequenceDiagram participant Bot as Your Bot participant Server as Battle Server participant Opponent as Opponent Bot Bot-\u003e\u003eServer: Submit RifleShot action Server-\u003e\u003eServer: Validate energy (15 available?) Server-\u003e\u003eServer: Validate cooldown (ready?) Server-\u003e\u003eServer: Validate equipment (Rifle equipped?) Server-\u003e\u003eServer: Validate line of sight Server-\u003e\u003eOpponent: Apply damage Server-\u003e\u003eBot: Action result + updated state Server-\u003e\u003eOpponent: Damage notification + updated state Costs and Constraints:\nEnergy Cost: 20 (TBD) Cooldown: 2 ticks (TBD) Equipment Required: Shotgun Parameters:\nTarget position or bot Effects:\nDamage: High at close range, decreases with distance (TBD) Range: Short to medium (TBD) Constraints:\nShotgun must be equipped Damage falloff reduces effectiveness at range Description: High burst damage at close range. Longer cooldown and higher energy cost than RifleShot. Positioning is critical to maximize damage output.\nDefensive Actions Defensive actions reduce or avoid incoming damage. These are universal actions available regardless of equipment.\nBlock Reduces damage from incoming attacks for one tick.\nCosts and Constraints:\nEnergy Cost: 10 (TBD) Cooldown: 2 ticks (TBD) Equipment Required: None (universal) Effects:\nDamage Reduction: X% of incoming damage (TBD) Duration: Active for current tick only Constraints:\nMust be activated before damage is received (prediction required) Description: Reduces incoming damage by a percentage. Active for only one tick, requiring anticipation of opponent actions.\nEvade Attempts to dodge incoming attacks with a chance-based mechanism.\nCosts and Constraints:\nEnergy Cost: 15 (TBD) Cooldown: Variable (TBD) Equipment Required: None (universal) Effects:\nSuccess Chance: TBD (may depend on bot Speed characteristic) Duration: Active for current tick only Constraints:\nSuccess is not guaranteed (chance-based) Higher energy cost than Block Description: Chance to completely avoid an attack. Higher risk than Block but potentially higher reward. Success rate may be influenced by bot characteristics.\nShield Activates energy shield that absorbs damage over multiple ticks.\nCosts and Constraints:\nEnergy Cost: 20 initial + ongoing drain per tick (TBD) Cooldown: Variable (TBD) Equipment Required: None (universal) Effects:\nDamage Absorption: Up to threshold (TBD) Duration: Sustained across multiple ticks with ongoing energy drain Constraints:\nOngoing energy cost while active may limit other actions Most expensive defensive action Description: Extended damage mitigation over multiple ticks. High initial and ongoing energy cost. Creates temporary window of increased survivability.\nUtility Actions Utility actions provide information gathering and state modification capabilities.\nScan Gathers information about environment and nearby bots.\nCosts and Constraints:\nEnergy Cost: 5 (TBD) Cooldown: Variable (TBD) Equipment Required: None (recommended: Sensor Array module for enhanced range) Effects:\nReturns information about nearby entities (positions, health, etc.) Detection Range: Limited radius, enhanced by Sensor Array equipment (TBD) Constraints:\nOnly reveals information within detection range Description: Information gathering about nearby bots and environment. Low energy cost. Detection range may be enhanced by optional equipment modules.\nCharge Increases energy regeneration rate temporarily.\nCosts and Constraints:\nEnergy Cost: Variable (may cost initial energy or pause regen - TBD) Cooldown: Variable (TBD) Equipment Required: None (universal) Effects:\nBoosts energy regeneration for duration (TBD) Duration: Multiple ticks (TBD) Constraints:\nBot may be vulnerable while charging (cannot perform other actions - TBD) Description: Enhances energy economy by boosting regeneration. Tradeoff between immediate action capability and future energy availability.\nEquipment-Dependent Actions Some actions require specific equipment modules. These are optional enhancements beyond the base weapon and armor system.\nBoost Temporary speed increase for repositioning.\nCosts and Constraints:\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Boost Engine module Effects:\nIncreases Speed temporarily (TBD) Description: Rapid repositioning capability. Requires optional Boost Engine equipment module.\nRepair Limited self-repair during combat.\nCosts and Constraints:\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Repair Kit module Effects:\nRestores HP (amount and usage limits TBD) Description: Self-healing capability. Requires optional Repair Kit equipment module.\nCloak Reduces detection range by enemies.\nCosts and Constraints:\nEnergy Cost: Variable (TBD) Cooldown: Variable (TBD) Equipment Required: Stealth Module Effects:\nReduces enemy detection range temporarily (TBD) Description: Stealth capability. Requires optional Stealth Module equipment.\nAction Execution and Processing Action Submission Bots submit actions via the SDK each tick Actions are queued and processed during the next game tick Multiple parallel actions may be possible (TBD) Action Validation The server validates three conditions before executing actions:\nEnergy Available: Sufficient energy in pool? Cooldown Ready: Action cooldown has expired? Equipment Requirement: Required equipment equipped? If any validation fails, the action is rejected with error feedback.\nAction Resolution Actions are resolved during the game tick cycle Action outcomes are broadcast to all bots State changes (damage, position, energy) are updated and synchronized Energy and Cooldown Management graph LR subgraph \"Energy System\" E1[\"Energy Pool: 100\"] --\u003e E2[\"Use Action: -15\"] E2 --\u003e E3[\"Remaining: 85\"] E3 --\u003e E4[\"Regenerate: +5/tick\"] end subgraph \"Cooldown System\" C1[\"Action Used: Tick 10\"] --\u003e C2[\"Cooldown: 2 ticks\"] C2 --\u003e C3[\"Available Again: Tick 12\"] end style E1 fill:#4CAF50 style E3 fill:#FFC107 style C1 fill:#2196F3 style C3 fill:#4CAF50 Managing Energy Monitor your current energy level Plan action sequences within energy budget Balance high-cost actions (combat, defense) with energy regeneration Consider Charge action for energy recovery Managing Cooldowns Track cooldowns for critical actions (combat, defense) Don’t rely on repeated use of actions with long cooldowns Plan alternative actions while waiting for cooldowns Dual-Constraint Strategy The dual-constraint system creates decision-making:\nCan’t spam powerful actions even with energy (cooldowns prevent) Can’t spam actions rapidly without energy (energy limits) Basic actions (Move) remain fluid with low cost and no cooldown Action Reference Table Action Category Energy Cooldown Equipment Description Move Movement 5 0 None Reposition in arena RifleShot Combat 15 1 tick Rifle Moderate damage, long range ShotgunBlast Combat 20 2 ticks Shotgun High damage close range Block Defensive 10 2 ticks None Reduce incoming damage Evade Defensive 15 Variable None Chance to avoid attack Shield Defensive 20 + drain Variable None Multi-tick damage absorption Scan Utility 5 Variable None Gather information Charge Utility Variable Variable None Boost energy regen All values are TBD and subject to balance adjustments.\nSummary The action system provides:\nDual-Constraint Resources: Energy + cooldowns govern action usage 4 Action Categories: Movement, Combat, Defensive, Utility Universal Actions: Available to all bots regardless of equipment Equipment-Dependent Actions: Enabled by specific equipment choices Fluid Movement: Move action has low cost and no cooldown Balanced Power: Powerful actions have high costs and long cooldowns Understanding action costs, cooldowns, and equipment requirements is essential for implementing effective bot logic and resource management.\n","categories":"","description":"All available actions, resource management, energy costs, and cooldowns\n","excerpt":"All available actions, resource management, energy costs, and …","ref":"/battlebots/pr-preview/pr-153/gameplay/1v1-battles/actions/","tags":"","title":"Actions"},{"body":"This section contains detailed technical analysis of communication protocol options evaluated for the Battle Bots bot-to-server and bot-to-bot interface.\nProtocol Evaluations gRPC - HTTP/2-based RPC with Protocol Buffers and bidirectional streaming HTTP-based Protocols - REST, WebSockets, and Server-Sent Events analysis Custom TCP/UDP - Low-level custom protocol evaluation These analyses inform ADR-0004: Bot to Battle Server Communication Protocol.\n","categories":"","description":"Analysis of communication protocols for Battle Bots bot-to-server interface\n","excerpt":"Analysis of communication protocols for Battle Bots bot-to-server …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/protocols/","tags":"","title":"Protocols"},{"body":"Welcome! Battle Bots is a game in which you, the human, implement an autonomous “bot” to do battle with “bots” implemented by other humans.\nWhat is a Bot? A bot is a independent piece of software which is programmed to battle other bots by reacting to state updates (e.g. bot B moved to point A) and performing its own actions (e.g. fire missile at point A).\n","categories":"","description":"Battle Bots is a PVP game for autonomous players","excerpt":"Battle Bots is a PVP game for autonomous players","ref":"/battlebots/pr-preview/pr-153/","tags":"","title":"Battle Bots"},{"body":"R\u0026D Process The Research \u0026 Design process follows a structured workflow to ensure comprehensive analysis and documentation of user experiences, technical solutions, and implementation details.\nProcess Steps Document the User Journey\nCreate a user journey document for the specific user experience Include flow diagrams using Mermaid to visualize user interactions Define prioritized technical requirements (P0/P1/P2) Use the /new-user-journey command to create standardized documentation Design the Solution\nCreate an ADR that designs a solution to implement the user journey Identify and document: Additional ADRs needed for specific components APIs that need to be defined User interface flows (mobile, web, etc.) Data flow from user to end systems (database, notification system, etc.) Capture the complete system architecture and integration points Document Component ADRs\nCreate ADRs for specific technical components identified in the solution design Examples: authentication strategy, session management, account linking, data storage Use the /new-adr command to create standardized MADR 4.0.0 format documents Document technical decisions with context, considered options, and consequences Document Required APIs\nFor each API endpoint identified in the solution, create comprehensive API documentation Use the /new-api-doc command to create standardized documentation Include: Request/response schemas Authentication requirements Business logic flows (Mermaid diagrams) Error responses and status codes Example curl requests Document API Implementation\nFor each documented API, create an ADR describing the implementation approach Document technical decisions including: Programming language selection Framework and libraries Architecture patterns Testing strategy Example: ADR-0006 documents the tech stack for API development (z5labs/humus framework) Design User Interface\nCreate UI/UX designs for the user journey Ensure designs align with the documented user flows and API contracts Consider platform-specific requirements (mobile, web, desktop) Documentation Structure The R\u0026D documentation is organized into the following sections:\nUser Journeys - User experience flows with technical requirements ADRs - Architectural Decision Records documenting technical decisions APIs - REST API endpoint documentation with schemas and examples Analysis - Research and analysis of technologies and solutions ","categories":"","description":"","excerpt":"R\u0026D Process The Research \u0026 Design process follows a structured …","ref":"/battlebots/pr-preview/pr-153/research_and_development/","tags":"","title":"Research \u0026 Design"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-153/categories/","tags":"","title":"Categories"},{"body":"Overview Grafana Loki is a horizontally scalable, highly available, multi-tenant log aggregation system inspired by Prometheus. Unlike other log aggregation systems, Loki is designed to be cost-effective and easy to operate by not indexing the contents of logs, but rather a set of labels for each log stream.\nThis analysis explores Loki as a potential log storage backend for the BattleBots platform, focusing on its architecture, deployment options, OTLP compatibility, and integration with the OpenTelemetry Collector.\nWhy Research Loki? For the BattleBots observability stack, Loki offers several compelling advantages:\nNative OTLP Support: Loki v3+ provides native OTLP ingestion endpoints, enabling seamless integration with the OpenTelemetry Collector Cost-Effective Storage: Index-free approach dramatically reduces storage costs compared to full-text indexing systems Horizontal Scalability: Microservices architecture supports scaling from development to production workloads Grafana Integration: Tight integration with Grafana provides unified visualization of logs, metrics, and traces Cloud-Native Design: Built for containerized environments with Kubernetes-first deployment patterns Document Structure The Loki analysis is organized into the following documents:\nLoki Overview Comprehensive overview covering architecture, deployment, and operational best practices.\nTopics covered:\nWhat is Loki and its design philosophy (index-free, label-based querying) Core concepts: streams, labels, chunks, index, LogQL Architecture components: distributor, ingester, querier, compactor Deployment modes: monolithic, simple scalable, microservices How to run Loki with Docker/Podman Compose for POC Best practices for label strategy and configuration When to use Loki vs. alternatives like Elasticsearch Audience: Everyone—provides foundational understanding for evaluating Loki as a log backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration.\nTopics covered:\nNative OTLP support in Loki v3+ (endpoints, configuration) OTel Collector otlphttp exporter setup Resource attribute mapping to Loki labels Log-trace correlation via TraceID/SpanID Complete working configuration examples Authentication and multi-tenancy Troubleshooting common integration issues Audience: Developers and operators implementing the OTel Collector to Loki pipeline.\nBattleBots Integration Context For the BattleBots platform, Loki would serve as the centralized log storage backend, receiving logs from the OpenTelemetry Collector via OTLP. This enables:\nGame Event Logging Battle events (bot actions, damage calculations, victory conditions) Game state transitions and timing information Player actions and command processing Error conditions and system anomalies Infrastructure Logging Container logs from game servers System logs from host infrastructure Application logs from Go services Network and security logs Unified Observability Log-trace correlation through shared TraceID/SpanID Linking log events to metrics and distributed traces Grafana dashboards combining logs, metrics, and traces Streamlined debugging workflows across all telemetry signals Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the log storage backend for BattleBots. Key decision factors include:\nFunctional fit: Does Loki meet log storage, query, and correlation requirements? Operational complexity: How difficult is it to deploy, monitor, and maintain Loki? Cost: What are the infrastructure and operational costs at POC and production scale? Integration: How well does it integrate with OTel Collector, Grafana, and the broader observability stack? External Resources Grafana Loki Official Documentation Loki GitHub Repository Grafana Labs Blog CNCF Loki Project ","categories":"","description":"Research and analysis of Grafana Loki log aggregation system for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Loki log aggregation system for the …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/logs/loki/","tags":"","title":"Grafana Loki"},{"body":"Overview Grafana Mimir is a horizontally scalable, highly available, multi-tenant, long-term storage solution for Prometheus metrics. It transforms Prometheus’s single-server architecture into a distributed microservices platform capable of handling over 1 billion active time series with unlimited retention.\nThis analysis explores Mimir as the metrics storage backend for the BattleBots platform, focusing on its architecture, native OTLP support, OpenTelemetry Collector integration, and operational characteristics.\nWhy Research Mimir? For the BattleBots observability stack, Mimir offers several compelling advantages over standalone Prometheus:\nNative OTLP Support: Direct ingestion via /otlp/v1/metrics endpoint since version 2.3.0, enabling seamless OpenTelemetry Collector integration without protocol translation Massive Scalability: Proven at 1 billion active time series (Grafana Labs internal testing) and 500 million series in production customer deployments Long-Term Storage: Object storage backend (S3, GCS, MinIO) enables months to years of retention at minimal cost compared to local disk storage Multi-Tenancy: Built-in tenant isolation with per-tenant limits, enabling future use cases like per-player metrics or per-battle analytics High Availability: 3x replication by default, distributed architecture, and automatic failover eliminate single points of failure Prometheus Compatibility: Full PromQL support ensures existing Prometheus queries, dashboards, and alerts work unchanged Document Structure The Mimir analysis is organized into the following documents:\nMimir Overview Comprehensive overview covering architecture, deployment modes, storage backends, and operational best practices.\nTopics covered:\nWhat is Mimir and its design philosophy (distributed Prometheus with object storage) Core concepts: blocks storage, time series, multi-tenancy, cardinality management Architecture components: distributor, ingester, querier, query-frontend, store-gateway, compactor, ruler Deployment modes: monolithic, read-write, microservices (with comparison table) How to run Mimir with Docker Compose and MinIO for POC environments Production deployment patterns with Kubernetes and Helm Best practices for label strategy, configuration, storage selection, and performance tuning When to use Mimir vs. Prometheus vs. Thanos (decision criteria matrix) Resource requirements and capacity planning guidance Complete configuration examples for all deployment modes Audience: Everyone—provides foundational understanding for evaluating Mimir as the metrics backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration (addresses critical user requirements).\nTopics covered:\nNative OTLP support in Mimir (status, version requirements, endpoint configuration) OTLP vs. Prometheus remote write comparison and recommendations OpenTelemetry Collector otlphttp exporter configuration for Mimir Alternative: OpenTelemetry Collector prometheusremotewrite exporter setup Batch processor, retry policies, and queue management best practices Resource attribute mapping from OTel to Mimir labels Label strategy and cardinality control for OTel-generated metrics Authentication and multi-tenancy setup with X-Scope-OrgID headers Complete working configuration examples (OTel Collector + Mimir + Grafana) Troubleshooting common integration issues (connection errors, cardinality, performance) BattleBots-specific integration patterns and example queries Audience: Developers and operators implementing the OpenTelemetry Collector to Mimir pipeline.\nBattleBots Integration Context For the BattleBots platform, Mimir would serve as the centralized metrics storage backend, receiving metrics from the OpenTelemetry Collector via native OTLP ingestion. This enables:\nGame Metrics Storage Bot Performance: Action latency, damage calculations, movement speed, resource utilization per bot Battle Events: Start/end times, player actions, victory conditions, matchmaking metrics Game State: Active battles count, queued players, concurrent users, session durations Quality Metrics: Frame rates, tick rates, network latency, synchronization quality Infrastructure Metrics Container Metrics: CPU/memory usage, restart counts, health checks for bot containers and game servers Kubernetes Metrics: Pod status, node utilization, deployment health, scaling events Network Metrics: Request rates, latency distributions, error rates, bandwidth consumption Host Metrics: System-level CPU, memory, disk I/O, network traffic across infrastructure Observability Stack Metrics OpenTelemetry Collector: Pipeline throughput, batch sizes, queue depths, export success/failure rates Loki (Log Storage): Log ingestion rates, query latency, storage utilization Tempo (Trace Storage): Span ingestion, trace completeness, sampling rates Mimir Self-Monitoring: Ingester series counts, query performance, compaction status, object storage health Long-Term Analytics Capacity Planning: Historical resource usage trends to predict scaling needs Cost Optimization: Identify underutilized resources and optimize allocation Performance Baselines: Establish normal behavior patterns for anomaly detection Business Intelligence: Player engagement metrics, battle frequency, peak usage times Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the metrics storage backend for BattleBots. Key decision factors specific to Mimir include:\nScalability Requirements: Can Mimir handle expected growth from POC (thousands of series) to production (millions+ of series)? OTLP Integration: Does native OTLP support simplify the OpenTelemetry Collector integration compared to alternatives? Operational Complexity: Is the team prepared to operate a distributed metrics system, or should we start with standalone Prometheus? Cost vs. Value: Do Mimir’s features (long-term storage, scalability, multi-tenancy) justify the increased infrastructure cost and complexity? Migration Path: If starting with Prometheus, how difficult is migration to Mimir when scale demands it? The ADR will also consider alternative approaches:\nStandalone Prometheus: Simpler but limited to ~10M series and short retention Thanos: Similar capabilities to Mimir with different architectural trade-offs Managed Services: Grafana Cloud or other hosted Prometheus-compatible solutions External Resources Grafana Mimir Official Documentation Mimir GitHub Repository Grafana Mimir Blog Mimir Capacity Calculator OpenTelemetry Metrics to Mimir Guide Prometheus Remote Write Specification ","categories":"","description":"Research and analysis of Grafana Mimir metrics storage system for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Mimir metrics storage system for the …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/metrics/mimir/","tags":"","title":"Grafana Mimir"},{"body":"Overview Grafana Tempo is a high-volume, minimal dependency distributed tracing backend designed for cost-efficiency and operational simplicity. Unlike traditional tracing systems that require complex database infrastructure, Tempo uses object storage as its only dependency, dramatically reducing operational complexity while providing powerful trace querying through TraceQL.\nThis analysis explores Tempo as the distributed tracing backend for the BattleBots platform, focusing on its architecture, native OTLP support, OpenTelemetry Collector integration, and integration with the broader Grafana observability stack (Loki, Mimir, Prometheus).\nWhy Research Tempo? For the BattleBots observability stack, Tempo offers several compelling advantages:\nNative OTLP Support: Full support for OTLP/gRPC (port 4317) and OTLP/HTTP (port 4318) protocols, enabling seamless integration with the OpenTelemetry Collector Cost-Effective Storage: Object storage-only design (S3, GCS, Azure, MinIO) eliminates expensive database infrastructure and reduces storage costs by 10x or more compared to Jaeger or Zipkin Minimal Dependencies: No Cassandra, Elasticsearch, or other complex databases required—only object storage Horizontal Scalability: Microservices architecture supports scaling from development to production workloads handling millions of spans per second TraceQL Query Language: Powerful SQL-like query language for filtering and analyzing traces by span attributes, duration, and relationships Grafana Integration: Seamless correlation with metrics (Mimir/Prometheus) and logs (Loki) through exemplars and trace IDs for unified observability Multi-Protocol Support: Accepts OTLP, Jaeger, Zipkin, and OpenCensus formats, enabling gradual migration from legacy tracing systems Document Structure The Tempo analysis is organized into the following documents:\nTempo Overview Comprehensive overview covering architecture, deployment modes, and operational best practices.\nTopics covered:\nWhat is Tempo and its design philosophy (index-free, object storage-based tracing) Core concepts: traces, spans, blocks storage, TraceQL, sampling strategies Architecture components: distributor, ingester, querier, query-frontend, compactor, metrics-generator Deployment modes: monolithic, scalable, microservices (with comparison table) How to run Tempo with Docker Compose for POC environments Best practices for sampling, storage optimization, and performance tuning When to use Tempo vs. Jaeger vs. Zipkin (decision criteria matrix) Resource requirements and capacity planning guidance Complete configuration examples with Grafana data source setup Audience: Everyone—provides foundational understanding for evaluating Tempo as the tracing backend.\nOTLP Integration Deep dive into OTLP compatibility and OpenTelemetry Collector integration (addresses critical user requirements).\nTopics covered:\nNative OTLP support in Tempo (status: YES since v1.3.0+, endpoint configuration) OTLP/gRPC and OTLP/HTTP protocol support and when to use each OpenTelemetry Collector otlp and otlphttp exporter configuration for Tempo Batch processor, retry policies, and queue management best practices Resource attribute mapping and span attribute strategies Sampling configuration (head-based, tail-based) in the OTel Collector Authentication and multi-tenancy setup with X-Scope-OrgID headers Complete working configuration examples (OTel Collector + Tempo + Grafana) Troubleshooting common integration issues (connection errors, missing traces) BattleBots-specific integration patterns and TraceQL query examples Audience: Developers and operators implementing the OpenTelemetry Collector to Tempo pipeline.\nBattleBots Integration Context For the BattleBots platform, Tempo would serve as the centralized distributed tracing backend, receiving traces from the OpenTelemetry Collector via OTLP. This enables:\nBattle Workflow Tracing Complete Battle Flow: Trace entire battle lifecycle from matchmaking → battle initialization → game loop execution → victory condition → results persistence Bot Action Traces: Track individual bot actions (move, attack, defend) with timing, success/failure, and resource costs State Transitions: Capture game state changes with span events marking key transitions (battle start, bot death, timer expiration) Latency Analysis: Identify performance bottlenecks in action processing, state synchronization, and event broadcasting Request Flow Tracing API Requests: Trace HTTP/gRPC requests from client → API gateway → game service → persistence layer WebSocket Connections: Track WebSocket connection lifecycle, authentication, and message flow Service Interactions: Visualize calls between microservices (matchmaking service → game server manager → bot runtime) External Dependencies: Monitor calls to external services (authentication, leaderboards, analytics) Error Investigation Exception Tracking: Capture stack traces and error context through span events Failure Propagation: Trace error propagation across service boundaries to identify root causes Timeout Analysis: Identify services or operations exceeding latency SLAs Retry Patterns: Visualize retry behavior and backoff strategies Unified Observability Trace-to-Logs Correlation: Link traces to logs via TraceID/SpanID for detailed debugging context Trace-to-Metrics Correlation: Use exemplars to jump from metric spikes to example slow traces Grafana Dashboards: Combine traces, metrics (Mimir), and logs (Loki) in unified visualizations Root Cause Analysis: Start with metric alert → find exemplar trace → examine correlated logs Performance Optimization Latency Profiling: Identify slow database queries, external API calls, or computational bottlenecks Resource Utilization: Correlate trace duration with CPU/memory metrics for capacity planning Sampling Strategies: Implement tail sampling to capture all errors and slow requests while reducing trace volume TraceQL Analysis: Query for patterns like “all traces with database latency \u003e 100ms” or “errors in bot action processing” Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the distributed tracing backend for BattleBots. Key decision factors specific to Tempo include:\nCost Efficiency: Does Tempo’s object storage-only design provide sufficient cost savings to justify its adoption over traditional tracing backends? OTLP Integration: Does native OTLP support simplify the OpenTelemetry Collector integration compared to alternatives requiring protocol translation? Search Trade-offs: Is Tempo’s trace-ID-first query model (with TraceQL for advanced queries) sufficient for BattleBots debugging workflows? Operational Complexity: Can the team operate Tempo with minimal infrastructure overhead compared to Jaeger (Cassandra/Elasticsearch) or Zipkin? Stack Integration: Does Tempo’s deep integration with Grafana, Loki, and Mimir provide enough value to justify adopting the full LGTM stack? Scalability: Can Tempo handle growth from POC (thousands of traces) to production (potentially millions of traces per day)? The ADR will also consider alternative approaches:\nJaeger: More mature with powerful tag-based search, but requires Cassandra or Elasticsearch and higher operational complexity Zipkin: Simpler but less scalable, requires database backend, limited query capabilities Elastic APM: Unified observability in Elasticsearch stack, but higher cost and complexity Managed Services: Grafana Cloud Tempo, Honeycomb, or Lightstep for reduced operational burden Related Observability Components Tempo completes the three-signal observability architecture alongside:\nGrafana Loki - Log aggregation and storage Grafana Mimir - Metrics storage and querying OpenTelemetry Collector - Telemetry data pipeline Together, these components form the LGTM stack (Loki, Grafana, Tempo, Mimir), providing unified observability with trace-metric-log correlation through exemplars and shared trace IDs.\nExternal Resources Grafana Tempo Official Documentation Tempo GitHub Repository TraceQL Query Language Reference Grafana Tempo Blog OpenTelemetry to Tempo Integration Guide Tempo Architecture Deep Dive LGTM Stack Overview ","categories":"","description":"Research and analysis of Grafana Tempo distributed tracing backend for the BattleBots observability stack.\n","excerpt":"Research and analysis of Grafana Tempo distributed tracing backend for …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/traces/tempo/","tags":"","title":"Grafana Tempo"},{"body":"Overview This section contains research and analysis of log storage solutions for the BattleBots platform. Effective log storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Log Backend Options Grafana Loki Analysis of Grafana Loki, a horizontally scalable, multi-tenant log aggregation system optimized for storing and querying log data.\nLoki uses a unique index-free approach that indexes only metadata labels rather than full log content, significantly reducing storage and operational costs compared to traditional log aggregation systems.\nKey features include:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and how to run Loki OTLP compatibility and OTel Collector integration Best practices for running and operating Loki Future Analysis Additional log backend options may be researched based on BattleBots requirements:\nElasticsearch/OpenSearch - Full-text search capabilities Cloud-native options - AWS CloudWatch Logs, Google Cloud Logging Self-hosted alternatives - ClickHouse, Vector Related Documentation R\u0026D Documentation OpenTelemetry Collector Analysis - Log collection and processing Observability Analysis - Overall observability strategy Future ADR on observability stack selection External Resources Grafana Loki Documentation OpenTelemetry Documentation CNCF Observability Projects ","categories":"","description":"Research and analysis of log storage backends for the BattleBots observability stack.\n","excerpt":"Research and analysis of log storage backends for the BattleBots …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/logs/","tags":"","title":"Log Storage Analysis"},{"body":"Overview Grafana Loki introduced native OpenTelemetry Protocol (OTLP) support in version 3.0, marking a significant advancement in how logs can be ingested into Loki. This native integration allows applications instrumented with OpenTelemetry to send logs directly to Loki using the standardized OTLP format, eliminating the need for format transformations and simplifying the observability pipeline.\nThe native OTLP endpoint provides a fully OpenTelemetry-compliant ingestion path where logs sent in OTLP format are stored directly in Loki without requiring conversion to JSON or logfmt blobs. This approach leverages Loki’s structured metadata feature, which stores log attributes and other OpenTelemetry LogRecord fields separately from the log body itself. The result is a more intuitive query experience and better performance, as queries no longer need to parse JSON at runtime to access fields.\nFor the BattleBots observability stack, native OTLP integration offers several advantages: unified telemetry collection across logs, metrics, and traces through the OpenTelemetry Collector; simplified configuration compared to legacy exporters; better correlation between logs and traces through preserved TraceId and SpanId fields; and vendor portability, making it easier to migrate between observability backends without changing instrumentation.\nOTLP Support in Loki Answer: YES - Loki versions 3.0 and later natively support the OpenTelemetry Protocol (OTLP) for log ingestion.\nNative OTLP Endpoint Loki exposes an OTLP-compliant endpoint at /otlp that accepts OpenTelemetry log data. When clients send logs to this endpoint, the collector automatically appends the appropriate path suffix (/v1/logs), resulting in requests to /otlp/v1/logs.\nSupported Protocols:\nHTTP: POST requests using HTTP/1.1 or HTTP/2 gRPC: Unary RPC calls using the OTLP service definition Default Port:\nLoki typically runs on port 3100 for all HTTP endpoints, including OTLP Version Requirements Minimum Loki Version: 3.0 or later\nSchema Requirements:\nSchema version: v13 or higher (required for structured metadata) Index type: tsdb (Time Series Database index required) Structured metadata is essential for OTLP ingestion because it stores the OpenTelemetry LogRecord fields (resource attributes, instrumentation scope, log attributes) separately from the log body. Without schema v13 and tsdb, Loki cannot properly handle OTLP data.\nBenefits of Native OTLP vs Legacy Loki Exporter The legacy lokiexporter component in the OpenTelemetry Collector encoded logs as JSON or logfmt blobs with Loki-specific label conventions. The native OTLP endpoint provides several improvements:\nSimplified Querying: No JSON parsing required at query time. Instead of {job=\"dev/auth\"} | json | severity=\"INFO\", you can query directly: {service_name=\"auth\"} | severity_text=\"INFO\"\nCleaner Log Bodies: The log message is stored as-is rather than wrapped in a JSON structure. A log “user logged in” is stored exactly as that string, with metadata in structured fields.\nStandard Resource Labels: Uses OpenTelemetry semantic conventions (service_name, service_namespace) instead of custom labels (job=service.namespace/service.name)\nBetter Performance: Structured metadata allows efficient filtering without parsing the entire log body\nVendor Portability: Standard OTLP configuration works across multiple backends without Loki-specific hints\nFuture-Proof: The native endpoint represents Grafana’s strategic direction for log ingestion\nOTLP Endpoint Configuration To enable OTLP ingestion in Loki, you must configure structured metadata support and optionally customize which resource attributes become index labels.\nEnabling Structured Metadata Structured metadata is enabled by default in Loki 3.0+, but you should explicitly configure it in your limits:\nlimits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 # Maximum metadata entries per log record Schema Configuration Your Loki schema must use version 13 or higher with the tsdb index:\nschema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: s3 # or filesystem, gcs, azure, etc. schema: v13 index: prefix: loki_index_ period: 24h Complete Loki Configuration Example Here’s a complete Loki configuration with OTLP support enabled:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 per_stream_rate_limit: 5MB per_stream_rate_limit_burst: 15MB # OTLP-specific configuration otlp_config: resource_attributes: # Configure which resource attributes become index labels attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.cluster.name - k8s.namespace.name - cloud.region - cloud.provider # Convert high-cardinality attributes to structured metadata - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 OTel Collector Export Configuration Answer: YES - The OpenTelemetry Collector can export logs to Loki using the otlphttp exporter.\nBasic OTLP HTTP Exporter The recommended exporter for Loki is otlphttp/logs:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true # For local development without TLS Important: Do not append /v1/logs to the endpoint URL. The OTLP exporter automatically adds the appropriate path suffix.\nComplete Exporter Configuration Here’s a production-ready configuration with retry, timeout, and queue settings:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" # TLS Configuration tls: insecure: false ca_file: /etc/ssl/certs/loki-ca.crt cert_file: /etc/ssl/certs/client.crt key_file: /etc/ssl/private/client.key min_version: \"1.2\" # Timeout for individual requests (default: 30s recommended) timeout: 30s # Retry configuration retry_on_failure: enabled: true initial_interval: 5s # Time to wait before first retry max_interval: 30s # Maximum backoff interval max_elapsed_time: 300s # Give up after 5 minutes # Queue configuration for reliability sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage # Reference to file_storage extension for persistence # Compression (gzip recommended for production) compression: gzip # Headers (for authentication, multi-tenancy) headers: X-Scope-OrgID: \"battlebots\" File Storage Extension for Persistence To persist queued logs across collector restarts, configure the file storage extension:\nextensions: file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s service: extensions: [file_storage] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Batch Processor Configuration Always include a batch processor before the exporter to optimize throughput:\nprocessors: batch: timeout: 10s # Send batch after this duration send_batch_size: 8192 # Send when batch reaches this size send_batch_max_size: 16384 # Never exceed this size Complete Service Pipeline service: extensions: [file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [resource_detection, batch, attributes] exporters: [otlphttp/logs] Resource Attribute Mapping When logs arrive via OTLP, resource attributes from the OpenTelemetry SDK map to either index labels or structured metadata in Loki.\nDefault Resource Attributes as Index Labels By default, Loki converts these 17 resource attributes to index labels:\nservice.name → service_name service.namespace → service_namespace service.instance.id → service_instance_id deployment.environment → deployment_environment cloud.region → cloud_region cloud.availability_zone → cloud_availability_zone cloud.platform → cloud_platform k8s.cluster.name → k8s_cluster_name k8s.namespace.name → k8s_namespace_name k8s.pod.name → k8s_pod_name k8s.container.name → k8s_container_name container.name → container_name k8s.replicaset.name → k8s_replicaset_name k8s.deployment.name → k8s_deployment_name k8s.statefulset.name → k8s_statefulset_name k8s.daemonset.name → k8s_daemonset_name k8s.cronjob.name → k8s_cronjob_name Note: Attribute names with dots (.) are converted to underscores (_) for Loki label compatibility.\nAttribute Transformation in Collector To add or modify resource attributes before sending to Loki:\nprocessors: resource: attributes: # Add static attributes - key: deployment.environment value: production action: insert # Copy attributes with character transformations - key: service_name from_attribute: service.name action: insert # Delete attributes you don't want - key: telemetry.sdk.version action: delete attributes: actions: # Add log-level attributes - key: environment value: production action: insert # Extract correlation IDs from log body - key: correlation_id pattern: \"correlation_id=([a-z0-9-]+)\" action: extract Example Attribute to Label Mapping Input (OpenTelemetry SDK):\n{ \"resource\": { \"attributes\": { \"service.name\": \"game-server\", \"service.namespace\": \"battlebots\", \"deployment.environment\": \"production\", \"k8s.pod.name\": \"game-server-5d7c8f9b-xq2wr\", \"k8s.namespace.name\": \"battlebots-prod\" } } } Output (Loki Labels):\n{ service_name=\"game-server\", service_namespace=\"battlebots\", deployment_environment=\"production\", k8s_namespace_name=\"battlebots-prod\" } Note: k8s.pod.name should be converted to structured metadata (see Label Strategy section).\nLabel Strategy and Best Practices Loki’s performance depends heavily on proper label cardinality management. Every unique combination of label values creates a new stream, and too many streams degrade performance significantly.\nAvoiding High Cardinality Issues Problem: High cardinality causes Loki to build a huge index and flush thousands of tiny chunks to object storage, resulting in poor performance and high costs.\nHigh-Cardinality Attributes (Avoid as Labels):\nk8s.pod.name - Each pod instance creates a new label value service.instance.id - Each service instance is unique process.pid - Changes on every process restart User IDs, request IDs, transaction IDs Timestamps, UUIDs, hashes Which Attributes to Use as Labels Good Label Candidates (Low Cardinality):\nservice.name - Limited number of services service.namespace - Few namespaces (dev, staging, prod) deployment.environment - Usually 3-5 values k8s.cluster.name - Fixed cluster names k8s.namespace.name - Limited namespaces per cluster cloud.region - Fixed set of regions log.severity or severity_text - Limited severity levels Cardinality Rule of Thumb: Keep total stream count under 10,000. With 5 labels averaging 10 values each, you get 10^5 = 100,000 streams (too many). Reduce to 3-4 labels with controlled values.\nWhich Attributes to Keep as Structured Metadata Configure high-cardinality attributes as structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id - process.pid - process.command_line - host.id Structured metadata remains queryable but doesn’t create new streams:\n{service_name=\"game-server\"} | k8s_pod_name=\"game-server-5d7c8f9b-xq2wr\" Recommended Label Strategy for BattleBots limits_config: otlp_config: resource_attributes: attributes_config: # Index labels (low cardinality) - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region # Structured metadata (high cardinality or optional) - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name - process.pid Expected Cardinality:\nservice.name: ~10 services (game-server, matchmaker, auth, etc.) service.namespace: 1 value (battlebots) deployment.environment: 3 values (dev, staging, production) k8s.namespace.name: ~5 namespaces cloud.region: ~3 regions Total streams: 10 × 1 × 3 × 5 × 3 = 450 streams (excellent)\nLog-Trace Correlation OpenTelemetry’s unified data model enables seamless correlation between logs and traces through TraceId and SpanId fields embedded in log records.\nTraceID and SpanID Storage When applications instrumented with OpenTelemetry SDKs emit logs within a trace context, the SDK automatically includes:\nTraceId: Unique identifier for the entire trace SpanId: Unique identifier for the current span TraceFlags: Sampling and other flags Loki stores these fields as structured metadata, making them queryable without parsing the log body.\nQuerying Logs by Trace ID Find all logs for a specific trace:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Find logs with any trace context:\n{service_name=\"game-server\"} | trace_id != \"\" Find logs for a specific span:\n{service_name=\"game-server\"} | span_id=\"00f067aa0ba902b7\" Grafana Integration for Correlation Configure Grafana data source correlations to link Loki and Tempo:\nLoki Data Source Configuration:\n{ \"derivedFields\": [ { \"datasourceUid\": \"tempo-uid\", \"matcherRegex\": \"trace_id=(\\\\w+)\", \"name\": \"TraceID\", \"url\": \"${__value.raw}\" } ] } This creates clickable trace ID links in the Grafana Explore view, allowing you to:\nView a log entry in Loki Click the trace ID link Jump directly to the full trace in Tempo See the complete request flow with timing information Example Correlation Workflow Scenario: Investigating a slow game server response\nStart with metrics: Notice elevated response times in Prometheus metrics Query slow traces: Find traces with duration \u003e 1s in Tempo Jump to logs: Click trace ID to see all logs for that request Identify root cause: Read detailed error messages and debug logs Correlate with resources: Use k8s_pod_name metadata to check pod health Query pattern:\n{service_name=\"game-server\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" | severity_text=\"ERROR\" Authentication \u0026 Multi-tenancy Loki supports both basic authentication and multi-tenant deployments for OTLP ingestion.\nBasic Authentication Setup Collector Configuration with Basic Auth:\nextensions: basicauth/otlp: client_auth: username: \"battlebots-collector\" password: \"${LOKI_PASSWORD}\" # Use environment variable exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" auth: authenticator: basicauth/otlp service: extensions: [basicauth/otlp] pipelines: logs: receivers: [otlp] processors: [batch] exporters: [otlphttp/logs] Loki Configuration with Basic Auth:\nConfigure authentication in your reverse proxy (nginx, Envoy) or API gateway rather than directly in Loki:\nlocation /otlp { auth_basic \"Loki OTLP Endpoint\"; auth_basic_user_file /etc/nginx/.htpasswd; proxy_pass http://loki:3100; } Multi-tenant Headers For multi-tenant Loki deployments, use the X-Scope-OrgID header to specify the tenant:\nexporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"battlebots-production\" Dynamic Tenant Selection:\nRoute different services to different tenants using the resource processor:\nprocessors: resource: attributes: - key: loki.tenant from_attribute: deployment.environment action: insert exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" headers: X-Scope-OrgID: \"${LOKI_TENANT}\" Loki Multi-tenancy Configuration:\nauth_enabled: true limits_config: # Per-tenant rate limits ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 # Per-tenant OTLP configuration per_tenant_override_config: /etc/loki/overrides.yaml Per-tenant overrides (/etc/loki/overrides.yaml):\noverrides: battlebots-production: ingestion_rate_mb: 50 retention_period: 720h # 30 days battlebots-staging: ingestion_rate_mb: 20 retention_period: 168h # 7 days TLS Configuration Collector with TLS:\nexporters: otlphttp/logs: endpoint: \"https://loki.battlebots.example.com/otlp\" tls: insecure: false ca_file: /etc/ssl/certs/ca-bundle.crt cert_file: /etc/ssl/certs/collector-client.crt key_file: /etc/ssl/private/collector-client.key min_version: \"1.3\" server_name_override: loki.battlebots.example.com Loki TLS Configuration:\nserver: http_listen_port: 3100 grpc_listen_port: 9096 http_tls_config: cert_file: /etc/loki/tls/server.crt key_file: /etc/loki/tls/server.key client_auth_type: RequireAndVerifyClientCert client_ca_file: /etc/loki/tls/ca.crt Complete Configuration Example This section provides a full, working configuration for integrating OpenTelemetry Collector with Loki OTLP.\nFull OpenTelemetry Collector Configuration # /etc/otelcol/config.yaml extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 file_storage: directory: /var/lib/otelcol/file_storage timeout: 10s receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 filelog: include: - /var/log/battlebots/*.log include_file_path: true include_file_name: false operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' processors: resourcedetection: detectors: [env, system, docker, kubernetes] timeout: 5s resource: attributes: - key: deployment.environment value: production action: insert - key: service.namespace value: battlebots action: insert attributes: actions: - key: loki.attribute.labels value: severity_text, service_name action: insert batch: timeout: 10s send_batch_size: 8192 send_batch_max_size: 16384 memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 exporters: otlphttp/logs: endpoint: \"http://loki:3100/otlp\" tls: insecure: true timeout: 30s retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 storage: file_storage compression: gzip headers: X-Scope-OrgID: \"battlebots\" debug: verbosity: detailed sampling_initial: 5 sampling_thereafter: 200 service: extensions: [health_check, pprof, zpages, file_storage] pipelines: logs: receivers: [otlp, filelog] processors: [memory_limiter, resourcedetection, resource, attributes, batch] exporters: [otlphttp/logs] telemetry: logs: level: info metrics: address: 0.0.0.0:8888 Full Loki Configuration with OTLP # /etc/loki/config.yaml auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 log_level: info common: path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: \"2024-04-01\" store: tsdb object_store: filesystem schema: v13 index: prefix: loki_index_ period: 24h limits_config: allow_structured_metadata: true max_structured_metadata_entries_count: 128 reject_old_samples: true reject_old_samples_max_age: 168h ingestion_rate_mb: 50 ingestion_burst_size_mb: 100 per_stream_rate_limit: 10MB per_stream_rate_limit_burst: 20MB max_label_names_per_series: 15 otlp_config: resource_attributes: attributes_config: - action: index_label attributes: - service.name - service.namespace - deployment.environment - k8s.namespace.name - cloud.region - action: structured_metadata attributes: - k8s.pod.name - k8s.container.name - service.instance.id - host.name storage_config: tsdb_shipper: active_index_directory: /loki/tsdb-index cache_location: /loki/tsdb-cache filesystem: directory: /loki/chunks compactor: working_directory: /loki/compactor compaction_interval: 10m retention_enabled: true retention_delete_delay: 2h retention_delete_worker_count: 150 querier: max_concurrent: 4 query_scheduler: max_outstanding_requests_per_tenant: 4096 frontend: max_outstanding_per_tenant: 4096 Docker Compose Example version: '3.8' services: loki: image: grafana/loki:3.0.0 container_name: loki ports: - \"3100:3100\" - \"9096:9096\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml - loki-data:/loki command: -config.file=/etc/loki/config.yaml networks: - battlebots-observability otel-collector: image: otel/opentelemetry-collector-contrib:0.96.0 container_name: otel-collector ports: - \"4317:4317\" # OTLP gRPC receiver - \"4318:4318\" # OTLP HTTP receiver - \"8888:8888\" # Metrics endpoint - \"13133:13133\" # Health check volumes: - ./otel-config.yaml:/etc/otelcol/config.yaml - /var/lib/otelcol:/var/lib/otelcol command: [\"--config=/etc/otelcol/config.yaml\"] depends_on: - loki networks: - battlebots-observability grafana: image: grafana/grafana:10.4.0 container_name: grafana ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_PASSWORD=admin - GF_USERS_ALLOW_SIGN_UP=false volumes: - grafana-data:/var/lib/grafana - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml depends_on: - loki networks: - battlebots-observability volumes: loki-data: grafana-data: networks: battlebots-observability: driver: bridge Grafana Data Source Configuration # grafana-datasources.yaml apiVersion: 1 datasources: - name: Loki type: loki access: proxy url: http://loki:3100 jsonData: derivedFields: - datasourceUid: tempo matcherRegex: \"trace_id=(\\\\w+)\" name: TraceID url: \"$${__value.raw}\" Step-by-Step Setup Create configuration files:\nmkdir -p battlebots-observability cd battlebots-observability # Copy the configurations above into: # - loki-config.yaml # - otel-config.yaml # - grafana-datasources.yaml # - docker-compose.yaml Start the stack:\ndocker-compose up -d Verify Loki is running:\ncurl http://localhost:3100/ready # Expected: ready Verify OTel Collector is running:\ncurl http://localhost:13133 # Expected: {\"status\":\"Server available\"} Send test logs:\ncurl -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d '{ \"resourceLogs\": [{ \"resource\": { \"attributes\": [{ \"key\": \"service.name\", \"value\": {\"stringValue\": \"test-service\"} }] }, \"scopeLogs\": [{ \"logRecords\": [{ \"timeUnixNano\": \"1640000000000000000\", \"severityText\": \"INFO\", \"body\": {\"stringValue\": \"Test log message\"} }] }] }] }' Query logs in Grafana:\nOpen http://localhost:3000 Login with admin/admin Navigate to Explore Select Loki data source Query: {service_name=\"test-service\"} Troubleshooting Connection Errors Problem: Collector cannot connect to Loki OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: Post \"http://loki:3100/otlp/v1/logs\": dial tcp: lookup loki: no such host Solutions:\nVerify Loki is running: curl http://loki:3100/ready Check DNS resolution: nslookup loki (or use IP address) Verify network connectivity: telnet loki 3100 Check Docker network configuration if using containers Verify endpoint URL doesn’t include /v1/logs suffix Problem: 404 Not Found on OTLP endpoint\nSymptoms:\nerror exporting items: failed to push logs: HTTP 404 Not Found Solutions:\nVerify Loki version is 3.0 or later: curl http://loki:3100/loki/api/v1/status/buildinfo Check schema version is v13 in Loki config Verify allow_structured_metadata: true in limits_config Restart Loki after configuration changes Label Cardinality Problems Problem: Too many streams causing performance degradation\nSymptoms:\nlevel=warn msg=\"stream limit exceeded\" limit=10000 streams=15234 Solutions:\nReview current label cardinality:\ncurl http://localhost:3100/loki/api/v1/label Check value distribution per label:\ncurl http://localhost:3100/loki/api/v1/label/service_name/values Move high-cardinality attributes to structured metadata:\nlimits_config: otlp_config: resource_attributes: attributes_config: - action: structured_metadata attributes: - k8s.pod.name - service.instance.id Set cardinality limits:\nlimits_config: max_label_names_per_series: 15 max_label_value_length: 2048 max_label_name_length: 1024 Structured Metadata Issues Problem: Structured metadata not appearing in queries\nSymptoms: Attributes missing from log entries in Grafana\nSolutions:\nVerify schema version 13 or higher Check allow_structured_metadata: true in limits Verify attributes aren’t being dropped by processors Query with explicit structured metadata filter: {service_name=\"game-server\"} | k8s_pod_name=\"pod-123\" Performance Issues Problem: Slow queries or high memory usage\nSymptoms: Grafana queries timeout or Loki OOM errors\nSolutions:\nEnable query limits:\nlimits_config: max_query_series: 500 max_query_lookback: 720h max_entries_limit_per_query: 5000 Optimize batch processor:\nprocessors: batch: timeout: 5s # Reduce for faster flushing send_batch_size: 4096 # Smaller batches Add memory limiter:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 Use query acceleration:\nlimits_config: bloom_gateway_enable_filtering: true Debugging Techniques Enable debug logging in Collector:\nexporters: debug: verbosity: detailed service: pipelines: logs: receivers: [otlp] processors: [batch] exporters: [debug, otlphttp/logs] # Add debug exporter telemetry: logs: level: debug # Collector internal logs Enable debug logging in Loki:\nserver: log_level: debug Check Loki metrics:\ncurl http://localhost:3100/metrics | grep loki_distributor Verify OTLP data structure:\n# Send test log and capture response curl -v -X POST http://localhost:4318/v1/logs \\ -H \"Content-Type: application/json\" \\ -d @test-log.json BattleBots Integration Points Observability Stack Architecture The Loki OTLP integration fits into the BattleBots observability stack as follows:\nGame Servers (Go) └─\u003e OpenTelemetry SDK └─\u003e OTLP/gRPC (4317) └─\u003e OTel Collector ├─\u003e Loki (Logs via OTLP) ├─\u003e Tempo (Traces via OTLP) └─\u003e Prometheus (Metrics via OTLP) └─\u003e Grafana (Visualization \u0026 Correlation) Collector → Loki Pipeline for Game Servers Recommended pipeline configuration:\nReceive logs from game servers via OTLP Detect resource attributes (Kubernetes, cloud provider) Add BattleBots-specific labels (environment, service namespace) Filter out verbose debug logs in production Batch and compress for efficiency Export to Loki via OTLP HTTP Log Types and Labeling Strategy Game Event Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" event_type: \"game_event\" # Custom label Structured Metadata: match_id: \"uuid\" player_count: 4 game_mode: \"elimination\" System Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" log_type: \"system\" Structured Metadata: k8s_pod_name: \"game-server-abc123\" severity_text: \"ERROR\" Client Connection Logs:\nLabels: service_name: \"game-server\" service_namespace: \"battlebots\" deployment_environment: \"production\" Structured Metadata: client_id: \"uuid\" connection_state: \"connected\" trace_id: \"trace-id\" # For correlation Example Queries for BattleBots Find all errors in production:\n{service_namespace=\"battlebots\", deployment_environment=\"production\"} | severity_text=\"ERROR\" Find logs for a specific match:\n{service_name=\"game-server\"} | match_id=\"550e8400-e29b-41d4-a716-446655440000\" Find all logs related to a slow trace:\n{service_namespace=\"battlebots\"} | trace_id=\"4bf92f3577b34da6a3ce929d0e0e4736\" Count game events by type:\nsum by (event_type) ( count_over_time({service_name=\"game-server\"}[5m]) ) Further Reading OTLP Specification OpenTelemetry Protocol Specification - Complete OTLP specification OTLP Logs Data Model - OpenTelemetry logs data model OTLP Exporter Configuration - SDK configuration guide Grafana Loki OTLP Documentation Ingesting logs to Loki using OpenTelemetry Collector - Official Loki OTLP guide Getting started with OTel Collector and Loki - Step-by-step tutorial Native OTLP vs Loki Exporter - Migration guide Loki 3.0 Release Notes - Features announcement Migration to Native OTLP Format - Cloud migration guide OpenTelemetry Collector Documentation Collector Configuration - General configuration guide Collector Resiliency - Retry and queue configuration Configuration Best Practices - Security and performance Exporter Helper Documentation - Retry and queue details Loki Configuration and Operations Understanding Labels - Label strategy guide Structured Metadata - Structured metadata overview Loki Schema Configuration - Schema v13 setup Upgrade to Loki 3.0 - Migration instructions Grafana Integration Trace Integration in Explore - Log-trace correlation Trace Correlations - Setting up correlations Configure Loki Data Source - Data source configuration Community Guides and Tutorials Grafana Loki 101: Ingesting with OTel Collector - Official blog tutorial Building a Logging Pipeline with OTel, Loki, and Grafana - Docker Compose guide Logging with OpenTelemetry and Loki - Practical implementation Efficient Application Log Collection - Analysis guide Troubleshooting and Best Practices OTel Batching Best Practices - Batch configuration Collector Persistence and Retry - Deep dive Label Cardinality Issues - Common problems and solutions Related BattleBots Documentation OpenTelemetry Collector: Logs Support - Collector log processing OpenTelemetry Collector: Overview - Collector architecture ","categories":"","description":"Detailed guide for integrating Grafana Loki with OpenTelemetry Collector via native OTLP support.\n","excerpt":"Detailed guide for integrating Grafana Loki with OpenTelemetry …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/logs/loki/loki-otlp-integration/","tags":"","title":"Loki: OTLP Integration"},{"body":"Overview Grafana Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system designed to be cost-effective and easy to operate. Inspired by Prometheus, Loki takes a fundamentally different approach to log storage compared to traditional systems like Elasticsearch.\nThe core innovation of Loki is its index-free architecture: instead of indexing the full contents of log lines, Loki only indexes metadata labels for each log stream. This design dramatically reduces storage costs, memory requirements, and operational complexity while still enabling fast queries through label-based filtering and grep-style text search.\nLoki integrates seamlessly with Grafana for visualization, supports native OTLP ingestion (v3+), and works alongside Prometheus and Tempo to provide a complete observability stack. The system is built for cloud-native environments with first-class support for Kubernetes, containerized workloads, and distributed architectures.\nKey Concepts Understanding Loki’s data model is essential for effective deployment and usage.\nStreams A stream is the fundamental data structure in Loki, representing a sequence of log entries that share the same set of labels. Each unique combination of labels creates a new stream. For example:\n{app=\"battleserver\", env=\"prod\", region=\"us-east\"} {app=\"battleserver\", env=\"prod\", region=\"us-west\"} {app=\"matchmaker\", env=\"prod\", region=\"us-east\"} These three label sets create three distinct streams. All log entries with identical labels are appended to the same stream in chronological order.\nLabels Labels are key-value pairs that categorize and identify log streams. Labels are the only metadata indexed by Loki, making label design the most critical aspect of Loki deployment.\nLabels should be:\nLow cardinality: Use labels that have bounded, predictable values (e.g., environment, service, host) Descriptive: Represent the source or context of logs (e.g., namespace, pod, container) Static: Avoid labels that change frequently or have unique values per log entry Anti-pattern: Using high-cardinality values like user IDs, trace IDs, or timestamps as labels will severely degrade performance.\n# Good label design (low cardinality) {service=\"game-server\", environment=\"production\", region=\"us-east-1\"} # Bad label design (high cardinality) {trace_id=\"abc123\", user_id=\"user456\", timestamp=\"2025-12-03T10:00:00Z\"} Loki recommends keeping total stream count below 10,000 for small deployments and under 100,000 active streams for larger deployments.\nChunks Chunks are compressed blocks of log data from a single stream. As logs arrive for a stream, the ingester accumulates them in memory, then periodically flushes completed chunks to object storage.\nKey chunk characteristics:\nContain only data from a single stream Compressed using LZ4 or Snappy Stored in object storage (S3, GCS, MinIO, filesystem) Typically span minutes to hours of log data Subject to configurable size and time limits The chunk format optimizes for sequential reads, making time-range queries efficient.\nIndex The index stores the mapping between label sets and their corresponding chunks. Unlike full-text indexes, Loki’s index only contains:\nLabel names and values Chunk references (location, time range) Stream metadata This minimal indexing approach is what makes Loki cost-effective. The index is stored separately from chunks, typically in a different backend (BoltDB, TSDB).\nLogQL LogQL is Loki’s query language, inspired by Prometheus’s PromQL. LogQL queries have two stages:\nLog stream selection: Filter streams using label matchers Log pipeline: Parse, filter, and transform selected log lines # Select streams, then filter log content {service=\"game-server\"} |= \"error\" | json | level=\"ERROR\" # Aggregate metrics from logs rate({service=\"game-server\"} |= \"battle completed\" [5m]) # Multi-stage pipeline with parsing and filtering {namespace=\"battlebots\"} | json | line_format \"{{.message}}\" | pattern `\u003c_\u003e level=\u003clevel\u003e \u003c_\u003e` | level = \"error\" LogQL supports metric queries, allowing you to generate time-series data from logs (e.g., error rates, request counts).\nArchitecture Components Loki uses a microservices architecture where each component can be scaled independently or combined into larger deployment targets.\ngraph TB A[Log Clients\u003cbr/\u003ePromtail/Alloy/Fluentd] --\u003e|Push logs| B[Distributor] B --\u003e|Replicate| C1[Ingester 1] B --\u003e|Replicate| C2[Ingester 2] B --\u003e|Replicate| C3[Ingester N] C1 \u0026 C2 \u0026 C3 --\u003e|Flush chunks| D[Object Storage\u003cbr/\u003eS3/GCS/MinIO] C1 \u0026 C2 \u0026 C3 --\u003e|Update index| E[Index Store\u003cbr/\u003eBoltDB/TSDB] F[Grafana/API] --\u003e|Query| G[Query Frontend] G --\u003e|Split queries| H[Querier] H --\u003e|Read recent| C1 \u0026 C2 \u0026 C3 H --\u003e|Read historical| D H --\u003e|Read index| E I[Compactor] --\u003e|Compact| D I --\u003e|Update| E J[Ruler] --\u003e|Evaluate rules| H J --\u003e|Store| D style B fill:#fff4e6 style C1 fill:#fff4e6 style C2 fill:#fff4e6 style C3 fill:#fff4e6 style G fill:#e1f5ff style H fill:#e1f5ff style I fill:#f3e5f5 style J fill:#e8f5e9 style D fill:#fce4ec style E fill:#fce4ec Distributor The distributor is the entry point for log ingestion. It receives log streams from clients (Promtail, Alloy, OTLP endpoints) and routes them to ingesters.\nResponsibilities:\nValidate incoming log streams for correctness and tenant limits Apply rate limiting per tenant Hash log streams by labels to determine target ingesters Replicate each stream to multiple ingesters (default: 3 replicas) Load balance across available ingesters Distributors are stateless and can be horizontally scaled to handle high ingestion rates.\nIngester The ingester receives log streams from distributors and is responsible for:\nBuffering logs in memory for each stream Building compressed chunks Flushing chunks to object storage periodically Writing index entries Serving queries for recent (unflushed) data Ingesters maintain an in-memory index of recent logs and use a write-ahead log (WAL) for crash recovery. Upon graceful shutdown, ingesters flush all buffered data to storage.\nIngesters are stateful and require careful scaling considerations. They use consistent hashing to distribute streams evenly across instances.\nQuerier The querier executes LogQL queries by:\nFetching index data to identify relevant chunks Retrieving chunks from object storage Querying ingesters for recent unflushed data Merging results from multiple sources Applying log pipeline operations (parsing, filtering) Returning results to the query frontend Queriers are stateless and can be scaled horizontally. They cache chunk data and index lookups to improve performance.\nQuery Frontend The query frontend sits in front of queriers and provides:\nQuery splitting: Breaks large time-range queries into smaller sub-queries Query queuing: Prevents overwhelming queriers during traffic spikes Caching: Stores query results to avoid redundant computation Fair scheduling: Ensures multiple tenants share query resources equitably The frontend is optional but highly recommended for production deployments. It significantly improves query performance and protects backend components from overload.\nCompactor The compactor is a background service that:\nMerges small chunks into larger ones to improve query performance Removes duplicate data from replicated writes Applies retention policies by deleting old data Updates index to reflect compacted chunks Only one compactor should run per tenant to avoid conflicts. The compactor is critical for long-term storage efficiency.\nRuler The ruler evaluates recording rules and alerting rules against stored logs:\nRuns LogQL queries on a schedule Generates derived metrics from log data Triggers alerts based on log patterns Stores rule evaluation results The ruler is optional and typically used for log-based alerting scenarios.\nIndex Gateway The index gateway (available in recent versions) centralizes index access:\nProvides a single point for index queries Reduces load on the index store Enables better caching of index data Simplifies index backend scaling This component is particularly useful with BoltDB index backends to avoid direct file access from multiple queriers.\nDeployment Modes Loki supports three deployment modes, each balancing simplicity against scalability and operational flexibility.\nMonolithic Mode In monolithic mode, all Loki components run in a single process. This is the simplest deployment option.\nConfiguration:\nloki -target=all -config.file=loki-config.yaml Characteristics:\nSingle binary or container All components share memory and resources Minimal operational complexity Limited horizontal scalability Suitable for development and small deployments When to use:\nDevelopment and testing environments Proof-of-concept deployments Small-scale production (\u003c100GB/day log ingestion) Single-server deployments Limitations:\nCannot scale components independently Single point of failure Resource contention between components Limited to vertical scaling (bigger instances) Simple Scalable Deployment (SSD) Simple scalable deployment groups components into three logical targets: read, write, and backend.\nTargets:\nRead (-target=read): Query Frontend, Querier Write (-target=write): Distributor, Ingester Backend (-target=backend): Compactor, Ruler, Index Gateway Configuration example:\n# Write path (3 replicas for high availability) loki -target=write -config.file=loki-config.yaml # Read path (scale based on query load) loki -target=read -config.file=loki-config.yaml # Backend (single instance) loki -target=backend -config.file=loki-config.yaml Characteristics:\nSeparates read and write paths Independent scaling of ingestion vs queries Easier to operate than full microservices Supports ~1TB/day log ingestion When to use:\nMedium-scale production deployments When you need to scale reads and writes independently Kubernetes environments using Helm charts Teams wanting operational simplicity with scalability This is the recommended starting point for most production deployments.\nMicroservices Mode Microservices mode runs each Loki component as a separate deployment, providing maximum flexibility.\nComponents:\nDistributor (multiple instances) Ingester (multiple instances, stateful) Querier (multiple instances) Query Frontend (multiple instances) Compactor (single instance per tenant) Ruler (multiple instances) Index Gateway (multiple instances) Characteristics:\nEach component scaled independently Fine-grained resource allocation Most complex to deploy and maintain Supports enterprise-scale deployments (multi-TB/day) When to use:\nVery large Loki clusters (\u003e1TB/day) Organizations requiring precise control over scaling Multi-tenant SaaS deployments Teams with dedicated Loki operations expertise Considerations:\nSignificantly higher operational complexity More components to monitor and maintain Requires sophisticated orchestration (Kubernetes) Network communication overhead between components How to Run Loki This section provides practical guidance for running Loki, focusing on Docker/Podman Compose for POC and development.\nQuick Start with Docker Compose The simplest way to evaluate Loki is using the official Docker Compose example.\nStep 1: Create directory and download configurations\nmkdir loki-poc \u0026\u0026 cd loki-poc # Download Loki configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/loki-config.yaml # Download Alloy (log shipper) configuration wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/alloy-local-config.yaml # Download Docker Compose file wget https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/docker-compose.yaml Step 2: Review the Docker Compose file\nThe compose file includes:\nLoki (monolithic mode) Grafana (for visualization) Grafana Alloy (log collection agent) flog (log generator for testing) Step 3: Start the stack\ndocker compose up -d Step 4: Verify deployment\n# Check Loki readiness curl http://localhost:3100/ready # Check Loki metrics curl http://localhost:3100/metrics # Access Grafana # URL: http://localhost:3000 # Default credentials: admin / admin Step 5: Query logs\nIn Grafana, navigate to Explore and select the Loki datasource to query logs using LogQL.\nBasic Configuration File Structure A minimal Loki configuration for local development:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /tmp/loki storage: filesystem: chunks_directory: /tmp/loki/chunks rules_directory: /tmp/loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24h limits_config: reject_old_samples: true reject_old_samples_max_age: 168h max_cache_freshness_per_query: 10m split_queries_by_interval: 15m query_range: align_queries_with_step: true cache_results: true ruler: alertmanager_url: http://localhost:9093 This configuration uses local filesystem storage and is suitable for development only.\nProduction Configuration with MinIO For a more production-like setup using MinIO as object storage:\nDocker Compose with MinIO:\nversion: \"3.8\" services: minio: image: minio/minio:latest entrypoint: - sh - -euc - | mkdir -p /data/loki-data minio server /data --console-address :9001 environment: - MINIO_ROOT_USER=loki - MINIO_ROOT_PASSWORD=supersecret - MINIO_PROMETHEUS_AUTH_TYPE=public ports: - \"9000:9000\" - \"9001:9001\" volumes: - minio-data:/data loki: image: grafana/loki:3.0.0 ports: - \"3100:3100\" volumes: - ./loki-config.yaml:/etc/loki/config.yaml command: -config.file=/etc/loki/config.yaml depends_on: - minio grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin volumes: - grafana-data:/var/lib/grafana volumes: minio-data: grafana-data: Loki configuration with MinIO:\nauth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /loki storage: s3: endpoint: minio:9000 bucketnames: loki-data access_key_id: loki secret_access_key: supersecret s3forcepathstyle: true insecure: true replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 schema: v13 index: prefix: index_ period: 24h limits_config: ingestion_rate_mb: 10 ingestion_burst_size_mb: 20 max_global_streams_per_user: 10000 max_query_length: 721h max_query_parallelism: 16 max_streams_per_user: 0 max_cache_freshness_per_query: 10m query_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 100 frontend: encoding: protobuf compress_responses: true max_outstanding_per_tenant: 2048 chunk_store_config: max_look_back_period: 0s table_manager: retention_deletes_enabled: true retention_period: 336h Resource Requirements Minimum requirements for development/POC:\nCPU: 2 cores Memory: 4 GB RAM Storage: 20 GB (filesystem) or object storage bucket Recommended production requirements (simple scalable mode):\nWrite path (per instance):\nCPU: 4-8 cores Memory: 8-16 GB RAM (for buffering chunks) Network: High bandwidth for ingestion Read path (per instance):\nCPU: 4-8 cores Memory: 16-32 GB RAM (for query caching) Network: High bandwidth for chunk retrieval Backend:\nCPU: 2-4 cores Memory: 4-8 GB RAM Storage: Object storage (S3, GCS, MinIO) with sufficient capacity for retention period Storage sizing:\nEstimate: ~5-10 GB/day per 1 million log lines (varies by compression ratio) Retention: storage_size = daily_volume * retention_days Index: ~1-2% of total chunk storage Getting Started Steps Choose deployment mode: Start with monolithic for POC, plan for simple scalable in production Set up object storage: MinIO for local dev, S3/GCS for production Configure Loki: Use appropriate schema version (v13 recommended) Deploy Loki: Docker Compose for POC, Helm for Kubernetes Configure log shippers: Alloy, Promtail, or OTLP endpoints Verify ingestion: Check /ready endpoint and metrics Set up Grafana: Add Loki datasource and create dashboards Test queries: Use LogQL to validate data retrieval Monitor Loki: Set up self-monitoring (metrics, logs, traces) Optimize configuration: Tune based on ingestion rate and query patterns Best Practices for Running Loki Successful Loki deployments depend on following these operational best practices.\nLabel Strategy 1. Keep labels low cardinality\nAim for 10-15 labels maximum across all streams. Each unique label combination creates a new stream.\n# Good: Low cardinality (bounded values) { service=\"game-server\", environment=\"production\", region=\"us-east-1\", cluster=\"battlebot-cluster-01\" } # Bad: High cardinality (unbounded values) { service=\"game-server\", trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", user_id=\"user_12345\", session_id=\"sess_67890\" } 2. Use static labels that describe log sources\nLabels should represent where logs come from, not what’s in them:\nnamespace, pod, container (Kubernetes) host, instance (infrastructure) service, application, component (application) environment, region, cluster (deployment context) 3. Avoid pod names and instance IDs as labels\nPod names and container IDs change frequently, creating stream churn:\n# Avoid {pod=\"game-server-abc123-xyz456\"} # Instead use {service=\"game-server\", namespace=\"battlebots\"} 4. Use filter expressions for high-cardinality data\nSearch for user IDs, trace IDs, or other high-cardinality values using LogQL filters:\n# Query for specific trace ID {service=\"game-server\"} |= \"trace_id=7a3f8c2d\" # Query for specific user {service=\"game-server\"} | json | user_id=\"12345\" 5. Use structured metadata for supplemental high-cardinality data\nLoki v2.9+ supports structured metadata, which stores high-cardinality data without indexing it:\n# Structured metadata (not indexed, not creating streams) labels: {service=\"game-server\"} structured_metadata: trace_id: \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" user_id: \"user_12345\" 6. Monitor stream count\nUse Loki metrics to track stream cardinality:\n# Total active streams loki_ingester_memory_streams # Streams per tenant sum by (tenant) (loki_ingester_memory_streams) Keep total streams under 10,000 for small deployments, under 100,000 for larger deployments.\nConfiguration Tips 1. Set appropriate limits\nlimits_config: # Rate limiting ingestion_rate_mb: 10 # MB/s per tenant ingestion_burst_size_mb: 20 # Burst allowance # Stream limits max_global_streams_per_user: 10000 # Total active streams max_line_size: 256000 # Bytes per log line max_entries_limit_per_query: 5000 # Max returned entries # Query limits max_query_length: 721h # 30 days max_query_parallelism: 16 # Concurrent query threads # Retention reject_old_samples: true reject_old_samples_max_age: 168h # 7 days 2. Configure retention\nlimits_config: retention_period: 744h # 31 days table_manager: retention_deletes_enabled: true retention_period: 744h Note: Retention requires compactor to be running.\n3. Enable caching\nquery_range: align_queries_with_step: true cache_results: true results_cache: cache: embedded_cache: enabled: true max_size_mb: 500 chunk_store_config: chunk_cache_config: embedded_cache: enabled: true max_size_mb: 1000 4. Use TSDB index (v13 schema)\nThe TSDB index (schema v13) offers better performance than BoltDB:\nschema_config: configs: - from: 2024-01-01 store: tsdb # Use TSDB object_store: s3 schema: v13 # Latest schema index: prefix: index_ period: 24h 5. Configure appropriate chunk settings\ningester: chunk_idle_period: 30m # Flush idle chunks after 30 min chunk_block_size: 262144 # 256 KB blocks chunk_encoding: snappy # Compression algorithm chunk_retain_period: 15m # Retain flushed chunks in memory max_chunk_age: 1h # Max time before forced flush wal: enabled: true dir: /loki/wal Storage Considerations 1. Choose appropriate object storage\nDevelopment: Filesystem or MinIO Production AWS: S3 with lifecycle policies Production GCP: GCS with object versioning Production Azure: Azure Blob Storage On-premises: MinIO cluster or compatible S3 service 2. Configure object storage lifecycle\nReduce storage costs by transitioning older data to cheaper tiers:\n# AWS S3 lifecycle example - Id: TransitionOldChunks Status: Enabled Transitions: - Days: 30 StorageClass: STANDARD_IA - Days: 90 StorageClass: GLACIER 3. Separate index and chunk storage\nFor better performance, use different backends:\nschema_config: configs: - from: 2024-01-01 store: tsdb object_store: s3 # Chunks in S3 schema: v13 index: prefix: index_ period: 24h storage_config: tsdb_shipper: active_index_directory: /loki/index cache_location: /loki/index_cache shared_store: s3 # Index in S3 aws: s3: s3://us-east-1/loki-chunks bucketnames: loki-chunks 4. Monitor storage usage\nTrack storage metrics to plan capacity:\n# Chunk storage rate rate(loki_ingester_chunk_stored_bytes_total[5m]) # Index entries created rate(loki_ingester_index_entries_total[5m]) Performance Tuning 1. Optimize ingestion\nUse batching in log shippers (Promtail, Alloy) Enable compression for network transport Scale distributors horizontally for high write load Scale ingesters based on stream count and retention 2. Optimize queries\nUse specific label matchers to reduce streams searched Limit query time ranges Use query frontend for splitting and caching Add parallelism for large queries # Good: Specific label selector, limited time range {service=\"game-server\", environment=\"prod\"} |= \"error\" [5m] # Suboptimal: Broad selector, large time range {environment=\"prod\"} [24h] 3. Use bloom filters (experimental)\nBloom filters can speed up log line filtering:\nbloom_compactor: enabled: true bloom_gateway: enabled: true 4. Tune querier parallelism\nquerier: max_concurrent: 10 # Concurrent queries per querier query_timeout: 1m # Per-query timeout limits_config: max_query_parallelism: 16 # Parallel workers per query Common Pitfalls to Avoid 1. High-cardinality labels\nProblem: Using trace IDs, user IDs, or timestamps as labels creates millions of streams.\nSolution: Use structured metadata or filter expressions instead.\n2. Not monitoring stream count\nProblem: Stream count grows unbounded, degrading performance.\nSolution: Monitor loki_ingester_memory_streams and set alerts at thresholds.\n3. Insufficient ingester memory\nProblem: Ingesters crash or flush chunks too frequently.\nSolution: Allocate 8-16 GB RAM per ingester, adjust max_chunk_age and chunk_idle_period.\n4. No retention policy\nProblem: Storage costs grow unbounded.\nSolution: Configure retention_period and enable compactor.\n5. Querying too much data\nProblem: Queries time out or overload queriers.\nSolution: Use query frontend, limit time ranges, add specific label selectors.\n6. Single ingester (no replication)\nProblem: Data loss during ingester failure.\nSolution: Set replication_factor: 3 in production.\n7. Using filesystem storage in production\nProblem: Data loss, no scalability, no durability.\nSolution: Always use object storage (S3, GCS, MinIO) for production.\n8. Not using WAL\nProblem: In-memory data lost on ingester crash.\nSolution: Enable write-ahead log:\ningester: wal: enabled: true dir: /loki/wal When to Use Loki Ideal Use Cases 1. Cloud-native and Kubernetes environments\nLoki excels in containerized environments with:\nAutomatic label extraction from Kubernetes metadata Efficient handling of ephemeral infrastructure Native Prometheus integration for unified observability 2. Cost-sensitive deployments\nLoki’s index-free architecture dramatically reduces:\nStorage costs (5-10x cheaper than Elasticsearch) Memory requirements Operational overhead 3. Integration with existing Prometheus/Grafana stacks\nIf you already use Prometheus and Grafana:\nUnified visualization across logs, metrics, and traces Similar query language (LogQL ~ PromQL) Consistent operational model 4. High-volume log aggregation with simple queries\nLoki handles massive log volumes efficiently when:\nQueries primarily filter by labels and time ranges Full-text search is limited to known patterns Aggregation and metrics-from-logs are common use cases 5. Correlation between logs, metrics, and traces\nLoki enables:\nLog-trace correlation via TraceID/SpanID Metrics extraction from logs Unified observability workflows in Grafana 6. Multi-tenant logging platforms\nLoki’s built-in multi-tenancy supports:\nIsolated log streams per tenant Per-tenant rate limiting and retention Shared infrastructure with tenant isolation Anti-Patterns 1. Complex full-text search requirements\nLoki is not a replacement for Elasticsearch when you need:\nAdvanced full-text search across all log content Complex query DSL with scoring and relevance Frequent regex searches without label filtering Ad-hoc exploratory searches without known labels 2. Frequent high-cardinality queries\nAvoid Loki if you regularly need to:\nSearch by unique identifiers (user IDs, session IDs) as primary access pattern Query without label-based filtering Perform analytics on unbounded dimensions 3. Long-term analytics and data warehouse use cases\nLoki is optimized for recent data access, not:\nHistorical data mining over years of logs Complex joins between log datasets Business intelligence and reporting workflows 4. Transactional workloads\nLoki does not provide:\nACID guarantees Immediate consistency for queries Strong durability guarantees (eventual consistency model) Loki vs. Elasticsearch Comparison Aspect Loki Elasticsearch Indexing Labels only (metadata) Full-text indexing Storage cost Low (index-free) High (full indexes) Memory usage Low High Query performance Fast for label-based queries Fast for full-text search Setup complexity Low Medium-high Operational overhead Low High Search capabilities Label filtering + grep-style Advanced full-text, DSL Best for Cloud-native, Kubernetes, cost-sensitive Enterprise search, analytics Scalability Horizontal (microservices) Horizontal (cluster) Multi-tenancy Built-in Via indexes/namespaces Integration Grafana, Prometheus, Tempo Kibana, Elastic ecosystem Decision Factors Choose Loki when:\nCost efficiency is a priority You have well-defined label taxonomy Logs are primarily time-series access patterns You use Kubernetes and Prometheus Queries filter by known dimensions (service, environment) Integration with Grafana is important Choose Elasticsearch when:\nComplex full-text search is required Ad-hoc exploratory queries are common Advanced analytics and aggregations are needed You need enterprise search capabilities Budget allows for higher infrastructure costs Team has existing ELK expertise BattleBots Integration Points For the BattleBots platform, Loki would serve as the centralized log storage backend in the observability stack.\nHow Loki Fits in the Observability Stack graph TB A[Game Servers\u003cbr/\u003eGo Services] --\u003e|Logs| B[OTel Collector] C[Infrastructure\u003cbr/\u003eContainers] --\u003e|Logs| B D[Application\u003cbr/\u003eLibraries] --\u003e|Logs| B B --\u003e|OTLP/HTTP| E[Loki\u003cbr/\u003eDistributor] E --\u003e F[Loki\u003cbr/\u003eIngesters] F --\u003e G[Object Storage\u003cbr/\u003eS3/MinIO] H[Grafana] --\u003e|LogQL| I[Loki\u003cbr/\u003eQuery Frontend] I --\u003e J[Loki\u003cbr/\u003eQueriers] J --\u003e F J --\u003e G B --\u003e|Metrics| K[Prometheus/Mimir] B --\u003e|Traces| L[Tempo] H -.Unified View.-\u003e K H -.Unified View.-\u003e L style B fill:#fff4e6 style E fill:#e1f5ff style F fill:#e1f5ff style H fill:#e8f5e9 style K fill:#f3e5f5 style L fill:#fce4ec Game Event Logging Use Cases 1. Battle event timeline\nLog all significant battle events with consistent labels:\n{service=\"battle-server\", battle_id=\"battle_123\"} | json | line_format \"{{.timestamp}} [{{.event_type}}] {{.description}}\" Example logs:\n2025-12-03T10:00:00Z [battle_start] Battle battle_123 initialized 2025-12-03T10:00:05Z [bot_action] Bot bot_456 executed attack on bot_789 2025-12-03T10:00:06Z [damage_calc] Bot bot_789 took 15 damage 2025-12-03T10:00:10Z [victory] Bot bot_456 won battle battle_123 2. Game state transitions\nTrack game state changes with structured logging:\n{ \"timestamp\": \"2025-12-03T10:00:00Z\", \"service\": \"game-server\", \"level\": \"info\", \"message\": \"Game state transition\", \"game_id\": \"game_123\", \"previous_state\": \"waiting_for_players\", \"new_state\": \"in_progress\", \"player_count\": 4 } Query pattern:\n{service=\"game-server\"} | json | message=\"Game state transition\" | game_id=\"game_123\" 3. Error tracking and debugging\nCapture errors with correlation to traces:\n{ \"timestamp\": \"2025-12-03T10:00:15Z\", \"service\": \"matchmaker\", \"level\": \"error\", \"message\": \"Failed to assign player to match\", \"error\": \"queue timeout exceeded\", \"trace_id\": \"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\", \"span_id\": \"8b4g9d3e-5f6g-22fd-92e4-1353bd241114\", \"player_id\": \"player_456\", \"queue_wait_time_ms\": 30000 } Query errors for a trace:\n{service=\"matchmaker\"} | json | trace_id=\"7a3f8c2d-4e5f-11ec-81d3-0242ac130003\" | level=\"error\" Server Log Aggregation Patterns 1. Label strategy for BattleBots\n# Standard labels for all services { service=\"battle-server\", # Service name environment=\"production\", # Deployment environment namespace=\"battlebots\", # Kubernetes namespace region=\"us-east-1\", # Deployment region version=\"v1.2.3\" # Application version } # Container-level labels (auto-discovered) { container=\"battle-server\", pod=\"battle-server-abc123-xyz456\", node=\"node-01\" } 2. Aggregating logs from multiple sources\n# All battle-related services {namespace=\"battlebots\"} | json | level=\"error\" # Specific service across all environments {service=\"matchmaker\"} | json # All production services in a region {environment=\"production\", region=\"us-east-1\"} | json 3. Metrics extraction from logs\nGenerate metrics from log data:\n# Battle completion rate rate({service=\"battle-server\"} |= \"battle completed\" [5m]) # Error rate by service sum by (service) (rate({namespace=\"battlebots\"} | json | level=\"error\" [5m])) # Average match duration avg_over_time({service=\"matchmaker\"} | json | unwrap duration_ms [5m]) Integration with OTel Collector Loki integrates with the OpenTelemetry Collector via native OTLP endpoints or exporters.\nOTel Collector configuration (brief example):\nexporters: otlphttp/loki: endpoint: http://loki:3100/otlp service: pipelines: logs: receivers: [otlp] processors: [batch, resourcedetection] exporters: [otlphttp/loki] For comprehensive OTel Collector integration details, see the dedicated Loki OTLP Integration document.\nFurther Reading Official Documentation Grafana Loki Documentation - Official documentation home Loki Architecture - Detailed architecture overview Loki Components - Component reference Loki Deployment Modes - Deployment mode comparison LogQL Language - Query language reference Label Best Practices - Label design guidelines Cardinality Management - Cardinality considerations Configuration Reference - Full configuration documentation Installation and Setup Install Loki with Docker - Docker and Docker Compose setup Quick Start Guide - Getting started tutorial Helm Installation - Kubernetes Helm charts Configure Storage - Storage backend configuration Best Practices and Guides How Labels Work in Loki - Label design deep dive The Concise Guide to Grafana Loki Labels - Comprehensive label guide Loki 2.4 Simple Scalable Deployment - Simple scalable mode introduction Grafana Loki Architecture Guide - Architecture deep dive Storage and Integration Using MinIO with Loki - MinIO integration guide Loki and MinIO Configuration - MinIO setup tutorial Storage Configuration - Storage backend options Comparisons and Decision Making Loki vs Elasticsearch - Detailed comparison Grafana Loki vs ELK Stack - Use case comparison Loki vs ELK: A Light Alternative - Lightweight alternative perspective Community and Source Code Loki GitHub Repository - Source code and issues Loki Examples - Configuration examples Grafana Community Forums - Community discussions Related BattleBots Documentation OpenTelemetry Collector Overview - OTel Collector architecture OTel Collector Logs - Log handling in OTel Collector Loki OTLP Integration - Detailed OTLP integration guide User Journey 0001: POC - Observability requirements ","categories":"","description":"Comprehensive overview of Grafana Loki log aggregation system, covering architecture, deployment modes, and operational best practices.\n","excerpt":"Comprehensive overview of Grafana Loki log aggregation system, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/logs/loki/loki-overview/","tags":"","title":"Loki: Overview"},{"body":"Overview Metrics storage is essential for the BattleBots platform’s observability infrastructure, enabling:\nReal-time monitoring of battle events and game state Historical analysis of bot performance and system behavior Capacity planning and infrastructure optimization Alerting on critical system conditions Long-term trend analysis and reporting The metrics storage backend must handle time-series data at scale, support efficient querying, and integrate seamlessly with the OpenTelemetry Collector to provide a unified observability platform alongside logs and traces.\nWhy Metrics Storage Matters for BattleBots The BattleBots platform generates metrics across multiple dimensions:\nGame Metrics Battle Events: Damage calculations, bot actions, victory conditions Performance Metrics: Bot response times, action execution latency Game State: Active battles, queued matches, player counts Resource Utilization: CPU, memory, and network usage per bot container Infrastructure Metrics Container Metrics: Resource usage, restart counts, health checks Host Metrics: Node-level CPU, memory, disk, and network utilization Network Metrics: Request rates, latency distributions, error rates Kubernetes Metrics: Pod status, deployments, scaling events Observability Stack Metrics OpenTelemetry Collector: Pipeline throughput, batch sizes, export success rates Log Storage: Ingestion rates, query performance, storage utilization Trace Storage: Span ingestion, sampling rates, trace completeness Components Grafana Mimir Research on Grafana Mimir, a horizontally scalable, highly available, multi-tenant metrics storage system built for long-term Prometheus data storage.\nMimir transforms Prometheus from a single-server monitoring system into a distributed platform capable of handling over 1 billion active time series, providing:\nNative OTLP Support: Direct integration with OpenTelemetry Collector via OTLP over HTTP Horizontal Scalability: Independent scaling of write path, read path, and backend components Long-Term Storage: Object storage backend (S3, GCS, MinIO) enables months to years of retention Multi-Tenancy: Built-in tenant isolation with per-tenant limits and resource controls High Availability: Replication and distributed architecture eliminate single points of failure PromQL Compatibility: Full Prometheus query language support for dashboards and alerts Includes detailed analysis of:\nArchitecture components and deployment modes Native OTLP ingestion and OpenTelemetry Collector integration Object storage backends and retention policies Multi-tenancy and cardinality management Comparison with Prometheus, Thanos, and Cortex Production deployment and operational best practices BattleBots Integration Context For the BattleBots platform, metrics storage serves as the foundation for understanding system behavior and performance:\nReal-Time Monitoring Monitor active battles and player engagement in real-time Alert on critical conditions (bot container failures, API errors, resource exhaustion) Track game server health and availability Identify performance degradation before user impact Historical Analysis Analyze battle outcome patterns and bot performance trends Capacity planning based on player growth and peak usage patterns Cost optimization through resource utilization analysis Root cause analysis for system incidents Unified Observability Metric-to-trace correlation through shared labels and exemplars Jumping from metric anomalies to related distributed traces Linking metrics to logs for comprehensive debugging Grafana dashboards combining metrics, logs, and traces in single views Example Metrics for BattleBots Bot Performance:\n# Bot action latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) # Bot health over time avg(bot_health_points) by (battle_id, bot_id) Infrastructure Health:\n# Container resource utilization container_memory_usage_bytes{namespace=\"battlebots\", container=\"game-server\"} / container_spec_memory_limit_bytes{namespace=\"battlebots\", container=\"game-server\"} # API request rate and errors sum(rate(http_requests_total{service=\"battle-api\"}[5m])) by (status_code) Game State:\n# Active battles sum(battles_active{environment=\"production\"}) # Player queue depth avg(player_queue_length) by (region) Decision Context This research will inform the upcoming ADR-NNNN: Observability Stack Selection, which will determine the metrics storage backend for BattleBots. Key decision factors include:\nFunctional Fit: Does the solution meet metrics storage, query, and correlation requirements? Scalability: Can it handle expected growth from POC to production scale? Integration: How well does it integrate with OpenTelemetry Collector, Grafana, and other observability components? Operational Complexity: What is the operational burden for deployment, monitoring, and maintenance? Cost: What are the infrastructure and operational costs at POC and production scale? Multi-Tenancy: Does it support per-player or per-battle isolation if needed? Related Documentation R\u0026D Documentation Observability Overview - Parent observability analysis section OpenTelemetry Collector Analysis - Metrics collection and processing Loki Analysis - Log storage for correlation with metrics User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources Prometheus Documentation PromQL Query Language OpenTelemetry Metrics Specification Grafana Metrics Documentation Contributing These analysis documents are living documents that should be updated as:\nNew metrics storage solutions emerge or mature BattleBots observability requirements evolve Team members gain operational experience with metrics backends Best practices and patterns are discovered Comparative analysis reveals new insights Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Analysis of metrics storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\n","excerpt":"Analysis of metrics storage backends for the BattleBots observability …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/metrics/","tags":"","title":"Metrics Storage"},{"body":"Overview This document provides comprehensive guidance on integrating Grafana Mimir with the OpenTelemetry Collector, addressing two critical questions:\nDoes Mimir work with OTLP? → YES - Native OTLP ingestion since version 2.3.0 Can it be integrated with the OTel Collector? → YES - Full integration via otlphttp or prometheusremotewrite exporters The integration enables a vendor-neutral observability pipeline where the OpenTelemetry Collector collects metrics from instrumented applications and forwards them to Mimir for long-term storage, querying, and alerting.\nWhy OTLP Matters for Metrics OpenTelemetry Protocol (OTLP) is the native protocol of the OpenTelemetry project, designed as a vendor-neutral standard for telemetry data transmission. Using OTLP with Mimir provides:\nFuture-Proof: OTLP is becoming the industry standard for telemetry data Simplified Pipeline: No protocol translation required (OTel → OTLP → Mimir) Resource Attributes: OTel resource attributes preserved in target_info metric Unified Stack: Same protocol for logs (Loki), metrics (Mimir), and traces (Tempo) Vendor Independence: Easy migration between OTLP-compatible backends Mimir’s Position in the OTLP Ecosystem Mimir acts as an OTLP-compatible metrics backend, receiving metrics via:\nPrimary Path: OpenTelemetry Collector → OTLP/HTTP → Mimir /otlp/v1/metrics endpoint Alternative Path: OpenTelemetry Collector → Prometheus Remote Write → Mimir /api/v1/push endpoint Both paths are fully supported, with OTLP recommended by Grafana for new deployments.\nOTLP Support in Mimir Native OTLP Support: YES Status: Grafana Mimir has native OTLP ingestion support.\nEndpoint: /otlp/v1/metrics\nVersion History:\nv2.3.0 (September 2022): OTLP support introduced (experimental) v2.15.0 (January 2025): OTLP support matured (no longer experimental) Removed experimental -distributor.direct-otlp-translation-enabled flag Added support for lz4 compression Added support for integer exemplar values v3.0.0 (October 2025): Additional OTLP enhancements Experimental -distributor.otel-translation-strategy flag for metric name translation Experimental -distributor.otel-native-delta-ingestion for native delta metric ingestion Protocol Support:\nOTLP over HTTP: Primary protocol (recommended) Encoding: Protocol Buffers Compression: GZIP and lz4 OTLP Endpoint Configuration Endpoint URL The OTLP endpoint in Mimir is:\nhttp://\u003cmimir-endpoint\u003e/otlp/v1/metrics Important: OpenTelemetry Collector clients automatically append /v1/metrics to the base path, so you only need to configure:\nhttp://\u003cmimir-endpoint\u003e/otlp Example Requests Using curl:\n# Send OTLP metrics to Mimir curl -X POST http://mimir:8080/otlp/v1/metrics \\ -H \"Content-Type: application/x-protobuf\" \\ -H \"X-Scope-OrgID: tenant-123\" \\ --data-binary @metrics.pb Using OpenTelemetry Collector:\nexporters: otlphttp: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: tenant-123 OTLP Features Supported Metric Types:\n✅ Gauge: Point-in-time values ✅ Sum (Counter): Cumulative or delta monotonic sums ✅ Histogram: Distribution of values with buckets ⚠️ Exponential Histogram: Requires enabling Prometheus Native Histograms first ✅ Summary: Pre-computed quantiles (compatibility mode) Resource Attributes:\n✅ Promoted Attributes: Converted to Prometheus labels (e.g., service.name → service_name) ✅ Target Info: Non-promoted attributes stored in separate target_info metric ✅ Queryable: Use info() function or join queries to access resource attributes Exemplars:\n✅ Metric Exemplars: Link metrics to traces via trace_id and span_id ✅ Integer Values: Support for integer exemplar values (v2.15.0+) Compression:\n✅ GZIP: Standard compression ✅ lz4: Faster compression (v2.15.0+) OTLP vs. Prometheus Remote Write Both protocols are supported by Mimir. Here’s a detailed comparison:\nFeature OTLP Prometheus Remote Write Grafana Recommendation ✅ Recommended Alternative Resource Attributes ✅ Stored in target_info ❌ Lost during conversion Bandwidth Efficiency Moderate ✅ Better (remote write 2.0: 40% reduction) Native Protocol ✅ OpenTelemetry native Prometheus native Exponential Histograms Requires config N/A (not applicable) Protocol Maturity Mature (since v2.15.0) Very Mature Configuration Complexity Simple Simple Future-Proof ✅ Industry standard Prometheus ecosystem Official Recommendation from Grafana:\n“It’s recommended that you use the OpenTelemetry protocol (OTLP).”\nGrafana Mimir Documentation Why Use OTLP Over Prometheus Remote Write? Advantages of OTLP:\nResource Attributes Preserved: Mimir stores OTel resource attributes in target_info metric, enabling rich context Native OpenTelemetry: Direct protocol compliance without translation Future Development: Active OTLP feature development in each Mimir release Unified Observability: Same protocol for logs (Loki), metrics (Mimir), traces (Tempo) Vendor Neutrality: Easy migration between OTLP-compatible backends When to Use Prometheus Remote Write:\nBandwidth Critical: Remote write 2.0 saves 40% bandwidth vs. remote write 1.0 Existing Prometheus Infrastructure: Already using Prometheus remote write Compatibility Issues: If encountering specific OTLP compatibility issues (rare) For BattleBots: Use OTLP as the primary integration path for future-proof observability.\nKnown OTLP Limitations (Minor) Request Size Units: OTel Collector uses samples-per-batch, Mimir uses bytes-per-batch (alignment difficult) Exponential Histograms: Must enable Prometheus Native Histograms in Mimir first Out-of-Order Samples: No ordering guarantees (can cause issues with Prometheus’s ordered data expectations) Response Format: Error responses don’t fully comply with OTLP spec (returns plain string vs. Protobuf Status) These are minor edge cases that don’t affect typical deployments.\nOpenTelemetry Collector Configuration This section provides complete configuration examples for integrating the OpenTelemetry Collector with Mimir.\nArchitecture Overview graph LR subgraph \"Application\" App[Instrumented App\u003cbr/\u003eOpenTelemetry SDK] end subgraph \"Collection\" OTel[OpenTelemetry Collector] App --\u003e|OTLP| OTel end subgraph \"Processing\" OTel --\u003e Batch[Batch Processor] Batch --\u003e Resource[Resource Processor] end subgraph \"Storage\" Resource --\u003e|OTLP HTTP| Mimir[(Grafana Mimir)] end subgraph \"Visualization\" Grafana[Grafana] --\u003e Mimir end style OTel fill:#e1f5ff style Mimir fill:#fff4e1 style Grafana fill:#ffe0b2 Option 1: OTLP HTTP Exporter (Recommended) The otlphttp exporter sends metrics using the native OTLP over HTTP protocol.\nBasic Configuration receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: batch: send_batch_size: 8192 timeout: 10s exporters: otlphttp: endpoint: http://mimir:8080/otlp compression: gzip service: pipelines: metrics: receivers: [otlp] processors: [batch] exporters: [otlphttp] Production Configuration with Authentication extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] timeout: 5s override: false resource: attributes: - key: environment value: ${ENVIRONMENT} action: upsert - key: cluster value: ${CLUSTER_NAME} action: upsert exporters: otlphttp: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/otlp timeout: 30s compression: gzip retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 headers: X-Scope-OrgID: ${MIMIR_TENANT_ID} service: extensions: [basicauth] pipelines: metrics: receivers: [otlp] processors: [memory_limiter, resourcedetection, resource, batch] exporters: [otlphttp] Option 2: Prometheus Remote Write Exporter The prometheusremotewrite exporter sends metrics using the Prometheus Remote Write protocol.\nBasic Configuration receivers: otlp: protocols: http: endpoint: 0.0.0.0:4318 processors: batch: send_batch_size: 8192 timeout: 10s exporters: prometheusremotewrite: endpoint: http://mimir:8080/api/v1/push service: pipelines: metrics: receivers: [otlp] processors: [batch] exporters: [prometheusremotewrite] Production Configuration extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] resource: attributes: - key: environment value: ${ENVIRONMENT} action: upsert exporters: prometheusremotewrite: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/api/v1/push # External labels added to all metrics external_labels: cluster: ${CLUSTER_NAME} environment: ${ENVIRONMENT} # Resource to telemetry conversion resource_to_telemetry_conversion: enabled: true # Target info generation target_info: enabled: true # Retry configuration retry_on_failure: enabled: true initial_interval: 1s max_interval: 30s max_elapsed_time: 1800s # Queue configuration remote_write_queue: enabled: true queue_size: 10000 num_consumers: 5 # WAL (Write-Ahead Log) for durability wal: directory: /var/lib/otelcol/wal buffer_size: 300 truncate_frequency: 1m service: extensions: [basicauth] pipelines: metrics: receivers: [otlp] processors: [memory_limiter, resourcedetection, resource, batch] exporters: [prometheusremotewrite] Batch Processor Configuration The batch processor is CRITICAL for production deployments. It improves efficiency by batching metrics before export.\nRecommended Settings processors: batch: # Number of items to batch before sending send_batch_size: 8192 # Default, works for most cases # Maximum time to wait before sending (even if batch not full) timeout: 10s # Maximum batch size (safety valve) send_batch_max_size: 10000 # Maximum metadata keys per batch metadata_keys: 1000 # Maximum cardinality per metadata key metadata_cardinality_limit: 1000 Tuning Guidelines High-Throughput Deployments:\nprocessors: batch: send_batch_size: 16384 # 2x default timeout: 5s # Shorter timeout for lower latency send_batch_max_size: 20000 Low-Latency Requirements:\nprocessors: batch: send_batch_size: 1000 # Smaller batches timeout: 1s # Quick flush send_batch_max_size: 2000 Key Principles:\nAlways use batch processor before network-based exporters Place after memory_limiter in pipeline Balance latency vs. efficiency: Larger batches = better compression but higher latency Retry and Timeout Configuration Configure retry logic to handle transient failures:\nexporters: otlphttp: # Request timeout timeout: 30s # Retry configuration retry_on_failure: enabled: true initial_interval: 5s # Wait 5s before first retry max_interval: 30s # Cap exponential backoff at 30s max_elapsed_time: 300s # Give up after 5 minutes # Sending queue (buffer during retries) sending_queue: enabled: true num_consumers: 10 # Parallel export workers queue_size: 5000 # Buffer size Guidelines:\ninitial_interval: Start with 1-5 seconds (avoid retry storms) max_interval: Cap at 30-60 seconds (prevent infinite backoff) max_elapsed_time: 300s (5 min): Low-latency, loss-tolerant 1800s (30 min): Standard production 3600s+ (1+ hour): Critical data that cannot be lost num_consumers: More consumers = more parallel requests (ensure backend can handle load) queue_size: Balance memory vs. buffering capacity Resource Attribute Mapping OpenTelemetry resource attributes provide context about the source of metrics. Mimir handles these attributes through two mechanisms:\nPromoted Attributes Certain resource attributes are automatically converted to Prometheus labels:\nDefault Promoted Attributes:\nservice.namespace + service.name → job label service.instance.id → instance label Example:\n# OpenTelemetry resource attributes resource: service.namespace: \"battlebots\" service.name: \"battle-api\" service.instance.id: \"pod-abc123\" # Resulting Prometheus labels { job=\"battlebots/battle-api\", instance=\"pod-abc123\" } Target Info Metric Non-promoted resource attributes are stored in a separate target_info metric:\nExample:\n# OpenTelemetry resource attributes resource: service.name: \"battle-api\" service.version: \"v1.2.3\" k8s.namespace.name: \"battlebots\" k8s.pod.name: \"battle-api-abc123\" k8s.deployment.name: \"battle-api\" cloud.provider: \"aws\" cloud.region: \"us-east-1\" # target_info metric created target_info{ job=\"battle-api\", instance=\"pod-abc123\", service_version=\"v1.2.3\", k8s_namespace_name=\"battlebots\", k8s_pod_name=\"battle-api-abc123\", k8s_deployment_name=\"battle-api\", cloud_provider=\"aws\", cloud_region=\"us-east-1\" } 1 Querying with Resource Attributes Direct Query (promoted attributes):\nhttp_requests_total{job=\"battlebots/battle-api\"} Join Query (non-promoted attributes):\n# Join metric with target_info to access resource attributes http_requests_total * on(job, instance) group_left(k8s_namespace_name, k8s_pod_name) target_info Using info() Function (Prometheus 3.0+):\n# Simpler syntax for joining with target_info http_requests_total * info(target_info) Label Name Conversion Prometheus labels don’t support . or - characters. OpenTelemetry attributes are converted:\nservice.name → service_name k8s-cluster → k8s_cluster http.method → http_method Resource Processor for Attribute Transformation Use the resource processor to add, modify, or remove resource attributes:\nprocessors: resource: attributes: # Add new attribute - key: environment value: production action: insert # Update existing attribute - key: service.version value: v2.0.0 action: update # Insert or update (upsert) - key: cluster value: us-east-1-prod action: upsert # Rename attribute - key: cluster_name from_attribute: k8s.cluster.name action: insert # Delete attribute - key: sensitive.data action: delete # Extract with regex - key: environment pattern: ^(dev|staging|prod)-.*$ action: extract Resource Detection Processor Automatically detect resource attributes from the environment:\nprocessors: resourcedetection: # Ordered list of detectors (first match wins) detectors: [env, system, docker, gcp, ec2, azure, k8s] timeout: 5s override: false # Don't overwrite existing attributes # System detector configuration system: hostname_sources: [\"os\", \"dns\", \"cname\", \"lookup\"] # Docker detector docker: resource_attributes: host.name: enabled: true os.type: enabled: true # GCP detector gcp: resource_attributes: gcp.project.id: enabled: true cloud.platform: enabled: true cloud.region: enabled: true # Kubernetes detector k8s: resource_attributes: k8s.namespace.name: enabled: true k8s.pod.name: enabled: true k8s.deployment.name: enabled: true k8s.node.name: enabled: true Detected Attributes by Detector:\nDetector Attributes env Reads from OTEL_RESOURCE_ATTRIBUTES environment variable system host.name, host.id, host.arch, os.type docker host.name, os.type from Docker environment gcp cloud.provider, cloud.platform, cloud.region, gcp.project.id, gcp.gce.instance.id ec2 cloud.provider, cloud.platform, cloud.region, cloud.account.id, host.id, host.type azure cloud.provider, cloud.platform, cloud.region, azure.vm.name, azure.resourcegroup.name k8s k8s.namespace.name, k8s.pod.name, k8s.deployment.name, k8s.node.name, k8s.cluster.name Label Strategy and Cardinality Control Managing cardinality is critical when using OpenTelemetry with Mimir. High cardinality can cause performance issues and increased costs.\nUnderstanding Cardinality in OTel Context Every unique combination of metric name + label key-value pairs = one time series.\nLow Cardinality (Good):\n# Resource attributes service.name: \"battle-api\" # Limited number of services environment: \"production\" # 3 values: dev, staging, prod region: \"us-east-1\" # Limited AWS regions # Metric-level attributes http.method: \"GET\" # ~7 HTTP methods http.status_code: \"200\" # ~50 HTTP status codes # Total series per metric = 10 services × 3 environments × 5 regions × 7 methods × 50 status codes = 52,500 series (ACCEPTABLE) High Cardinality (Bad):\n# Adding unbounded attributes user.id: \"user-12345\" # Millions of users # New total series = 52,500 × 1,000,000 users = 52.5 billion series (UNSUSTAINABLE!) Cardinality Best Practices 1. Avoid Unbounded Resource Attributes:\nBad:\nresource: user.id: \"12345\" # Unbounded session.id: \"abc-def-ghi\" # Unbounded request.id: \"uuid-...\" # Unbounded timestamp: \"1634567890\" # Unbounded Good:\nresource: service.name: \"battle-api\" # Bounded environment: \"production\" # Bounded (3 values) region: \"us-east-1\" # Bounded (AWS regions) version: \"v1.2.3\" # Bounded (release versions) 2. Use Metric Attributes Sparingly:\nOpenTelemetry SDK allows setting attributes on individual metric data points. Use bounded sets only:\n// Good: Bounded attribute meter.NewInt64Counter(\"http.requests\", metric.WithDescription(\"HTTP requests\"), ).Add(ctx, 1, attribute.String(\"method\", \"GET\"), // ~7 values attribute.Int(\"status_code\", 200), // ~50 values ) // Bad: Unbounded attribute meter.NewInt64Counter(\"http.requests\").Add(ctx, 1, attribute.String(\"user_id\", userID), // Unbounded! ) 3. Drop High-Cardinality Attributes:\nUse the resource processor to remove problematic attributes:\nprocessors: resource: attributes: # Drop user-specific attributes - key: user.id action: delete - key: session.id action: delete - key: request.id action: delete 4. Aggregate Before Storing:\nFor user-specific metrics, aggregate at collection time:\n// Instead of per-user metrics: // user_requests{user_id=\"123\"} → High cardinality // Use aggregated metrics: // requests_by_service{service=\"api\"} → Low cardinality // Then query logs for user-specific debugging Attribute Transformation Strategies Strategy 1: Bounded Enumeration\nConvert unbounded values to bounded categories:\nprocessors: attributes: actions: # Convert specific status codes to categories - key: http.status_code action: update pattern: ^2\\d\\d$ value: \"2xx\" - key: http.status_code action: update pattern: ^4\\d\\d$ value: \"4xx\" - key: http.status_code action: update pattern: ^5\\d\\d$ value: \"5xx\" Strategy 2: Drop After Threshold\nOnly keep top N values, drop the rest:\nThis requires custom collector processing or accept all values with limits configured in Mimir:\n# Mimir limits limits: max_global_series_per_metric: 50000 # Cap per metric Recommended Labels for BattleBots Infrastructure Labels:\nresource: service.name: \"battle-api\" # Service name service.namespace: \"battlebots\" # Application namespace service.instance.id: \"pod-abc123\" # Pod/container ID service.version: \"v1.2.3\" # Release version deployment.environment: \"production\" # dev/staging/prod cloud.region: \"us-east-1\" # Geographic region k8s.cluster.name: \"us-east-1-prod\" # Cluster identifier k8s.namespace.name: \"battlebots\" # Kubernetes namespace Game-Specific Labels (metric-level attributes):\n# Battle events battle.type: \"team-deathmatch\" # Limited game modes bot.type: \"tank\" # Enumerable bot types game.region: \"us-east\" # Geographic game region # DO NOT USE AS LABELS: # battle.id: \"12345\" # If battles are long-lived and accumulate # player.id: \"user-abc\" # High cardinality # bot.id: \"bot-xyz\" # High cardinality if bots are per-player Estimated Cardinality:\nhttp_requests_total{ service_name: 10 services environment: 3 environments region: 5 regions method: 7 methods status_code: 50 codes endpoint: 50 endpoints battle_type: 5 game modes } = 1 × 10 × 3 × 5 × 7 × 50 × 50 × 5 = 13,125,000 series # Acceptable for Mimir Authentication and Multi-Tenancy Basic Authentication Use the basicauth extension for username/password authentication:\nextensions: basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} exporters: otlphttp: auth: authenticator: basicauth endpoint: http://mimir:8080/otlp service: extensions: [basicauth] pipelines: metrics: exporters: [otlphttp] Bearer Token Authentication Use the bearertoken extension for token-based auth:\nStatic Token:\nextensions: bearertoken: token: ${MIMIR_API_TOKEN} exporters: otlphttp: auth: authenticator: bearertoken endpoint: http://mimir:8080/otlp service: extensions: [bearertoken] pipelines: metrics: exporters: [otlphttp] Token from File (rotating tokens):\nextensions: bearertoken: filename: /var/run/secrets/mimir-token exporters: otlphttp: auth: authenticator: bearertoken endpoint: http://mimir:8080/otlp service: extensions: [bearertoken] pipelines: metrics: exporters: [otlphttp] Multi-Tenancy with X-Scope-OrgID Mimir uses the X-Scope-OrgID header for tenant identification:\nSingle Tenant:\nexporters: otlphttp: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-production\" Multiple Tenants (separate pipelines):\nexporters: otlphttp/tenant1: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-1\" otlphttp/tenant2: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-2\" service: pipelines: metrics/tenant1: receivers: [otlp] processors: [batch] exporters: [otlphttp/tenant1] metrics/tenant2: receivers: [otlp] processors: [batch] exporters: [otlphttp/tenant2] Dynamic Tenant Routing:\nFor dynamic tenant routing based on resource attributes, use the routing processor:\nprocessors: routing: from_attribute: tenant.id default_exporters: [otlphttp/default] table: - value: \"tenant-1\" exporters: [otlphttp/tenant1] - value: \"tenant-2\" exporters: [otlphttp/tenant2] exporters: otlphttp/tenant1: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-1\" otlphttp/tenant2: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"tenant-2\" otlphttp/default: endpoint: http://mimir:8080/otlp headers: X-Scope-OrgID: \"default\" service: pipelines: metrics: receivers: [otlp] processors: [routing, batch] exporters: [otlphttp/tenant1, otlphttp/tenant2, otlphttp/default] TLS Configuration Enable TLS for secure communication:\nexporters: otlphttp: endpoint: https://mimir.example.com/otlp tls: insecure: false # Verify server certificate cert_file: /path/to/client-cert.pem # Client certificate key_file: /path/to/client-key.pem # Client private key ca_file: /path/to/ca-cert.pem # CA certificate for server verification Mutual TLS (mTLS):\nexporters: otlphttp: endpoint: https://mimir.example.com/otlp tls: insecure: false cert_file: /path/to/client-cert.pem key_file: /path/to/client-key.pem ca_file: /path/to/ca-cert.pem min_version: \"1.2\" # Minimum TLS version max_version: \"1.3\" # Maximum TLS version Complete Configuration Example This example demonstrates a complete, production-ready OpenTelemetry Collector configuration for BattleBots integrating with Mimir.\n# OpenTelemetry Collector Configuration for BattleBots + Mimir # Production-ready configuration with OTLP HTTP exporter extensions: # Health check endpoint health_check: endpoint: 0.0.0.0:13133 # Memory ballast to reduce GC pressure memory_ballast: size_mib: 165 # 1/3 of memory_limiter limit_mib # Authentication for Mimir basicauth: client_auth: username: ${MIMIR_USERNAME} password: ${MIMIR_PASSWORD} receivers: # OTLP receiver for metrics from instrumented apps otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 # Prometheus receiver for scraping Prometheus exporters prometheus: config: scrape_configs: # Scrape OpenTelemetry Collector's own metrics - job_name: 'otel-collector' scrape_interval: 30s static_configs: - targets: ['localhost:8888'] # Scrape BattleBots services - job_name: 'battlebots' kubernetes_sd_configs: - role: pod namespaces: names: ['battlebots'] relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ processors: # Memory limiter (MUST be first in pipeline) memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 # 25% of limit_mib # Resource detection resourcedetection: detectors: [env, system, docker, gcp, ec2, k8s] timeout: 5s override: false # Resource attribute transformation resource: attributes: # Add deployment environment - key: deployment.environment value: ${ENVIRONMENT} action: upsert # Add cluster name - key: cluster.name value: ${CLUSTER_NAME} action: upsert # Add BattleBots platform identifier - key: platform value: battlebots action: upsert # Add version - key: version value: ${APP_VERSION} action: upsert # Remove sensitive attributes - key: host.id action: delete # Filter processor (optional - drop unwanted metrics) filter: metrics: exclude: match_type: strict metric_names: - unwanted_metric_1 - unwanted_metric_2 # Batch processor (CRITICAL for production) batch: send_batch_size: 8192 timeout: 10s send_batch_max_size: 10000 exporters: # Primary: OTLP HTTP to Mimir otlphttp: auth: authenticator: basicauth endpoint: ${MIMIR_ENDPOINT}/otlp timeout: 30s compression: gzip retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 300s sending_queue: enabled: true num_consumers: 10 queue_size: 5000 headers: X-Scope-OrgID: ${MIMIR_TENANT_ID} # Debug: Logging exporter (disable in production) logging: loglevel: info sampling_initial: 5 sampling_thereafter: 200 service: extensions: [health_check, memory_ballast, basicauth] # Collector's own telemetry telemetry: logs: level: info metrics: address: 0.0.0.0:8888 level: detailed pipelines: # Metrics pipeline metrics: receivers: [otlp, prometheus] processors: [memory_limiter, resourcedetection, resource, filter, batch] exporters: [otlphttp] # Optional: Separate pipeline for debugging # metrics/debug: # receivers: [otlp] # processors: [batch] # exporters: [logging] Environment Variables # Mimir connection export MIMIR_ENDPOINT=\"http://mimir-gateway:8080\" export MIMIR_USERNAME=\"battlebots-collector\" export MIMIR_PASSWORD=\"supersecret\" export MIMIR_TENANT_ID=\"battlebots-production\" # Application metadata export ENVIRONMENT=\"production\" export CLUSTER_NAME=\"us-east-1-prod\" export APP_VERSION=\"v1.2.3\" # Memory configuration export GOMEMLIMIT=\"410MiB\" # 80% of memory_limiter limit_mib Kubernetes Deployment apiVersion: v1 kind: ConfigMap metadata: name: otel-collector-config namespace: battlebots data: config.yaml: | # Paste complete configuration from above --- apiVersion: v1 kind: Secret metadata: name: mimir-credentials namespace: battlebots type: Opaque stringData: username: battlebots-collector password: supersecret --- apiVersion: apps/v1 kind: Deployment metadata: name: otel-collector namespace: battlebots spec: replicas: 3 selector: matchLabels: app: otel-collector template: metadata: labels: app: otel-collector spec: containers: - name: otel-collector image: otel/opentelemetry-collector-contrib:0.115.0 args: - --config=/conf/config.yaml env: - name: GOMEMLIMIT value: \"410MiB\" - name: MIMIR_ENDPOINT value: \"http://mimir-gateway.mimir.svc.cluster.local:8080\" - name: MIMIR_USERNAME valueFrom: secretKeyRef: name: mimir-credentials key: username - name: MIMIR_PASSWORD valueFrom: secretKeyRef: name: mimir-credentials key: password - name: MIMIR_TENANT_ID value: \"battlebots-production\" - name: ENVIRONMENT value: \"production\" - name: CLUSTER_NAME value: \"us-east-1-prod\" - name: APP_VERSION value: \"v1.2.3\" resources: limits: memory: 512Mi cpu: 500m requests: memory: 256Mi cpu: 200m ports: - containerPort: 4317 # OTLP gRPC name: otlp-grpc - containerPort: 4318 # OTLP HTTP name: otlp-http - containerPort: 8888 # Metrics name: metrics - containerPort: 13133 # Health check name: health livenessProbe: httpGet: path: / port: health readinessProbe: httpGet: path: / port: health volumeMounts: - name: config mountPath: /conf volumes: - name: config configMap: name: otel-collector-config --- apiVersion: v1 kind: Service metadata: name: otel-collector namespace: battlebots spec: selector: app: otel-collector ports: - name: otlp-grpc port: 4317 targetPort: 4317 - name: otlp-http port: 4318 targetPort: 4318 - name: metrics port: 8888 targetPort: 8888 Docker Compose Example version: '3.8' services: otel-collector: image: otel/opentelemetry-collector-contrib:0.115.0 command: [\"--config=/etc/otelcol/config.yaml\"] environment: - GOMEMLIMIT=410MiB - MIMIR_ENDPOINT=http://mimir:8080 - MIMIR_USERNAME=battlebots - MIMIR_PASSWORD=supersecret - MIMIR_TENANT_ID=battlebots - ENVIRONMENT=development - CLUSTER_NAME=local - APP_VERSION=v1.0.0 ports: - \"4317:4317\" # OTLP gRPC - \"4318:4318\" # OTLP HTTP - \"8888:8888\" # Metrics - \"13133:13133\" # Health check volumes: - ./otel-config.yaml:/etc/otelcol/config.yaml networks: - battlebots mimir: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] ports: - \"8080:8080\" volumes: - ./mimir-config.yaml:/etc/mimir.yaml - mimir-data:/data networks: - battlebots grafana: image: grafana/grafana:latest ports: - \"3000:3000\" environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin networks: - battlebots networks: battlebots: volumes: mimir-data: Troubleshooting Common Issues and Solutions 1. Connection Refused / Network Errors Symptoms:\nfailed to export metrics: connection refused Diagnosis:\nVerify Mimir is running and reachable:\ncurl http://mimir:8080/ready Check network connectivity:\n# From collector pod/container nc -zv mimir 8080 Verify endpoint configuration:\n# Ensure correct endpoint format endpoint: http://mimir:8080/otlp # Correct # NOT: http://mimir:8080/otlp/v1/metrics (client auto-appends) Solutions:\nFix network policies/firewall rules Verify Kubernetes service DNS resolution Check load balancer configuration Ensure Mimir port 8080 is exposed 2. Authentication Failures Symptoms:\nfailed to export metrics: 401 Unauthorized failed to export metrics: 403 Forbidden Diagnosis:\nCheck credentials in environment variables Verify basicauth extension configuration Test authentication manually: curl -u username:password http://mimir:8080/ready Solutions:\n# Verify basicauth configuration extensions: basicauth: client_auth: username: ${MIMIR_USERNAME} # Check env var set password: ${MIMIR_PASSWORD} # Check env var set exporters: otlphttp: auth: authenticator: basicauth # Must reference extension service: extensions: [basicauth] # Must be listed here 3. Multi-Tenancy Header Issues Symptoms:\nfailed to export metrics: no org id Solution:\nexporters: otlphttp: headers: X-Scope-OrgID: \"your-tenant-id\" # Must include header Or disable multi-tenancy in Mimir:\n# mimir-config.yaml multitenancy_enabled: false 4. Invalid Metric Names or Labels Symptoms:\nerr-mimir-metric-name-invalid err-mimir-label-invalid Cause: Metric names must match [a-zA-Z_:][a-zA-Z0-9_:]*\nSolution: Use the metricstransform processor to rename metrics:\nprocessors: metricstransform: transforms: - include: .* match_type: regexp action: update operations: # Replace invalid characters with underscores - action: update_label label: \"invalid-label\" new_label: \"invalid_label\" 5. High Memory Usage / OOM Kills Symptoms:\nCollector pod/container killed with OOM High memory usage in metrics Diagnosis:\n# Monitor collector memory process_runtime_go_mem_heap_alloc_bytes{job=\"otel-collector\"} Solutions:\nTune memory_limiter:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 # Adjust based on container limit spike_limit_mib: 128 # 25% of limit Set GOMEMLIMIT (80% of container memory):\nenv: - name: GOMEMLIMIT value: \"410MiB\" # For 512Mi container limit Reduce batch sizes:\nprocessors: batch: send_batch_size: 4096 # Reduce from 8192 send_batch_max_size: 5000 # Reduce from 10000 Increase num_consumers:\nexporters: otlphttp: sending_queue: num_consumers: 20 # More parallel exports 6. Metrics Not Appearing in Mimir Diagnosis Checklist:\nCheck collector logs for export errors:\nkubectl logs -f deployment/otel-collector Verify metrics received by collector:\n# Check collector's own metrics otelcol_receiver_accepted_metric_points{receiver=\"otlp\"} otelcol_exporter_sent_metric_points{exporter=\"otlphttp\"} Check Mimir ingester logs:\nkubectl logs -f deployment/mimir-ingester Query Mimir directly:\ncurl -H \"X-Scope-OrgID: your-tenant\" \\ http://mimir:8080/prometheus/api/v1/query?query=up Common Causes:\nMetrics filtered out by filter processor Batch processor holding data (check timeout) Invalid metric names (check for errors in logs) Wrong tenant ID in query vs. export 7. Cardinality Limit Errors Symptoms:\nerr-mimir-max-series-per-metric err-mimir-max-series-per-user Diagnosis:\n# Check active series cortex_ingester_active_series # Check per-metric cardinality topk(10, count by (__name__) ({__name__=~\".+\"})) Solutions:\nIdentify high-cardinality metrics:\n# Use Mimir's cardinality analysis API curl -H \"X-Scope-OrgID: tenant\" \\ http://mimir:8080/prometheus/api/v1/cardinality/label_names Drop high-cardinality labels:\nprocessors: resource: attributes: - key: user_id action: delete - key: request_id action: delete Increase Mimir limits (temporary):\n# Mimir config limits: max_global_series_per_user: 10000000 # Increase limit max_global_series_per_metric: 100000 # Increase limit 8. Slow Query Performance Symptoms:\nQueries timeout High query latency Solutions:\nEnable caching in Mimir:\n# Mimir config query_frontend: results_cache: backend: memcached Reduce query time range:\n# Instead of querying 30 days: rate(http_requests_total[30d]) # Query smaller range: rate(http_requests_total[1h]) Optimize PromQL queries:\n# Inefficient sum(rate(metric[5m])) # Efficient (specify labels) sum(rate(metric{job=\"api\"}[5m])) by (status) Monitoring Collector Health Key Metrics to Monitor:\n# Successful exports rate(otelcol_exporter_sent_metric_points{exporter=\"otlphttp\"}[5m]) # Failed exports rate(otelcol_exporter_send_failed_metric_points{exporter=\"otlphttp\"}[5m]) # Batch processor metrics otelcol_processor_batch_batch_send_size otelcol_processor_batch_timeout_trigger_send # Memory limiter backpressure rate(otelcol_processor_refused_metric_points{processor=\"memory_limiter\"}[5m]) # Queue depth otelcol_exporter_queue_size{exporter=\"otlphttp\"} Recommended Alerts:\ngroups: - name: otel-collector-alerts rules: # Export failures - alert: OTelCollectorExportFailures expr: | rate(otelcol_exporter_send_failed_metric_points[5m]) \u003e 0 for: 5m annotations: summary: \"OpenTelemetry Collector failing to export metrics\" # Memory pressure - alert: OTelCollectorMemoryPressure expr: | rate(otelcol_processor_refused_metric_points[5m]) \u003e 0 for: 5m annotations: summary: \"OpenTelemetry Collector under memory pressure\" # Queue filling up - alert: OTelCollectorQueueFull expr: | otelcol_exporter_queue_size / otelcol_exporter_queue_capacity \u003e 0.8 for: 10m annotations: summary: \"OpenTelemetry Collector export queue filling up\" BattleBots Integration Points Complete Observability Pipeline graph TB subgraph \"BattleBots Services\" Bot[Bot Containers\u003cbr/\u003eOTel SDK] Game[Game Servers\u003cbr/\u003eOTel SDK] API[Battle API\u003cbr/\u003eOTel SDK] end subgraph \"Kubernetes\" K8s[kube-state-metrics] Node[node-exporter] end subgraph \"OpenTelemetry Collector\" OTLP[OTLP Receiver\u003cbr/\u003e:4317 / :4318] Prom[Prometheus Receiver] Bot --\u003e OTLP Game --\u003e OTLP API --\u003e OTLP K8s --\u003e Prom Node --\u003e Prom OTLP --\u003e ResDetect[Resource Detection] Prom --\u003e ResDetect ResDetect --\u003e ResTrans[Resource Transform] ResTrans --\u003e Batch[Batch Processor] Batch --\u003e Export[OTLP HTTP Exporter] end subgraph \"Grafana Mimir\" Export --\u003e|OTLP/HTTP| Dist[Distributor] Dist --\u003e Ing[Ingesters] Ing --\u003e ObjStore[(Object Storage)] Grafana[Grafana] --\u003e QF[Query Frontend] QF --\u003e Querier[Querier] Querier --\u003e Ing Querier --\u003e SG[Store Gateway] SG --\u003e ObjStore end style OTLP fill:#e1f5ff style Export fill:#e1f5ff style Dist fill:#fff4e1 style Grafana fill:#ffe0b2 Example Metrics for BattleBots Bot Action Latency (from OpenTelemetry SDK):\n// In bot implementation histogram, _ := meter.Int64Histogram( \"bot.action.duration\", metric.WithDescription(\"Bot action execution time\"), metric.WithUnit(\"ms\"), ) histogram.Record(ctx, durationMs, attribute.String(\"action\", \"attack\"), attribute.String(\"bot.type\", \"tank\"), ) Query in Mimir (after OTLP export):\n# 95th percentile attack latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) API Request Metrics:\n// In Battle API counter, _ := meter.Int64Counter( \"http.server.requests\", metric.WithDescription(\"HTTP requests\"), ) counter.Add(ctx, 1, attribute.String(\"http.method\", \"POST\"), attribute.String(\"http.route\", \"/api/v1/battles\"), attribute.Int(\"http.status_code\", 201), ) Query in Mimir:\n# Request rate by endpoint and status sum(rate(http_server_requests[5m])) by (http_route, http_status_code) # Error rate sum(rate(http_server_requests{http_status_code=~\"5..\"}[5m])) / sum(rate(http_server_requests[5m])) Resource Attribute Examples for BattleBots # Detected by resourcedetection processor resource: # Kubernetes attributes k8s.namespace.name: \"battlebots\" k8s.pod.name: \"battle-api-abc123\" k8s.deployment.name: \"battle-api\" k8s.node.name: \"node-us-east-1a\" # Cloud attributes cloud.provider: \"aws\" cloud.platform: \"aws_ec2\" cloud.region: \"us-east-1\" cloud.availability_zone: \"us-east-1a\" # Service attributes (from OTel SDK) service.name: \"battle-api\" service.namespace: \"battlebots\" service.version: \"v1.2.3\" service.instance.id: \"pod-abc123\" # Added by resource processor deployment.environment: \"production\" cluster.name: \"us-east-1-prod\" platform: \"battlebots\" Example PromQL Queries with Resource Attributes Query with promoted attributes:\n# All metrics from battle-api service {job=\"battlebots/battle-api\"} Query with target_info join:\n# Metrics filtered by Kubernetes deployment http_server_requests * on(job, instance) group_left(k8s_deployment_name) target_info{k8s_deployment_name=\"battle-api\"} Using info() function (Prometheus 3.0+):\n# Simpler syntax http_server_requests{job=\"battlebots/battle-api\"} * info(target_info) BattleBots Dashboards Request Rate Dashboard:\n{ \"title\": \"BattleBots API Request Rate\", \"targets\": [ { \"expr\": \"sum(rate(http_server_requests{job=~\\\"battlebots/.*\\\"}[5m])) by (http_route)\" } ] } Bot Performance Dashboard:\n{ \"title\": \"Bot Action Latency (p95)\", \"targets\": [ { \"expr\": \"histogram_quantile(0.95, sum(rate(bot_action_duration_bucket[5m])) by (bot_type, action, le))\" } ] } Linked Dashboard (Metrics → Traces):\nClick on metric spike in Grafana Grafana shows exemplars (trace IDs embedded in metrics) Click exemplar → Opens trace in Tempo Trace shows detailed spans with logs Further Reading Official Documentation Configure OpenTelemetry Collector for Mimir - Official integration guide OTLP Format Considerations - OTLP best practices Mimir OTLP Endpoint - API reference Prometheus Resource Attribute Promotion - How resource attributes become labels OpenTelemetry Collector Documentation OTLP HTTP Exporter - Official exporter docs Prometheus Remote Write Exporter - Alternative exporter Batch Processor - Batching guide Resource Detection Processor - Attribute detection Resource Processor - Attribute transformation Guides and Tutorials OpenTelemetry at Grafana Labs 2025 - Latest updates Using Prometheus as OpenTelemetry Backend - Prometheus perspective Mastering the Batch Processor - Deep dive Mastering the Memory Limiter - Prevent OOM OpenTelemetry Processors Best Practices - Configuration tips Troubleshooting OpenTelemetry Collector Troubleshooting - Official guide Mimir Runbooks - Operational guides Using Authenticator Extension - Auth setup Performance and Optimization Prometheus Remote Write Tuning - Optimize ingestion High Cardinality Management - Cardinality strategies Metric Cardinality Explained - Understanding cardinality Community Resources OpenTelemetry Community - Get help, contribute Grafana Community Forum - Ask questions CNCF OpenTelemetry Project - Project homepage ","categories":"","description":"Deep dive into Grafana Mimir's native OTLP support and integration with the OpenTelemetry Collector, including configuration examples, best practices, and troubleshooting guidance.\n","excerpt":"Deep dive into Grafana Mimir's native OTLP support and integration …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/metrics/mimir/mimir-otlp-integration/","tags":"","title":"Mimir: OTLP and OpenTelemetry Collector Integration"},{"body":"Overview Grafana Mimir is a horizontally scalable, highly available, multi-tenant, long-term storage solution for Prometheus metrics. It transforms Prometheus’s single-server architecture into a distributed microservices platform capable of handling over 1 billion active time series with unlimited retention backed by object storage.\nWhat is Mimir? Mimir takes the Prometheus Time Series Database (TSDB) and splits it into microservices, creating a distributed system where each component can scale independently. While Prometheus excels at real-time monitoring on a single machine, Mimir extends this capability to enterprise scale with:\nHorizontal Scalability: Scale from thousands to billions of active time series Long-Term Storage: Store metrics for months or years using cost-effective object storage High Availability: Built-in replication and distributed architecture eliminate single points of failure Multi-Tenancy: Isolated metrics per tenant with per-tenant resource limits Prometheus Compatibility: 100% PromQL compatibility ensures existing queries and dashboards work unchanged Core Innovation Mimir’s key innovation is separating Prometheus into write path, read path, and backend components, each independently scalable:\nWrite Path (Distributor → Ingester): Handles metric ingestion with validation and replication Read Path (Query-Frontend → Querier → Store-Gateway): Executes queries across recent and historical data Backend (Compactor, Ruler): Background processing for storage optimization and alerting This separation enables scaling write throughput independently from query performance, and both independently from long-term storage management.\nRelationship to Prometheus and Cortex Prometheus Foundation: Mimir uses the Prometheus TSDB format and PromQL query language, providing seamless migration paths and familiar operations.\nCortex Successor: Mimir began as a fork of Cortex in March 2022 when Grafana Labs stopped contributing to Cortex. Mimir inherits Cortex’s distributed architecture while adding:\nReduced operational complexity through monolithic deployment mode Split-and-merge compactor that overcomes TSDB’s 64GB index limit Performance optimizations and simplified configuration Active development and feature additions Recommendation: Always choose Mimir over Cortex for new deployments. Cortex is in maintenance mode with minimal active development.\nKey Concepts Blocks Storage Architecture Mimir uses a blocks-based storage system derived from Prometheus TSDB:\nTSDB Blocks Time series data is broken into fixed-time blocks (default: 2 hours) containing:\nChunks: Highly compressed time series sample data (~1.5 bytes per sample) Index: Inverted index mapping metric names and labels to time series Meta.json: Block metadata including time range, statistics, and compaction level Block Lifecycle Creation: Ingesters create 2-hour blocks from in-memory data and upload to object storage Compaction: Compactor merges small blocks into larger, optimized blocks (2h → 12h → 24h) Querying: Queriers fetch recent data from ingesters, historical data from store-gateways Retention: Compactor deletes blocks older than configured retention period Cleanup: Soft-deleted blocks removed after deletion delay (default: 12 hours) Storage Backends Mimir requires object storage for long-term block storage:\nAmazon S3 or S3-compatible services (MinIO, Ceph, etc.) Google Cloud Storage (GCS) Microsoft Azure Blob Storage OpenStack Swift Object storage provides:\nDurability: Built-in replication and fault tolerance Cost-Effectiveness: ~$0.02-0.03/GB/month vs. ~$0.10-0.20/GB/month for SSD Unlimited Capacity: No practical storage limits Geographic Distribution: Multi-region replication for disaster recovery Time Series and Cardinality Time Series Definition A time series is uniquely identified by a metric name and a set of label key-value pairs:\nhttp_requests_total{method=\"GET\", endpoint=\"/api/battles\", status=\"200\"} This creates one time series. Each unique combination of labels creates a separate time series.\nCardinality Cardinality is the number of unique time series (distinct label combinations). High cardinality occurs when labels have many possible values:\nLow Cardinality (Good):\nhttp_requests{method=\"GET\"} # method has ~10 values (GET, POST, PUT, DELETE, etc.) http_requests{status=\"200\"} # status has ~50 values (HTTP status codes) High Cardinality (Problematic):\nhttp_requests{user_id=\"12345\"} # user_id could have millions of values http_requests{request_id=\"abc123\"} # request_id has infinite possible values Best Practice: Avoid labels with unbounded values (UUIDs, timestamps, user IDs, email addresses). Use labels with bounded, enumerable values (service names, environments, HTTP methods, status codes).\nMimir’s Scale: Tested at 1 billion active series, real-world deployments at 500 million series.\nTenants and Tenant Isolation Multi-Tenancy by Default Mimir is multi-tenant by default. Each request must include a tenant ID via the X-Scope-OrgID HTTP header:\ncurl -H \"X-Scope-OrgID: tenant-123\" \\ http://mimir:8080/prometheus/api/v1/query?query=up Tenant Isolation Mechanisms Automatic Creation: Tenants created on first write (no pre-registration needed) Data Segregation: Complete separation of data between tenants in object storage Resource Limits: Per-tenant limits for ingestion rate, active series, query concurrency Query Isolation: Tenants can only query their own data No Authentication: Mimir trusts the X-Scope-OrgID header; add authentication layer (reverse proxy/API gateway) for production Tenant Federation Query across multiple tenants using pipe-separated tenant IDs:\nX-Scope-OrgID: tenant-1|tenant-2|tenant-3 This enables cross-tenant analytics and aggregation while maintaining isolation for writes.\nDisabling Multi-Tenancy For single-tenant deployments, disable multi-tenancy:\nmultitenancy_enabled: false All requests use a default tenant ID without requiring the X-Scope-OrgID header.\nPromQL Compatibility Mimir provides 100% PromQL (Prometheus Query Language) compatibility:\nAll Prometheus query functions supported Recording rules and alerting rules work unchanged Grafana dashboards require no modifications Existing Prometheus alerts can be migrated directly Example Queries:\n# Instant query: Current values up{job=\"battlebots-api\"} # Range query: Historical data rate(http_requests_total[5m]) # Aggregation: Summary across labels sum(rate(http_requests_total[5m])) by (status_code) # Histogram quantiles: Latency percentiles histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service) ) Architecture Components Mimir’s microservices architecture comprises multiple horizontally scalable components that operate independently and in parallel.\nComponent Overview graph TB subgraph \"Write Path\" Prom[Prometheus\u003cbr/\u003eRemote Write] --\u003e Dist[Distributor] OTel[OpenTelemetry\u003cbr/\u003eCollector OTLP] --\u003e Dist Dist --\u003e Ing1[Ingester 1] Dist --\u003e Ing2[Ingester 2] Dist --\u003e Ing3[Ingester 3] end subgraph \"Storage\" Ing1 --\u003e ObjStore[(Object Storage\u003cbr/\u003eS3/GCS/MinIO)] Ing2 --\u003e ObjStore Ing3 --\u003e ObjStore Compact[Compactor] --\u003e ObjStore end subgraph \"Read Path\" Client[Grafana/API] --\u003e QF[Query Frontend] QF --\u003e QS[Query Scheduler] QS --\u003e Q1[Querier 1] QS --\u003e Q2[Querier 2] Q1 --\u003e Ing1 Q1 --\u003e Ing2 Q1 --\u003e Ing3 Q2 --\u003e Ing1 Q2 --\u003e Ing2 Q2 --\u003e Ing3 Q1 --\u003e SG[Store Gateway] Q2 --\u003e SG SG --\u003e ObjStore end subgraph \"Backend\" Ruler[Ruler] --\u003e QF Ruler --\u003e SG end style Dist fill:#e1f5ff style Ing1 fill:#fff4e1 style Ing2 fill:#fff4e1 style Ing3 fill:#fff4e1 style ObjStore fill:#e8f5e9 style QF fill:#f3e5f5 style Q1 fill:#f3e5f5 style Q2 fill:#f3e5f5 Write Path Components Distributor Role: Entry point for the write path; receives and validates incoming metrics.\nKey Functions:\nReceives write requests from Prometheus remote write or OTLP endpoints Validates metric format and labels (must match [a-zA-Z_:][a-zA-Z0-9_:]*) Enforces per-tenant rate limits and metadata limits Shards incoming time series across ingesters based on consistent hashing Replicates data to multiple ingesters (default: 3 replicas) Characteristics:\nStateless: Can be scaled horizontally without coordination CPU-Bound: Scales with sample ingestion rate Resource Estimate: 1 core per 25,000 samples/second Configuration Example:\ndistributor: ring: kvstore: store: memberlist # Service discovery pool: health_check_ingesters: true Ingester Role: Writes incoming time series to long-term storage; serves recent data for queries.\nKey Functions:\nReceives samples from distributors and appends to per-tenant TSDB on local disk Writes samples to Write-Ahead Log (WAL) for crash recovery Compacts in-memory samples into TSDB blocks (default: every 2 hours) Uploads newly created blocks to object storage Serves queries for recent data (within ingester retention window) Participates in hash ring for data sharding and replication Characteristics:\nStateful: Stores active series in memory and on local disk Memory-Bound: Scales with number of active series Resource Estimates: CPU: 1 core per 300,000 in-memory series Memory: 2.5 GB per 300,000 in-memory series Disk: 5 GB per 300,000 in-memory series (for WAL and blocks) Recommended Limits:\nConservative: Up to 1.5 million series per ingester Maximum: Up to 5 million series per ingester (with sufficient memory) Configuration Example:\ningester: ring: replication_factor: 3 # Number of ingester replicas kvstore: store: memberlist blocks_storage: tsdb: dir: /data/tsdb # Local TSDB directory block_ranges_period: [2h] # Block creation interval retention_period: 6h # Keep blocks locally for 6 hours Read Path Components Query-Frontend Role: Receives and optimizes PromQL queries before dispatching to queriers.\nKey Functions:\nProvides the same HTTP API as Prometheus (/prometheus/api/v1/query, etc.) Splits large time-range queries into smaller sub-queries (query splitting) Caches query results to avoid re-computation (query result caching) Shards queries across time series to enable parallel execution (query sharding) Dispatches queries to queriers via query scheduler for better load distribution Retries failed queries automatically Characteristics:\nStateless: Can be scaled horizontally CPU-Bound: Scales with query rate Resource Estimate: 1 core per 250 queries/second Configuration Example:\nquery_frontend: results_cache: backend: memcached memcached: addresses: memcached:11211 split_queries_by_interval: 24h # Split queries into 24h chunks Query-Scheduler (Optional) Role: Intermediary between query-frontend and queriers for better queue management.\nKey Functions:\nReceives queries from query-frontends and maintains a queue Dispatches queries to available queriers Provides fair scheduling across tenants (prevents single tenant monopolizing queriers) Enables scaling query-frontends independently from queriers Characteristics:\nStateless: Lightweight component Resource Estimate: 1 core per 500 queries/second Configuration Example:\nquery_scheduler: max_outstanding_requests_per_tenant: 100 Querier Role: Executes PromQL queries by fetching data from ingesters and store-gateways.\nKey Functions:\nReceives query requests from query-frontend or query-scheduler Fetches recent data (within retention window) from ingesters Fetches historical data (older than retention window) from store-gateways Merges data from multiple sources and evaluates PromQL expression Returns results to query-frontend Characteristics:\nStateless: Can be scaled horizontally CPU + Memory Bound: Scales with query complexity and time range Resource Estimate: 1 core per 10 queries/second (assumes ~100ms average latency) Configuration Example:\nquerier: max_concurrent: 20 # Maximum concurrent queries per querier timeout: 2m # Query timeout query_ingesters_within: 13h # Query ingesters for data within 13h query_store_after: 12h # Query store-gateway for data older than 12h Backend Components Store-Gateway Role: Provides access to historical blocks stored in object storage.\nKey Functions:\nSynchronizes the list of blocks from object storage (bucket index) Downloads and memory-maps index-header files for fast block querying Serves queries from queriers and rulers for historical data Implements block-level sharding (each store-gateway responsible for subset of blocks) Downloads only necessary portions of blocks (chunks and index sections), not entire blocks Characteristics:\nStateful: Memory-maps index-headers, participates in hash ring Disk I/O Bound: Benefits from SSD for index-header operations Resource Estimates: CPU: 1 core per 10 queries/second Memory: 1 GB per 10 queries/second Disk: 13 GB per 1 million active series (for index-header files) Configuration Example:\nstore_gateway: sharding_ring: replication_factor: 3 kvstore: store: memberlist blocks_storage: bucket_store: sync_dir: /data/tsdb-sync index_cache: backend: memcached memcached: addresses: memcached:11211 Compactor Role: Compacts and optimizes TSDB blocks; manages retention and cleanup.\nKey Functions:\nVertical Compaction: Merges blocks from same tenant covering same time range Deduplicates replicated samples (from replication factor \u003e 1) Reduces index size and chunk overhead Horizontal Compaction: Combines blocks across adjacent time periods (2h → 12h → 24h) Split-and-Merge Compaction: For large tenants (\u003e20M series), splits compaction into shards to overcome TSDB 64GB index limit Maintains per-tenant bucket index (metadata about all blocks) Deletes blocks older than configured retention period Implements two-stage deletion: soft delete (mark) → hard delete (remove after delay) Characteristics:\nStateful: Participates in hash ring for per-tenant compaction ownership I/O Bound: Downloads source blocks, writes compacted blocks Resource Estimates: CPU: 1 core per compactor instance Memory: 4 GB per instance Disk: 300 GB per instance (for downloading/uploading blocks) Scaling Guideline: 1 compactor instance per 20 million active series\nConfiguration Example:\ncompactor: data_dir: /data/compactor compaction_interval: 30m block_ranges: [2h, 12h, 24h] # Compaction levels sharding_ring: kvstore: store: memberlist # For large tenants (\u003e20M series) split_and_merge_shards: 4 # Number of shards split_groups: 4 # Number of groups (match shards or use next power of 2) Ruler Role: Evaluates Prometheus recording and alerting rules.\nKey Functions:\nExecutes Prometheus recording rules (pre-compute expensive queries) Evaluates alerting rules and sends alerts to Alertmanager Supports multi-tenant rules (per-tenant rule groups) Stores recording rule results back to Mimir via remote write Uses store-gateway for querying when evaluating rules Characteristics:\nStateful: Participates in hash ring for rule group sharding CPU Bound: Scales with number and complexity of rules Configuration Example:\nruler: enable_api: true # Enable ruler API for rule management rule_path: /data/rules ring: kvstore: store: memberlist ruler_storage: backend: s3 s3: bucket_name: mimir-ruler endpoint: s3.amazonaws.com Deployment Modes Mimir supports three deployment modes with different trade-offs between simplicity and scalability.\nComparison Table Feature Monolithic Read-Write Microservices Complexity Low Medium High Scalability Limited (all together) Medium (3 groups) Maximum (per-component) Resource Efficiency Lower (over-provisioning) Medium Highest (fine-grained) Failure Isolation Single process failure Tier-level failure Component-level isolation Ideal Scale \u003c1M series 1-10M series 10M+ series Operational Overhead Minimal Medium High Deployment Tool Docker/VM Kubernetes Kubernetes + Helm Monolithic Mode Architecture: All Mimir components run in a single process.\nConfiguration:\ntarget: all # Run all components in one process # Or via environment variable: # MIMIR_MODE=all Characteristics:\nSimplest deployment model with lowest operational overhead All components scale together (cannot scale independently) Single binary to deploy and monitor Suitable for development, testing, and small production deployments High Availability: Deploy multiple -target=all instances with shared object storage:\nEach instance runs full component stack Ingesters replicate data across instances (default: 3x replication) Queriers query all ingesters and merge results Provides HA without microservices complexity Resource Requirements:\nMemory: Sized for peak ingester + querier memory needs CPU: Sum of all component CPU needs Disk: Sized for WAL and local TSDB blocks When to Use:\nDevelopment and testing environments POC deployments Production deployments with \u003c1M active series Teams preferring operational simplicity over granular scalability When total resource requirements fit on a single machine (vertically scaled) Limitations:\nCannot scale write path independently from read path Resource-intensive queries impact ingestion performance Maximum scale limited by largest available machine Not supported in Jsonnet deployment tooling Example Docker Compose Configuration:\nversion: '3.8' services: mimir: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] ports: - \"9009:9009\" volumes: - ./mimir.yaml:/etc/mimir.yaml - mimir-data:/data volumes: mimir-data: Read-Write Mode (Experimental) Architecture: Three-tier deployment separating read, write, and backend responsibilities.\nTiers:\nWrite Tier: Distributors + Ingesters (handles data ingestion) Read Tier: Query-Frontends + Queriers (handles queries) Backend Tier: Store-Gateways + Compactors + Rulers (background processing) Configuration:\n# Write tier target: write # Read tier target: read # Backend tier target: backend Characteristics:\nSimpler than full microservices (3 tiers vs. 7+ components) Independent scaling of read vs. write workloads Logical grouping of related components Requires multi-zone ingesters and store-gateways When to Use:\nMedium-scale deployments (1-10M active series) Organizations wanting simpler architecture than microservices Workloads with varying read vs. write loads Teams with some distributed systems experience Limitations:\nLess granular scaling than microservices mode Currently experimental (use with caution in production) Cannot scale individual components within a tier Scaling Example:\nWrite-heavy workload: Scale write tier (more distributors + ingesters) Query-heavy workload: Scale read tier (more query-frontends + queriers) Storage optimization: Scale backend tier (more compactors) Microservices Mode Architecture: Each component runs as a separate process/deployment.\nConfiguration: Each process invoked with specific -target parameter:\n# Distributor mimir -target=distributor -config.file=mimir.yaml # Ingester mimir -target=ingester -config.file=mimir.yaml # Query-frontend mimir -target=query-frontend -config.file=mimir.yaml # Querier mimir -target=querier -config.file=mimir.yaml # Store-gateway mimir -target=store-gateway -config.file=mimir.yaml # Compactor mimir -target=compactor -config.file=mimir.yaml # Ruler mimir -target=ruler -config.file=mimir.yaml # Optional: Query-scheduler mimir -target=query-scheduler -config.file=mimir.yaml # Optional: Alertmanager mimir -target=alertmanager -config.file=mimir.yaml Characteristics:\nMaximum scalability and flexibility Each component scales independently based on workload Granular failure domains (component failures don’t affect entire system) Component-specific resource allocation and optimization Recommended for production environments When to Use:\nProduction deployments with \u003e10M active series Large-scale systems requiring fine-grained control Deployments with highly variable component loads Organizations with distributed systems expertise Kubernetes environments with operator/Helm support Scaling Strategies:\nWrite-Heavy Workload:\nScale distributors (1 core per 25K samples/sec) Scale ingesters (1 core per 300K series) Query-Heavy Workload:\nScale query-frontends (1 core per 250 queries/sec) Scale queriers (1 core per 10 queries/sec) Scale store-gateways (1 core per 10 queries/sec) Large Data Volume:\nScale compactors (1 per 20M series) Increase store-gateway replicas Example Kubernetes Deployment (using Helm):\nhelm repo add grafana https://grafana.github.io/helm-charts helm install mimir grafana/mimir-distributed \\ --namespace mimir \\ --values production-values.yaml Benefits:\nIndependent failure domains (ingester crash doesn’t affect queriers) Fine-tuned resource allocation (queriers get more CPU, ingesters get more memory) Targeted scaling based on bottlenecks Easier capacity planning and cost optimization Trade-offs:\nHigher operational complexity More components to monitor and maintain Network communication overhead between components Requires service discovery and coordination (Kubernetes, Consul, etcd) How to Run Mimir Quick Start with Docker Compose For POC environments and local development, Docker Compose provides the fastest path to running Mimir.\nPrerequisites Docker and Docker Compose installed At least 4 GB RAM available 10 GB free disk space Complete Docker Compose Stack This example deploys:\nMinIO: S3-compatible object storage Mimir (3 instances): High-availability monolithic deployment NGINX: Load balancer distributing traffic across Mimir instances Grafana: Visualization and dashboarding Directory Structure:\nmimir-poc/ ├── docker-compose.yml ├── config/ │ ├── mimir.yaml │ ├── nginx.conf │ └── alertmanager-fallback-config.yaml └── data/ (created automatically) docker-compose.yml:\nversion: '3.8' services: # MinIO object storage minio: image: minio/minio entrypoint: [\"\"] command: [\"sh\", \"-c\", \"mkdir -p /data/mimir-blocks /data/mimir-ruler /data/mimir-alertmanager \u0026\u0026 minio server --quiet /data --console-address :9001\"] environment: - MINIO_ROOT_USER=mimir - MINIO_ROOT_PASSWORD=supersecret ports: - \"9000:9000\" # S3 API - \"9001:9001\" # Web console volumes: - minio-data:/data healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"] interval: 30s timeout: 10s retries: 3 # Mimir instance 1 mimir-1: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-1 depends_on: - minio ports: - \"8001:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-1-data:/data # Mimir instance 2 mimir-2: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-2 depends_on: - minio ports: - \"8002:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-2-data:/data # Mimir instance 3 mimir-3: image: grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] hostname: mimir-3 depends_on: - minio ports: - \"8003:8080\" volumes: - ./config/mimir.yaml:/etc/mimir.yaml - ./config/alertmanager-fallback-config.yaml:/etc/alertmanager-fallback-config.yaml - mimir-3-data:/data # NGINX load balancer nginx: image: nginx:alpine ports: - \"9009:9009\" volumes: - ./config/nginx.conf:/etc/nginx/nginx.conf:ro depends_on: - mimir-1 - mimir-2 - mimir-3 # Grafana for visualization grafana: image: grafana/grafana:latest environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin ports: - \"3000:3000\" volumes: - grafana-data:/var/lib/grafana volumes: minio-data: mimir-1-data: mimir-2-data: mimir-3-data: grafana-data: config/mimir.yaml:\n# Monolithic mode configuration target: all,alertmanager # Multi-tenancy (disable for single-tenant POC) multitenancy_enabled: false # Server configuration server: http_listen_port: 8080 log_level: info # Common configuration (shared by all components) common: storage: backend: s3 s3: endpoint: minio:9000 access_key_id: mimir secret_access_key: supersecret insecure: true # Use HTTP (not HTTPS) for local MinIO # Blocks storage configuration blocks_storage: backend: s3 s3: bucket_name: mimir-blocks tsdb: dir: /data/tsdb bucket_store: sync_dir: /data/tsdb-sync # Compactor configuration compactor: data_dir: /data/compactor sharding_ring: kvstore: store: memberlist # Distributor configuration distributor: ring: instance_addr: 127.0.0.1 kvstore: store: memberlist # Ingester configuration ingester: ring: instance_addr: 127.0.0.1 kvstore: store: memberlist replication_factor: 3 # 3 replicas across 3 Mimir instances # Ruler storage configuration ruler_storage: backend: s3 s3: bucket_name: mimir-ruler # Alertmanager configuration alertmanager_storage: backend: s3 s3: bucket_name: mimir-alertmanager alertmanager: fallback_config_file: /etc/alertmanager-fallback-config.yaml data_dir: /data/alertmanager # Store-gateway configuration store_gateway: sharding_ring: replication_factor: 3 # Limits configuration limits: # Ingestion limits ingestion_rate: 10000 # Samples per second per tenant ingestion_burst_size: 20000 # Series limits max_global_series_per_user: 1000000 # 1M series max max_global_series_per_metric: 50000 # Query limits max_query_lookback: 0 # No limit on query time range # Retention compactor_blocks_retention_period: 0 # Indefinite retention (0 = disabled) config/nginx.conf:\nevents { worker_connections 1024; } http { upstream mimir { server mimir-1:8080; server mimir-2:8080; server mimir-3:8080; } server { listen 9009; location / { proxy_pass http://mimir; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } config/alertmanager-fallback-config.yaml:\nroute: receiver: 'default-receiver' group_wait: 10s group_interval: 10s repeat_interval: 1h receivers: - name: 'default-receiver' Launch the Stack # Start all services docker-compose up -d # Verify services are running docker-compose ps # Check Mimir logs docker-compose logs -f mimir-1 # Access MinIO console open http://localhost:9001 # Username: mimir, Password: supersecret # Access Grafana open http://localhost:3000 Configure Prometheus Remote Write prometheus.yml:\nglobal: scrape_interval: 15s scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] remote_write: - url: http://localhost:9009/api/v1/push queue_config: capacity: 10000 max_shards: 10 min_shards: 1 Configure Grafana Data Source Navigate to Connections → Data sources Click Add data source Select Prometheus Configure: Name: Mimir URL: http://nginx:9009/prometheus Prometheus type: Mimir Click Save \u0026 test Production Configuration with MinIO For on-premises production deployments or when AWS S3 is not available, MinIO provides enterprise-grade S3-compatible object storage.\nDifferences from POC Setup:\nDistributed MinIO: Deploy 4+ MinIO servers for high availability (erasure coding) Persistent Volumes: Use network-attached storage or cloud block storage TLS Encryption: Enable HTTPS for MinIO and Mimir Authentication: Implement proper access control and API key rotation Resource Limits: Set CPU/memory limits based on capacity planning Monitoring: Deploy Prometheus + Grafana to monitor Mimir and MinIO Example Production Values:\n# Mimir production configuration with MinIO common: storage: backend: s3 s3: endpoint: minio.storage.svc.cluster.local:9000 access_key_id: ${MINIO_ACCESS_KEY} # From secret secret_access_key: ${MINIO_SECRET_KEY} # From secret insecure: false # Use HTTPS bucket_name: mimir-blocks ingester: ring: replication_factor: 3 instance_addr: ${POD_IP} kvstore: store: memberlist limits: ingestion_rate: 50000 # Higher for production max_global_series_per_user: 10000000 # 10M series blocks_storage: tsdb: retention_period: 24h # Ingesters retain blocks for 24h before relying on object storage block_ranges_period: [2h] compactor: compaction_interval: 30m cleanup_interval: 15m block_ranges: [2h, 12h, 24h] Resource Requirements Based on the capacity planning guide:\nSmall Deployment (~1M active series):\nDistributors: 2 instances × (1 core, 1 GB RAM) Ingesters: 3 instances × (2 cores, 8 GB RAM, 20 GB disk) Queriers: 2 instances × (1 core, 2 GB RAM) Query-Frontends: 2 instances × (1 core, 1 GB RAM) Store-Gateways: 2 instances × (1 core, 2 GB RAM, 15 GB disk) Compactor: 1 instance × (1 core, 4 GB RAM, 300 GB disk) Object Storage: ~50 GB (1 year retention) Large Deployment (~10M active series):\nTotal Resources: ~140 CPUs, 800 GB memory Use the Mimir capacity calculator for detailed estimates Grafana Data Source Setup After deploying Mimir, configure Grafana to query metrics:\nVia UI:\nConnections → Data sources → Add data source → Prometheus URL: http://mimir-gateway:8080/prometheus (adjust for your deployment) Prometheus type: Select “Mimir” from dropdown Save \u0026 test Via Provisioning (GitOps):\n# grafana/provisioning/datasources/mimir.yaml apiVersion: 1 datasources: - name: Mimir type: prometheus access: proxy url: http://mimir-gateway.mimir.svc.cluster.local:8080/prometheus jsonData: prometheusType: Mimir timeInterval: 15s httpMethod: POST Best Practices Label Strategy and Cardinality Management Golden Rule: Avoid unbounded labels (infinite possible values).\nGood Labels (bounded cardinality):\n{service=\"battle-api\"} # Limited number of services {environment=\"production\"} # dev, staging, production (3 values) {region=\"us-east-1\"} # Limited AWS regions {status_code=\"200\"} # ~50 HTTP status codes {method=\"GET\"} # GET, POST, PUT, DELETE, PATCH (~7 values) Bad Labels (unbounded cardinality):\n{user_id=\"12345\"} # Millions of users = millions of series {request_id=\"abc-123-def\"} # Every request unique = infinite series {email=\"user@example.com\"} # Unbounded user emails {timestamp=\"1634567890\"} # Infinite timestamp values {session_id=\"xyz789\"} # Every session unique Cardinality Calculation:\nTotal Series = Metric × (Label1_Values × Label2_Values × ... × LabelN_Values) Example:\nhttp_requests_total{service, environment, region, method, status_code} = 1 metric × (10 services × 3 environments × 5 regions × 7 methods × 50 status codes) = 1 × 52,500 = 52,500 time series Adding user_id (1M users):\n= 52,500 × 1,000,000 = 52.5 billion time series (UNSUSTAINABLE!) Best Practices:\nUse Bounded Sets: Labels should have enumerable values Aggregate Before Storing: Use recording rules to pre-aggregate high-cardinality metrics Drop Unused Labels: Use metric_relabel_configs in Prometheus to drop unnecessary labels Monitor Cardinality: Enable cardinality analysis in Mimir Set Limits: Configure per-tenant series limits to prevent runaway cardinality Configuration Best Practices Ingester Tuning File Descriptors:\n# Set system limits for ingester pods/containers ulimit -n 65536 # Minimum ulimit -n 1048576 # For 1000+ tenants Memory and Disk:\ningester: # Target 1.5M series per ingester (conservative) # Max 5M series per ingester (with sufficient memory) blocks_storage: tsdb: # Reduce block creation interval for lower memory usage block_ranges_period: [1h] # vs. default 2h # Tune for high tenant count head_chunks_write_buffer_size_bytes: 2097152 # 2MB (default 4MB) stripe_size: 8192 # Default 16384 Zone-Aware Replication:\ningester: ring: zone_awareness_enabled: true instance_availability_zone: ${AZ} # us-east-1a, us-east-1b, etc. Querier and Store-Gateway Optimization Enable Caching (CRITICAL for production):\nblocks_storage: bucket_store: # Metadata cache metadata_cache: backend: memcached memcached: addresses: memcached-metadata:11211 # Index cache (high CPU usage) index_cache: backend: memcached memcached: addresses: memcached-index:11211 # Chunks cache (high bandwidth usage) chunks_cache: backend: memcached memcached: addresses: memcached-chunks:11211 max_item_size: 5242880 # 5MB max chunk size query_frontend: # Query results cache results_cache: backend: memcached memcached: addresses: memcached-results:11211 timeout: 500ms File Descriptors:\n# Store-gateway needs many open files for index-headers ulimit -n 65536 # Minimum SSD Recommendations:\nIngesters: SSD for WAL performance Store-Gateways: SSD for index-header operations Compactor: SSD for faster compaction Compactor Configuration For Standard Tenants:\ncompactor: compaction_interval: 30m cleanup_interval: 15m data_dir: /data/compactor block_ranges: [2h, 12h, 24h] For Large Tenants (\u003e20M series):\ncompactor: # Enable split-and-merge compaction split_and_merge_shards: 4 # 1 shard per 8M series split_groups: 4 # Match shard count or next power of 2 Retention Configuration:\nlimits: # Global default: 1 year retention compactor_blocks_retention_period: 1y # Per-tenant overrides via runtime config overrides: tenant-production: compactor_blocks_retention_period: 2y # 2 years for production tenant-development: compactor_blocks_retention_period: 4w # 4 weeks for dev Storage Backend Selection Cloud Deployments:\nAWS: Use Amazon S3 (native integration, lowest latency in AWS) GCP: Use Google Cloud Storage (native integration, lowest latency in GCP) Azure: Use Azure Blob Storage (disable hierarchical namespace!) On-Premises Deployments:\nMinIO: Deploy distributed MinIO (4+ nodes with erasure coding) Ceph: Use Ceph RADOS Gateway (S3-compatible) OpenStack Swift: For OpenStack environments Bucket Configuration:\n# CRITICAL: Use separate buckets for each storage type blocks_storage: s3: bucket_name: mimir-blocks # TSDB blocks ruler_storage: s3: bucket_name: mimir-ruler # Recording/alerting rules alertmanager_storage: s3: bucket_name: mimir-alertmanager # Alertmanager state Bucket Lifecycle Policies (cost optimization):\n\u003c!-- AWS S3 Lifecycle Policy Example --\u003e \u003cLifecycleConfiguration\u003e \u003cRule\u003e \u003cID\u003eTransitionOldMetrics\u003c/ID\u003e \u003cStatus\u003eEnabled\u003c/Status\u003e \u003cTransition\u003e \u003cDays\u003e90\u003c/Days\u003e \u003cStorageClass\u003eSTANDARD_IA\u003c/StorageClass\u003e \u003c/Transition\u003e \u003cTransition\u003e \u003cDays\u003e180\u003c/Days\u003e \u003cStorageClass\u003eGLACIER_IR\u003c/StorageClass\u003e \u003c/Transition\u003e \u003c/Rule\u003e \u003c/LifecycleConfiguration\u003e Performance Tuning gRPC Compression Enable compression between components to reduce network bandwidth:\n# Ingester → Object Storage compression blocks_storage: s3: # Enable gzip compression for block uploads send_content_encoding: gzip # Query-Frontend → Querier compression querier: frontend_client: grpc_client_config: # Enable gzip compression grpc_compression: gzip Compression Trade-offs:\nSnappy: ~5x compression, 400 MiB/s throughput (low CPU) Gzip: 6-8x compression, 50-135 MiB/s throughput (higher CPU) Query Optimization Query Splitting:\nquery_frontend: # Split large time-range queries into smaller chunks split_queries_by_interval: 24h # Split into 24-hour chunks align_queries_with_step: true Query Sharding:\nquery_frontend: # Shard queries across time series for parallel execution parallelize_shardable_queries: true Query Caching:\nquery_frontend: cache_results: true results_cache: backend: memcached memcached: addresses: memcached:11211 # Cache queries for 10 minutes cache_unaligned_requests: true Avoid Querying Non-Compacted Blocks Use default values for these settings to avoid querying uncompacted blocks:\nquerier: # Query store-gateway for data older than 12h query_store_after: 12h # Query ingesters for data within 13h query_ingesters_within: 13h blocks_storage: bucket_store: # Store-gateway ignores blocks uploaded within 10h ignore_blocks_within: 10h This ensures queriers only fetch compacted, optimized blocks from store-gateways.\nMulti-Tenancy Best Practices Always Use Reverse Proxy:\nClient → Auth Proxy (validates user, injects X-Scope-OrgID) → Mimir Never expose Mimir directly to untrusted clients.\nPer-Tenant Limits:\n# Global defaults (conservative) limits: ingestion_rate: 10000 max_global_series_per_user: 1000000 # Runtime config for per-tenant overrides overrides: premium-tenant: ingestion_rate: 100000 max_global_series_per_user: 10000000 standard-tenant: ingestion_rate: 25000 max_global_series_per_user: 2500000 Enable Shuffle Sharding:\ningester: ring: # Reduce blast radius: each tenant uses subset of ingesters instance_enable_ipv6: false unregister_on_shutdown: true Common Pitfalls to Avoid Not Enabling Caching: Queries will hammer object storage (high cost, poor performance) Unbounded Label Cardinality: User IDs, request IDs, timestamps in labels Insufficient File Descriptors: Ingesters and store-gateways need high limits Same Bucket for Different Stores: Always use separate buckets Azure Hierarchical Namespace: Must be disabled (causes orphaned directories) No Monitoring: Deploy mimir-mixin dashboards and alerts from day one Under-provisioning Compactor Disk: Needs space for source + destination blocks Ignoring Latency Spikes: Upgrade to v2.15+ for improved block-cutting When to Use Mimir Ideal Use Cases 1. Enterprise Scale (\u003e10M active series)\nPrometheus hits memory and disk limits around 10-50M series depending on hardware. Mimir scales horizontally to billions of series.\nExample: Multi-region infrastructure with 100+ Kubernetes clusters, each running hundreds of services.\n2. Long-Term Retention (months to years)\nPrometheus retention limited by local disk capacity (typically days to weeks). Mimir uses object storage for years of retention.\nExample: Compliance requirements for 2-year metrics retention, capacity planning based on historical trends.\n3. Multi-Cluster Aggregation\nMultiple Prometheus instances across regions/environments need unified querying.\nExample: Global view of service health across US, EU, and APAC deployments.\n4. Multi-Tenancy\nPer-customer, per-team, or per-environment isolation with resource limits.\nExample: SaaS platform providing per-customer metrics dashboards, or large organization isolating team metrics.\n5. High Availability Requirements\nNo tolerance for data loss or query unavailability.\nExample: Financial services, healthcare, or e-commerce platforms requiring 99.9%+ uptime.\nAnti-Patterns (When NOT to Use Mimir) 1. Small-Scale Deployments (\u003c1M active series, \u003c30 days retention)\nProblem: Mimir’s complexity unjustified for workloads Prometheus handles easily.\nSolution: Use standalone Prometheus until scale demands distributed storage.\n2. Operational Complexity Constraints\nProblem: Team lacks distributed systems expertise or Kubernetes experience.\nSolution: Start with Prometheus, build expertise, migrate to Mimir when needed.\n3. Cost-Constrained Environments\nProblem: Mimir has higher baseline infrastructure costs (multiple components + object storage).\nSolution: Prometheus is more cost-effective at small scale.\n4. Real-Time Only (No Historical Analysis)\nProblem: If only real-time alerting needed (no dashboards, no historical queries).\nSolution: Prometheus sufficient; Mimir’s long-term storage unused.\nMimir vs. Prometheus Comparison Factor Prometheus Grafana Mimir Active Series Limit 10-50M (single machine) 1B+ (distributed) Retention Days to weeks (disk-limited) Months to years (object storage) Scalability Vertical only Horizontal (all components) High Availability Manual (federation/replica pairs) Built-in (replication + distribution) Multi-Tenancy Not built-in Native support Operational Complexity Low (single binary) High (microservices) Setup Time Minutes Hours to days Cost (Small Scale) Lower Higher Cost (Large Scale) Not feasible Medium (object storage efficient) Query Performance Excellent (local disk) Good (distributed, cached) Use Case Small-medium, real-time Enterprise, long-term, multi-cluster Mimir vs. Thanos Comparison Both Mimir and Thanos solve similar problems but with different approaches.\nFactor Thanos Grafana Mimir Architecture Sidecar or Receiver mode Receiver mode only (push-based) Design Focus Operational simplicity, cost Performance, scalability Compaction Standard TSDB compaction Split-and-merge (overcomes 64GB index limit) Max Index Size 64GB (TSDB limit) No practical limit Query Caching Metadata caching only Full query result caching Maturity Mature, CNCF project Newer, actively developed Deployment Model Pull (sidecar) or push (receiver) Push only (remote write) Grafana Integration Good Excellent (same vendor) Community Large CNCF community Grafana Labs + community When to Choose Mimir:\nNeed maximum performance and scalability Prefer Grafana ecosystem integration Large tenants (\u003e20M series per tenant) Active development and new features valued When to Choose Thanos:\nPrefer sidecar deployment model (pull-based) CNCF governance important Operational simplicity over raw performance Existing Thanos deployments Mimir vs. Cortex Relationship: Mimir is the successor to Cortex (forked March 2022).\nKey Differences:\nDevelopment: Mimir actively developed by Grafana Labs; Cortex in maintenance mode Features: Mimir has newer features (monolithic mode, improved compactor, OTLP support matured first) Complexity: Mimir simplified some operational aspects of Cortex Migration: Cortex → Mimir migration supported and documented Recommendation: Always use Mimir for new deployments. Only run Cortex if already deployed and not ready to migrate.\nDecision Criteria Matrix Use this matrix to determine if Mimir is right for your use case:\nCriterion Prometheus Mimir Thanos Active Series \u003c10M \u003e10M \u003e10M Retention Requirement \u003c30 days \u003e30 days \u003e30 days Multi-Cluster Single cluster Multiple clusters Multiple clusters Multi-Tenancy Not needed Required Via labels Team Expertise Limited Distributed systems Distributed systems Budget Constrained Medium-High Medium Deployment Model Any Kubernetes preferred Kubernetes or VMs Vendor Preference Neutral Grafana ecosystem CNCF ecosystem Decision Guide:\nChoose Prometheus if:\nActive series \u003c 10M Retention \u003c 30 days Single Kubernetes cluster Team prefers simplicity Choose Mimir if:\nActive series \u003e 10M (or expect to reach soon) Retention \u003e 30 days Multiple Prometheus instances to aggregate Multi-tenancy required Grafana ecosystem preferred Choose Thanos if:\nWant to keep Prometheus sidecar model CNCF governance important Operational simplicity over maximum performance Existing Thanos deployment BattleBots Integration Points For the BattleBots platform, Mimir would serve as the centralized metrics storage backend within the broader observability stack.\nObservability Stack Architecture graph TB subgraph \"Metric Sources\" Bot[Bot Containers] Game[Game Servers] API[Battle API] K8s[Kubernetes] Host[Host Systems] end subgraph \"Collection Layer\" OTel[OpenTelemetry Collector] Bot --\u003e OTel Game --\u003e OTel API --\u003e OTel K8s --\u003e OTel Host --\u003e OTel end subgraph \"Storage Layer\" OTel --\u003e|OTLP Metrics| Mimir[(Grafana Mimir\u003cbr/\u003eMetrics Storage)] OTel --\u003e|OTLP Logs| Loki[(Grafana Loki\u003cbr/\u003eLog Storage)] OTel --\u003e|OTLP Traces| Tempo[(Grafana Tempo\u003cbr/\u003eTrace Storage)] end subgraph \"Query \u0026 Visualization\" Grafana[Grafana] Grafana --\u003e Mimir Grafana --\u003e Loki Grafana --\u003e Tempo end subgraph \"Alerting\" Mimir --\u003e|Alerts| AM[Alertmanager] AM --\u003e|Notifications| Slack[Slack/PagerDuty] end style OTel fill:#e1f5ff style Mimir fill:#fff4e1 style Loki fill:#e8f5e9 style Tempo fill:#f3e5f5 style Grafana fill:#ffe0b2 Game Metrics Use Cases Bot Performance Metrics Action Latency:\n# 95th percentile attack action latency by bot type histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket{action=\"attack\"}[5m])) by (bot_type, le) ) # Slow bots (p95 \u003e 100ms) histogram_quantile(0.95, sum(rate(bot_action_duration_seconds_bucket[5m])) by (bot_id, le) ) \u003e 0.1 Bot Health Tracking:\n# Average bot health per battle avg(bot_health_points) by (battle_id, bot_id) # Bots eliminated in last hour count(bot_health_points == 0) by (battle_id) Resource Usage:\n# CPU usage per bot container rate(container_cpu_usage_seconds_total{ namespace=\"battlebots\", pod=~\"bot-.*\" }[5m]) # Memory usage per bot container_memory_working_set_bytes{ namespace=\"battlebots\", pod=~\"bot-.*\" } / 1024 / 1024 # Convert to MB Battle Event Metrics Battle State:\n# Active battles sum(battle_state{state=\"active\"}) # Average battle duration rate(battle_duration_seconds_sum[5m]) / rate(battle_duration_seconds_count[5m]) # Battles per minute rate(battles_total[1m]) * 60 Matchmaking Metrics:\n# Players in queue player_queue_length # Average queue wait time rate(queue_wait_seconds_sum[5m]) / rate(queue_wait_seconds_count[5m]) # Matchmaking success rate rate(matchmaking_success_total[5m]) / rate(matchmaking_attempts_total[5m]) Infrastructure Metrics Use Cases API Performance:\n# Request rate by endpoint and status sum(rate(http_requests_total{service=\"battle-api\"}[5m])) by (endpoint, status_code) # Error rate sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) # p99 latency histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (endpoint, le) ) Kubernetes Metrics:\n# Pod restart rate rate(kube_pod_container_status_restarts_total{ namespace=\"battlebots\" }[1h]) # Pods not ready count(kube_pod_status_phase{ namespace=\"battlebots\", phase!=\"Running\" }) # Node resource utilization sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes) Label Strategy for BattleBots Recommended Labels:\n{ # Infrastructure labels cluster=\"us-east-1-prod\", namespace=\"battlebots\", service=\"battle-api\", environment=\"production\", # Game labels battle_id=\"12345\", # OK if battles are finite and expire bot_type=\"tank\", # Enumerable bot types game_mode=\"team-deathmatch\", # Bounded game modes region=\"us-east\", # Geographic regions # Avoid these! # player_id=\"...\", # High cardinality # session_id=\"...\", # Unbounded # request_id=\"...\", # Infinite values } Cardinality Estimate:\nhttp_requests_total { service: 10 values (battle-api, game-server, matchmaking, etc.) environment: 3 values (dev, staging, prod) region: 5 values (us-east, us-west, eu-west, ap-south, ap-east) method: 7 values (GET, POST, PUT, DELETE, PATCH, OPTIONS, HEAD) endpoint: 50 values (API endpoints) status_code: 50 values (HTTP status codes) } = 1 × 10 × 3 × 5 × 7 × 50 × 50 = 2,625,000 time series # Acceptable cardinality for 1 metric Example PromQL Queries for BattleBots Battle Analytics:\n# Total battles completed today increase(battles_completed_total[24h]) # Win rate by bot type sum(rate(battle_outcomes_total{outcome=\"victory\"}[1h])) by (bot_type) / sum(rate(battle_outcomes_total[1h])) by (bot_type) # Average players online by hour avg_over_time(players_online_total[1h]) Capacity Planning:\n# CPU headroom (sum(node_cpu_capacity_cores) - sum(rate(node_cpu_usage_seconds_total[5m]))) / sum(node_cpu_capacity_cores) # Memory headroom (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemUsed_bytes)) / sum(node_memory_MemTotal_bytes) # Projected series growth predict_linear(mimir_ingester_active_series[24h], 7*24*3600) Cost Optimization:\n# Underutilized bot containers (CPU \u003c 10%) avg(rate(container_cpu_usage_seconds_total{pod=~\"bot-.*\"}[5m])) by (pod) \u003c 0.1 # Idle game servers count(game_server_active_battles == 0) by (instance) Alerting Examples Critical Alerts:\ngroups: - name: battlebots-critical interval: 30s rules: # Battle API down - alert: BattleAPIDown expr: up{job=\"battle-api\"} == 0 for: 1m labels: severity: critical annotations: summary: \"Battle API is down\" description: \"Battle API {{ $labels.instance }} has been down for more than 1 minute\" # High error rate - alert: HighErrorRate expr: | sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) \u003e 0.05 for: 5m labels: severity: critical annotations: summary: \"High error rate detected\" description: \"Error rate is {{ $value | humanizePercentage }}\" # Ingester approaching series limit - alert: MimirIngesterSeriesLimit expr: | mimir_ingester_active_series / mimir_limits_overrides{limit_name=\"max_global_series_per_user\"} \u003e 0.8 for: 15m labels: severity: warning annotations: summary: \"Mimir ingester approaching series limit\" Integration with Loki and Tempo Metric-to-Log Correlation:\n# In Grafana, link from metric spike to logs {namespace=\"battlebots\", service=\"battle-api\"} |= \"error\" | logfmt | battle_id=\"12345\" Metric-to-Trace Correlation:\nUse exemplars in Prometheus metrics to link to traces Query by trace_id in Tempo from metric anomalies Grafana’s Explore view shows metrics → traces → logs correlation Unified Dashboard Example:\n┌─────────────────────────────────────┐ │ Battle API Request Rate (Mimir) │ │ [Graph showing spike at 14:30] │ └─────────────────────────────────────┘ ↓ Click spike ┌─────────────────────────────────────┐ │ Traces at 14:30 (Tempo) │ │ [Slow traces listed] │ └─────────────────────────────────────┘ ↓ Click trace ┌─────────────────────────────────────┐ │ Logs for trace_id (Loki) │ │ [Error logs with stack trace] │ └─────────────────────────────────────┘ Further Reading Official Documentation Grafana Mimir Documentation - Comprehensive official docs Mimir GitHub Repository - Source code, issues, discussions Mimir Architecture Overview - Detailed architecture guide Mimir Configuration Parameters - Complete config reference Mimir Runbooks - Troubleshooting guides Deployment and Operations Helm Chart Documentation - Kubernetes deployment guide Production Tips - Best practices Capacity Planning - Resource estimation Mimir Capacity Calculator - Interactive sizing tool Monitor Mimir Health - Self-monitoring setup Integration Guides Configure OpenTelemetry Collector for Mimir - OTLP integration Migrate from Prometheus to Mimir - Migration guide Remote Write Tuning - Optimize Prometheus → Mimir ingestion Grafana Dashboards for Mimir - Pre-built dashboards Performance and Scaling How We Scaled Mimir to 1 Billion Active Series - Grafana Labs blog Scaling Mimir to 500M Series (Customer Story) - Pipedrive case study Mimir vs Prometheus Scalability - Real-world comparison Query Performance Optimization - PromQL tuning Comparisons and Decision Guides Mimir vs Prometheus Comparison - Detailed comparison Mimir vs Thanos Discussion - Community comparison Prometheus and Centralized Storage - When to use centralized metrics Community and Support Mimir Community Forum - Ask questions, share knowledge Grafana Slack #mimir Channel - Real-time community support Mimir Release Notes - Version history and breaking changes CNCF OpenTelemetry Project - Related OTLP ecosystem Tutorials and Workshops Play with Grafana Mimir - Hands-on tutorial Mimir Workshop - Full Docker Compose examples OTLP Integration Tutorial - OpenTelemetry + Mimir guide Advanced Topics Cardinality Management - Label strategy Multi-Tenancy Setup - Tenant isolation guide Ingest Storage Architecture - Kafka-based ingestion (Mimir 3.0+) Split-and-Merge Compactor - Large tenant optimization ","categories":"","description":"Comprehensive overview of Grafana Mimir architecture, deployment modes, storage backends, and operational best practices for long-term Prometheus metrics storage.\n","excerpt":"Comprehensive overview of Grafana Mimir architecture, deployment …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/metrics/mimir/mimir-overview/","tags":"","title":"Mimir: Overview and Architecture"},{"body":"Overview This section contains research and analysis of observability solutions for the BattleBots platform. Observability is critical for:\nMonitoring real-time battle events and game state Tracking bot performance and system health Debugging issues in distributed game architecture Analyzing player behavior and system usage patterns Ensuring reliable service operation Components OpenTelemetry Collector Analysis of the OpenTelemetry Collector, a vendor-agnostic telemetry data pipeline that can receive, process, and export logs, metrics, and traces to multiple backends.\nThe OpenTelemetry Collector serves as a centralized telemetry hub, providing:\nVendor neutrality for any observability backend Protocol translation between Prometheus, Jaeger, Zipkin, and OTLP Unified collection pipeline for logs, metrics, and traces Flexible deployment in agent, gateway, or hybrid modes Signal correlation linking traces, metrics, and logs Includes detailed analysis of:\nArchitecture and core components Logs, metrics, and traces support Self-monitoring and operational considerations BattleBots platform integration patterns Log Storage Analysis of log storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nLog storage is essential for:\nAggregating logs from distributed game servers and services Enabling fast search and filtering for debugging Correlating logs with traces and metrics for unified observability Long-term retention for compliance and historical analysis Cost-effective storage at scale Grafana Loki Research on Grafana Loki, a horizontally scalable, multi-tenant log aggregation system designed for cost-effective log storage and querying.\nLoki uses an index-free approach that indexes only metadata labels rather than full log content, providing:\nNative OTLP support (Loki v3+) for seamless OpenTelemetry Collector integration Label-based querying through LogQL Efficient storage with compressed chunks Horizontal scalability and multi-tenancy Tight integration with Grafana for visualization Includes detailed analysis of:\nArchitecture and core concepts Deployment modes and operational best practices OTLP compatibility and OTel Collector integration Label strategy and performance considerations Metrics Storage Analysis of metrics storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nMetrics storage is essential for:\nReal-time monitoring of battle events and game state Historical analysis of bot performance and system behavior Capacity planning and infrastructure optimization Alerting on critical system conditions Long-term trend analysis and reporting Grafana Mimir Research on Grafana Mimir, a horizontally scalable, highly available, multi-tenant metrics storage system for long-term Prometheus data retention.\nMimir transforms Prometheus from a single-server monitoring system into a distributed platform capable of handling over 1 billion active time series, providing:\nNative OTLP support for direct integration with OpenTelemetry Collector Horizontal scalability through independent scaling of write path, read path, and backend components Long-term storage using object storage backends (S3, GCS, MinIO) with months to years of retention Built-in multi-tenancy with per-tenant resource limits and isolation Full Prometheus (PromQL) compatibility for queries, dashboards, and alerts High availability through replication and distributed architecture Includes detailed analysis of:\nArchitecture components (distributor, ingester, querier, store-gateway, compactor) and deployment modes Native OTLP ingestion endpoints and OpenTelemetry Collector integration (otlphttp and prometheusremotewrite exporters) Object storage backends, blocks storage architecture, and retention policies Multi-tenancy setup, cardinality management, and label strategy Comparison with Prometheus, Thanos, and Cortex Production deployment patterns, resource requirements, and operational best practices Traces Storage Analysis of distributed tracing storage backends for the BattleBots observability stack, focusing on systems that integrate with the OpenTelemetry Collector.\nTraces storage is essential for:\nTracking end-to-end request flow through distributed game servers and services Debugging performance bottlenecks and latency issues in battle workflows Understanding service dependencies and call patterns Root cause analysis when correlating with metrics and logs Visualizing complete battle lifecycles from matchmaking to results Grafana Tempo Research on Grafana Tempo, a high-volume, minimal dependency distributed tracing backend designed for cost-efficiency and operational simplicity.\nTempo uses an object storage-only architecture that eliminates complex database dependencies, providing:\nNative OTLP support (gRPC port 4317, HTTP port 4318) for seamless OpenTelemetry Collector integration Cost-effective storage using object storage backends (S3, GCS, MinIO) with 10x+ cost reduction compared to traditional tracing systems TraceQL query language for powerful trace filtering and analysis Horizontal scalability through microservices architecture Seamless correlation with Grafana, Loki, and Mimir through exemplars and trace IDs for unified observability Multi-protocol support (OTLP, Jaeger, Zipkin, OpenCensus) for flexible integration Includes detailed analysis of:\nArchitecture components (distributor, ingester, querier, compactor, metrics-generator) and deployment modes Native OTLP ingestion endpoints and OpenTelemetry Collector integration (otlp and otlphttp exporters) Object storage backends, blocks storage architecture, and sampling strategies TraceQL query language and trace-to-metrics-to-logs correlation Comparison with Jaeger, Zipkin, and Elastic APM Production deployment patterns, resource requirements, and operational best practices Future ADR Dependencies This analysis will inform:\nADR-NNNN: Observability Stack Selection - Which backends to use (Loki, Prometheus, Jaeger, etc.) ADR-NNNN: Telemetry Collection Strategy - Agent vs. gateway deployment, sampling policies ADR-NNNN: Telemetry Data Retention - Storage duration and cost management Related Documentation R\u0026D Documentation User Journey 0001: POC - Observability requirements context Future ADRs on observability stack architecture External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of observability solutions for the BattleBots platform.\n","excerpt":"Research and analysis of observability solutions for the BattleBots …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/","tags":"","title":"Observability Analysis"},{"body":"Overview The OpenTelemetry Collector serves as a centralized telemetry hub, removing the need to run multiple agents or collectors for different formats and backends. It provides:\nVendor neutrality: Works with any observability backend Protocol translation: Converts between Prometheus, Jaeger, Zipkin, and OTLP formats Unified collection: Single pipeline for logs, metrics, and traces Flexible deployment: Agent mode, gateway mode, or hybrid Signal correlation: Links traces, metrics, and logs through shared context Document Structure The analysis is organized into the following documents:\nOpenTelemetry Collector Overview High-level architectural overview covering:\nCore components (receivers, processors, exporters, extensions) Pipeline-based architecture and data flow Deployment patterns (agent, gateway, hybrid) Configuration fundamentals When to use the Collector vs. direct exports Audience: Everyone—provides foundational understanding for all subsequent documents.\nLogs Support Deep dive into log data handling:\nOTLP logs data model and structure Log receivers (filelog, syslog, OTLP) Log processors (attributes, filter, transform) Log exporters (Loki, Elasticsearch, OTLP) Log correlation with traces and metrics Configuration patterns for log collection Audience: Developers implementing log collection, operations teams configuring log pipelines.\nMetrics Support Deep dive into metrics data handling:\nOpenTelemetry metrics data model Metric types (counters, gauges, histograms, summaries) Temporality (delta vs. cumulative) Metrics receivers (Prometheus, hostmetrics, OTLP) Metrics processors and exporters Performance and cardinality considerations Audience: Developers instrumenting applications, SREs monitoring infrastructure.\nTraces Support Deep dive into distributed tracing:\nTrace and span data model Context propagation mechanisms Trace receivers (OTLP, Jaeger, Zipkin) Sampling strategies (head vs. tail sampling) Trace processors and exporters Multi-backend routing Audience: Developers implementing distributed tracing, architects designing observability strategy.\nSelf-Monitoring How to observe the Collector itself:\nInternal metrics and telemetry Extensions (health_check, zpages, pprof) Debugging and troubleshooting techniques Performance monitoring and optimization Production monitoring best practices Audience: Operations teams, SREs responsible for Collector reliability.\nBattleBots Platform Context For the BattleBots platform, the OpenTelemetry Collector would support:\nGame Event Observability Logs: Battle events, bot actions, game state transitions, error conditions Metrics: Match duration, action rates, player counts, system resource usage Traces: Request flows from player action to state update to broadcast Infrastructure Monitoring Host metrics: Server CPU, memory, disk, network utilization Application metrics: Go runtime metrics, HTTP latency, WebSocket connections Container metrics: Resource limits, restart counts, health status Cross-Signal Correlation The Collector enables powerful debugging workflows:\nAlert fires on high error rate (metrics) Drill down to traces showing failing requests View logs associated with failing trace spans Identify root cause with full context This unified observability is particularly valuable during live battles when quick diagnosis is essential.\nImplementation Considerations Deployment Architecture For BattleBots, a recommended deployment would include:\nAgent Mode:\nCollectors running alongside each game server Local log file collection with filelog receiver Host metrics collection for server monitoring OTLP receiver for application telemetry Gateway Mode:\nCentralized collectors receiving data from agents Tail sampling for intelligent trace retention Multi-backend routing (analytics, debugging, long-term storage) Buffering and retry for backend resilience Signal-Specific Patterns Logs:\nCollect structured JSON logs from game servers Parse and enrich with resource attributes Filter debug logs in production Route to Loki or Elasticsearch Metrics:\nScrape Prometheus metrics from Go services Collect host metrics from servers Aggregate and downsample for cost efficiency Export to Prometheus or cloud backends Traces:\nInstrument Go services with OpenTelemetry SDK Use head sampling for baseline reduction (10%) Apply tail sampling to always capture errors Export to Jaeger or Grafana Tempo External Resources OpenTelemetry Documentation OpenTelemetry Collector GitHub OpenTelemetry Collector Contrib CNCF OpenTelemetry Project Contributing These analysis documents are living documents that should be updated as:\nNew OpenTelemetry Collector features are released BattleBots observability requirements evolve Team members gain operational experience with the Collector Best practices and patterns are discovered Updates should maintain the high-level overview focus with links to authoritative sources for technical deep-dives.\n","categories":"","description":"Research and analysis of the OpenTelemetry Collector for logs, metrics, and traces collection and processing.\n","excerpt":"Research and analysis of the OpenTelemetry Collector for logs, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/","tags":"","title":"OpenTelemetry Collector"},{"body":"Overview The OpenTelemetry Collector is a vendor-agnostic application that receives, processes, and exports telemetry data (traces, metrics, and logs). It serves as a centralized component in observability architectures, removing the need to run multiple agents or collectors for different telemetry formats and backends.\nThe Collector supports open-source observability data formats including Jaeger, Prometheus, Fluent Bit, and others, while providing a unified approach to telemetry handling. It enables services to offload telemetry data quickly while the Collector handles retries, batching, encryption, and sensitive data filtering.\nKey benefits include vendor independence, reduced operational complexity, and the ability to route telemetry data to multiple backends simultaneously without modifying application code.\nKey Concepts The Collector is built around five guiding principles:\nUsability: Provides functional defaults with support for popular protocols out-of-the-box Performance: Maintains stability under varying loads with predictable resource usage Observability: Designed as an observable service itself, exposing its own metrics and health status Extensibility: Allows customization through plugins without requiring modifications to core code Unification: Single codebase supporting all three telemetry signals (traces, metrics, logs) Core Architecture The Collector uses a pipeline-based architecture where data flows through three primary component types, orchestrated by a configuration file.\ngraph LR A[Telemetry Sources] --\u003e B[Receivers] B --\u003e C[Processors] C --\u003e D[Exporters] D --\u003e E[Observability Backends] F[Extensions] -.Optional.-\u003e B F -.Optional.-\u003e C F -.Optional.-\u003e D style A fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style C fill:#f3e5f5 style D fill:#e8f5e9 style F fill:#fce4ec Data Flow Telemetry data flows unidirectionally through the Collector:\nReceivers accept incoming telemetry in various formats (OTLP, Jaeger, Prometheus) Processors transform, filter, or enrich the data in a sequential chain Exporters send processed data to one or more backend destinations This pipeline architecture allows the same data to be simultaneously:\nSampled differently for different backends Enriched with environment-specific attributes Routed to multiple observability platforms Components Receivers Receivers gather telemetry data through two mechanisms:\nPush-based: Listen on network endpoints for incoming data Pull-based: Actively scrape metrics from instrumented services Multiple receivers can feed into a single pipeline, with their outputs merged before reaching processors. Common receivers include:\notlp - OpenTelemetry Protocol (gRPC/HTTP) prometheus - Prometheus metrics scraping jaeger - Jaeger trace data filelog - Log file collection For comprehensive receiver documentation, see the OpenTelemetry Collector Receivers.\nProcessors Processors form a sequential chain that transforms data between receivers and exporters. Each processor in the chain operates on the data before passing it to the next processor.\nCommon operations include:\nAdding or removing attributes Sampling (probabilistic, tail-based) Batching for efficient transmission Filtering unwanted telemetry Resource detection (cloud provider, Kubernetes metadata) Important: Each pipeline maintains independent processor instances, even when referencing the same configuration name across multiple pipelines. This prevents unintended state sharing between pipelines.\nFor comprehensive processor documentation, see the OpenTelemetry Collector Processors.\nExporters Exporters forward processed data to external destinations, typically by sending to network endpoints or writing to logging systems. Multiple exporters can receive identical data copies through a fan-out mechanism, enabling simultaneous transmission to different backends.\nCommon exporters include:\notlp - OpenTelemetry Protocol destinations prometheus - Prometheus remote write jaeger - Jaeger backend logging - Standard output for debugging file - Local file storage For comprehensive exporter documentation, see the OpenTelemetry Collector Exporters.\nConnectors Connectors enable inter-pipeline communication and data flow between different telemetry signal types. A connector acts as both an exporter (for one pipeline) and a receiver (for another pipeline).\nUse cases include:\nGenerating metrics from trace spans Creating logs from metric anomalies Correlating signals for enhanced observability For more information, see the OpenTelemetry Collector Connectors.\nExtensions Extensions provide auxiliary functionality that doesn’t directly process telemetry data but supports Collector operations:\nhealth_check - HTTP endpoint for health monitoring pprof - Go profiling endpoint for performance analysis zpages - In-process debugging pages ballast - Memory ballast for GC optimization Extensions are optional but highly recommended for production deployments. See self-monitoring documentation for more details.\nConfiguration The Collector uses YAML configuration files (default: /etc/otelcol/config.yaml) with four main sections:\nreceivers: # Define how to collect telemetry processors: # Define how to transform telemetry exporters: # Define where to send telemetry service: pipelines: # Connect receivers → processors → exporters Pipeline Types Three pipeline types handle different telemetry signals:\ntraces: Distributed tracing data metrics: Time-series measurements logs: Event records Each pipeline independently connects receivers to exporters through an optional processor chain. The same receiver can feed multiple pipelines simultaneously, though this creates a potential bottleneck if one processor blocks.\nConfiguration Best Practices Validate configurations before deployment: otelcol validate --config=file.yaml Use environment variables for sensitive values: ${env:API_KEY} Bind endpoints to localhost for local-only access Apply TLS certificates for production environments Use the type/name syntax for multiple instances of the same component type For detailed configuration guidance, see the Configuration Documentation.\nDeployment Patterns The Collector supports multiple deployment models based on operational requirements and scale.\nAgent Mode In agent mode, lightweight Collector instances run as daemons alongside applications (on the same host, container, or Kubernetes pod). This pattern:\nCollects telemetry from co-located services Performs local sampling and aggregation Reduces network overhead by local processing Simplifies application configuration (default OTLP exporters assume localhost:4317) Use when: You need local collection with per-host resource limits and want to offload telemetry handling from application processes.\nGateway Mode In gateway mode, centralized Collector instances receive data from distributed agents and application libraries, then route to backend systems. This pattern:\nProvides centralized control over data transformation Enables consistent sampling decisions across services Supports complex routing logic to multiple backends Allows for sensitive data filtering before egress Use when: You need centralized policy enforcement, multi-backend routing, or want to isolate backend credentials from application environments.\nHybrid Mode Many production deployments use both agent and gateway patterns:\nAgents perform local collection and basic processing Gateways handle aggregation, complex transformations, and backend routing This hybrid approach balances local efficiency with centralized control.\nNo Collector For development environments or initial experimentation, services can export directly to backends using OTLP. However, this approach loses the benefits of buffering, retries, and transformation capabilities.\nWhen to Use the Collector Recommended Use Cases Deploy a Collector when you need to:\nSupport multiple telemetry formats (Jaeger, Prometheus, custom formats) Route telemetry to multiple backends simultaneously Perform data transformation or enrichment before export Sample or filter telemetry based on configurable rules Isolate backend credentials from application deployments Buffer telemetry during backend outages Offload retry and batching logic from applications Direct Export Scenarios Direct service-to-backend export may be sufficient for:\nDevelopment and testing environments Single-backend deployments with no transformation needs Prototyping and proof-of-concept work Very small-scale deployments Integration Points BattleBots Observability Requirements For the BattleBots platform, the Collector would support:\nBattle Event Tracking: Collecting logs of bot actions and game state changes Performance Metrics: Gathering metrics on bot response times and system resource usage Distributed Tracing: Tracking request flows across client/server or P2P architectures See User Journey 0001: POC for observability requirements context.\nMulti-Signal Correlation The Collector enables correlation between:\nTrace spans and related logs (via trace context) Metrics and traces (via exemplars) Resource attributes across all signals This correlation is particularly valuable for debugging battle scenarios where you need to understand both the timeline of events (traces), the quantitative measurements (metrics), and the detailed context (logs).\nFurther Reading Official Documentation OpenTelemetry Collector Main Documentation Collector Architecture Configuration Reference Component Registry Specifications OTLP Specification OpenTelemetry Specification Implementation Resources Collector GitHub Repository Collector Contrib Repository (additional components) Related Analysis Documents Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"High-level architectural overview of the OpenTelemetry Collector, covering core components, deployment patterns, and use cases.\n","excerpt":"High-level architectural overview of the OpenTelemetry Collector, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/opentelemetry-collector-overview/","tags":"","title":"OpenTelemetry Collector Overview"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting log data from various sources to multiple backends. Unlike traditional logging agents that focus on specific formats or destinations, the Collector treats logs as first-class observability signals alongside metrics and traces.\nThe Collector’s log support enables correlation between logs and traces through shared execution context (TraceId and SpanId), allowing unified observability across all three signal types. This correlation is particularly valuable for debugging complex distributed systems where understanding both the quantitative measurements and the detailed event context is essential.\nKey capabilities include parsing structured and unstructured log formats, enriching logs with resource attributes, filtering and transforming log content, and routing logs to multiple backends simultaneously.\nKey Concepts OTLP Logs Data Model OpenTelemetry defines a standardized log data model to establish a common understanding of what a LogRecord is and what data needs to be recorded, transferred, stored, and interpreted by logging systems.\nLogRecord Structure:\nA LogRecord contains several key components:\nTimestamp: The moment in time when the event occurred TraceId and SpanId: Execution context identifiers enabling correlation between logs and traces Resource: Describes the origin of the log (host name, container name, pod name, etc.) Instrumentation Scope: Identifies the library or component that generated the log Severity: Indicates the importance or criticality of the log entry Body: The actual log message content (string, structured data, or binary) Attributes: Structured key-value pairs providing additional context This standardized model allows logs from different sources to be processed uniformly while preserving correlation capabilities.\nLog Correlation The logs data model enables correlation across three dimensions:\nTemporal correlation: Based on timestamp alignment Execution context correlation: Using TraceId and SpanId to link logs to specific trace spans Origin correlation: Through Resource context describing the source infrastructure and application This unified approach allows observability backends to perform exact and unambiguous correlation between logs, metrics, and traces.\nLog Receivers Receivers collect log data from various sources and convert it into the OpenTelemetry logs data model.\nFilelog Receiver The filelog receiver tails and parses logs from files, making it ideal for collecting logs from applications that write to local disk.\nKey Capabilities:\nFile monitoring: Tracks multiple log files using glob patterns for inclusion and exclusion Automatic rotation handling: Detects and follows rotated log files and symlinks Compression support: Reads gzip-compressed files with auto-detection Multiline support: Combines log entries spanning multiple lines using custom patterns Format parsing: Built-in parsers for JSON, regex patterns, and structured text Metadata extraction: Parses timestamps, severity levels, and custom fields Persistent offsets: Maintains file positions across collector restarts Example use cases:\nCollecting application logs written to /var/log/ Parsing container logs from Kubernetes Reading structured JSON logs from microservices Configuration note: The filelog receiver uses a pipeline of operators that transform raw file content into structured LogRecords. Each operator performs a specific transformation (parsing, extraction, modification) before passing data to the next operator.\nOTLP Receiver The OTLP receiver accepts log data transmitted using the OpenTelemetry Protocol.\nSupported transports:\ngRPC (default port 4317): Uses unary requests with ExportLogsServiceRequest messages HTTP (default port 4318): POST requests to /v1/logs endpoint Encoding formats:\nBinary Protobuf (application/x-protobuf) JSON Protobuf (proto3 JSON mapping) Use when: Collecting logs directly from applications instrumented with OpenTelemetry SDKs or from upstream OpenTelemetry Collectors in a multi-tier deployment.\nSyslog Receiver The syslog receiver listens for syslog messages over TCP or UDP, supporting RFC 3164 and RFC 5424 formats.\nUse cases:\nCollecting system logs from Linux/Unix hosts Receiving logs from network devices (routers, switches, firewalls) Integrating with legacy applications that use syslog Other Log Receivers The OpenTelemetry Collector ecosystem includes receivers for:\njournald: Reads logs from systemd journal tcplog/udplog: Generic TCP/UDP log receivers windowseventlog: Collects Windows Event Logs kafka: Consumes logs from Kafka topics For a comprehensive list, see the Receiver Components documentation.\nLog Processors Processors transform, filter, and enrich log data as it flows through the pipeline.\nAttributes Processor The attributes processor modifies attributes of log records.\nCapabilities:\nInsert new attributes Update existing attributes Delete attributes Hash attribute values for privacy Extract values from one attribute to another Common use cases:\nAdding environment labels (e.g., environment=production) Removing sensitive data from log attributes Normalizing attribute names across different sources For more details, see the Mastering the OpenTelemetry Attributes Processor guide.\nFilter Processor The filter processor drops log records that match specified conditions using the OpenTelemetry Transformation Language (OTTL).\nCapabilities:\nFilter by log severity level Drop logs matching specific patterns Exclude logs from certain resources Reduce log volume by dropping debug logs in production Example scenarios:\nDropping health check logs to reduce noise Filtering out logs below a certain severity threshold Excluding logs from specific namespaces or services See also the Filter Processor for OpenTelemetry Collector documentation.\nTransform Processor The transform processor modifies log records using OTTL statements.\nCapabilities:\nParse log body content into structured attributes Modify log severity based on content Extract values using regex or JSON path Compute new attributes from existing ones Normalize timestamps Use cases:\nConverting unstructured log messages to structured attributes Extracting user IDs or request IDs from log text Standardizing log formats from multiple sources For transformation guidance, see Transforming telemetry.\nResource Detection Processor The resource detection processor enriches logs with metadata about their execution environment:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information (Docker, containerd) Host information (hostname, OS, architecture) This automatic enrichment enables filtering and grouping logs by infrastructure context without manual configuration.\nBatch Processor The batch processor groups log records before sending to exporters, improving throughput and reducing network overhead.\nConfiguration considerations:\nTimeout: Maximum time to wait before sending a batch Batch size: Number of log records per batch Memory limits: Prevents excessive memory usage Batching is recommended for production deployments to optimize resource usage.\nLog Exporters Exporters send processed log data to observability backends and storage systems.\nOTLP HTTP Exporter The OTLP HTTP exporter is the recommended exporter for modern observability backends that support native OTLP ingestion.\nSupported destinations:\nGrafana Loki (v3+): Send logs to Loki’s native OTLP endpoint at http://loki:3100/otlp Elasticsearch: Use OTLP-compatible endpoints Commercial backends: Datadog, New Relic, Honeycomb, Dynatrace Important: The Loki-specific exporter is deprecated. Use the standard otlphttp/logs exporter for Loki v3+ which supports native OTLP ingestion.\nFor setup guidance, see Getting started with the OpenTelemetry Collector and Loki tutorial.\nElasticsearch Exporter The Elasticsearch exporter sends logs, metrics, traces, and profiles directly to Elasticsearch.\nSupported versions:\nElasticsearch 7.17.x Elasticsearch 8.x Elasticsearch 9.x Features:\nIndex routing based on log attributes Dynamic index naming with time-based patterns Bulk API for efficient ingestion File Exporter The file exporter writes logs to local files, useful for:\nDebugging collector pipelines Creating log archives Forwarding to systems that process files Note: Not recommended for production logging backends—use OTLP or dedicated exporters instead.\nLogging Exporter The logging (debug) exporter writes logs to the collector’s standard output. Use this for:\nDevelopment and testing Troubleshooting pipeline configuration Verifying data transformation For additional exporters, see the Exporter Components documentation.\nLog Pipeline Flow A typical log pipeline in the Collector follows this pattern:\ngraph LR A[Log Files] --\u003e B[Filelog Receiver] C[OTLP SDK] --\u003e D[OTLP Receiver] B --\u003e E[Resource Detection] D --\u003e E E --\u003e F[Transform Processor] F --\u003e G[Filter Processor] G --\u003e H[Attributes Processor] H --\u003e I[Batch Processor] I --\u003e J[OTLP HTTP Exporter] J --\u003e K[Loki] I --\u003e L[Elasticsearch Exporter] L --\u003e M[Elasticsearch] style A fill:#e1f5ff style C fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#e8f5e9 style L fill:#e8f5e9 style K fill:#e1f5ff style M fill:#e1f5ff Configuration Considerations Multiline Log Handling Many applications emit logs spanning multiple lines (e.g., stack traces, JSON objects). The filelog receiver supports multiline patterns to combine these entries:\nreceivers: filelog: include: [/var/log/app/*.log] multiline: line_start_pattern: '^\\d{4}-\\d{2}-\\d{2}' Parsing Structured Logs For JSON-formatted logs, configure the filelog receiver with JSON parsing:\nreceivers: filelog: include: [/var/log/app/*.log] operators: - type: json_parser timestamp: parse_from: attributes.time layout: '%Y-%m-%dT%H:%M:%S.%fZ' Log Volume Management High log volumes can overwhelm collectors and backends. Strategies include:\nSampling: Use the probabilistic sampler processor to keep a percentage of logs Filtering: Drop debug/trace logs in production using the filter processor Batching: Configure appropriate batch sizes to balance latency and throughput Tail sampling: Keep only logs associated with interesting traces Performance Tuning For high-throughput log collection:\nIncrease the number of concurrent file readers in filelog receiver Tune batch processor settings (size, timeout) Use multiple collector instances with load balancing Consider gateway deployment to centralize processing Integration Points BattleBots Log Collection For the BattleBots platform, log collection would capture:\nGame events: Bot actions, state transitions, match outcomes System logs: Server startup, configuration changes, errors Client logs: User actions, connection events, performance issues The filelog receiver can parse structured JSON logs from the game server while the OTLP receiver collects logs directly from instrumented Go services.\nLog-Trace Correlation When both logs and traces are collected, correlation enables:\nFinding all logs for a specific trace (query by TraceId) Jumping from a log entry to its parent trace span Identifying logs that occurred during slow requests This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically.\nCross-Signal Analysis Logs complement metrics and traces:\nMetrics show aggregate patterns (error rate spike) Traces show request flow (which service failed) Logs show detailed context (exception message, variable values) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Logs Specification OTLP Specification Receiver Components Processor Components Exporter Components Transforming Telemetry Component-Specific Resources Filelog Receiver Attributes Processor Guide Filter Processor Guide Transform Processor Elasticsearch Exporter Integration Guides Ingesting logs to Loki using OpenTelemetry Collector Getting started with the OpenTelemetry Collector and Loki tutorial Honeycomb Filter Processor Documentation Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles log data, including receivers, processors, exporters, and the OTLP logs data model.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles log data, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/otel-collector-logs/","tags":"","title":"OpenTelemetry Collector: Logs Support"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for collecting, processing, and exporting metrics data from diverse sources to multiple backends. It serves as a bridge between different metrics ecosystems, enabling seamless integration of Prometheus metrics, host system metrics, and custom application metrics within a unified observability platform.\nThe Collector’s metrics support emphasizes signal correlation—connecting metrics to traces through exemplars and enriching attributes via Baggage and Context. This enables powerful observability patterns such as jumping from a metric anomaly to related traces or finding metrics that explain slow trace spans.\nKey capabilities include scraping Prometheus endpoints, collecting host system metrics, transforming metric formats, aggregating data points, and routing metrics to multiple backends simultaneously while handling different temporality preferences.\nKey Concepts OpenTelemetry Metrics Data Model The OpenTelemetry Metrics data model defines how metrics are represented and processed. The data model serves to:\nCapture raw measurements efficiently and simultaneously Decouple instrumentation from the SDK implementation Enable correlation with traces and logs Support migration from OpenCensus and Prometheus Architecture layers:\nMeterProvider \u0026 Instruments: Applications collect measurements through Meters and their associated Instruments In-Memory Aggregation: Measurements aggregate into an intermediate representation MetricReader: Processes aggregated metrics for export to backends Metric Types OpenTelemetry supports four primary metric types, each suited for different measurement scenarios.\nCounter (Sum) Counters represent cumulative or delta measurements that can only increase over time (or be reset to zero). Common examples include:\nRequest count Error count Bytes transmitted Items processed Characteristics:\nMonotonically increasing Supports both delta and cumulative temporality Can be aggregated across instances Typically visualized as rate-of-change For detailed specifications, see OTLP Metrics Types.\nGauge Gauges represent sampled values that can arbitrarily increase or decrease over time. Unlike counters, gauges are not cumulative—they reflect the current value at the time of measurement.\nCommon examples include:\nCPU usage percentage Memory utilization Queue depth Active connection count Temperature readings Characteristics:\nNon-monotonic (can increase or decrease) No aggregation temporality (uses “last sample value”) Represents point-in-time state Cannot be meaningfully aggregated across instances without additional context Histogram Histograms convey a population of recorded measurements in a compressed format by grouping measurements into configurable buckets. This enables statistical analysis without storing individual data points.\nCommon examples include:\nRequest latency distribution Response size distribution Query execution time Message size distribution Characteristics:\nProvides count, sum, and bucket distributions Supports both delta and cumulative temporality Enables percentile calculations (p50, p95, p99) More efficient than storing individual measurements Histograms are particularly valuable for understanding the distribution of latency or size measurements, revealing whether most requests are fast with occasional slow outliers, or if performance degrades uniformly.\nSummary Summaries provide pre-calculated quantile values (percentiles) over a time window. Unlike histograms, which send bucket distributions for backend calculation, summaries compute quantiles client-side.\nImportant: Summary points cannot always be merged meaningfully. This point type is not recommended for new applications and exists primarily for compatibility with other formats like Prometheus summaries.\nFor comprehensive metric type details, see OpenTelemetry Metrics.\nTemporality Temporality defines how metric values are accumulated and reported over time. OpenTelemetry supports two aggregation temporality modes:\nDelta Temporality Delta temporality reports the change since the last collection period. Each data point represents only new measurements since the previous export.\nCharacteristics:\nNon-overlapping time windows Measures rate of change Preferred by some backends (StatsD, Carbon) Requires stateless aggregation Example: A request counter shows +100 requests in period 1, then +150 requests in period 2.\nCumulative Temporality Cumulative temporality reports the total value since process start (or a fixed start point). Each data point includes all measurements from the beginning.\nCharacteristics:\nOverlapping time windows from fixed start Accumulates over application lifetime Preferred by Prometheus Resilient to collection gaps Example: A request counter shows 100 total requests in period 1, then 250 total requests in period 2.\nImportant note: Set the temporality preference to DELTA when possible, as setting it to CUMULATIVE may discard some data points during application or collector startup. However, Prometheus backends require cumulative temporality.\nFor more details, see OpenTelemetry Metrics Aggregation.\nMetrics Receivers Receivers collect metrics data from various sources and convert it into the OpenTelemetry metrics data model.\nPrometheus Receiver The Prometheus receiver enables the OpenTelemetry Collector to act as a Prometheus server by scraping Prometheus-compatible endpoints, then converting the metrics into OTLP format.\nKey capabilities:\nScrapes any Prometheus /metrics endpoint Supports service discovery mechanisms Converts Prometheus metrics to OpenTelemetry format Handles metric relabeling and filtering Scales with Target Allocator for large deployments Configuration example:\nreceivers: prometheus: config: scrape_configs: - job_name: 'otel-collector' scrape_interval: 15s static_configs: - targets: ['localhost:8888'] Scaling with Target Allocator:\nThe Target Allocator decouples service discovery and metric collection, allowing independent scaling. Each Collector pod registers with the Target Allocator, which uses consistent hashing to distribute discovered targets evenly among active Collectors, ensuring each target is scraped exactly once without overlap.\nFor comprehensive guidance, see Prometheus and OpenTelemetry Collector Integration.\nHost Metrics Receiver The hostmetrics receiver collects comprehensive system-level metrics from host machines, providing visibility into infrastructure health.\nAvailable scrapers:\ncpu: CPU utilization, time, and frequency disk: Disk I/O operations and throughput filesystem: Filesystem usage and available space memory: Memory utilization and swap usage network: Network interface statistics and errors load: System load averages paging: Paging and swapping activity processes: Process count and resource usage Configuration example:\nreceivers: hostmetrics: collection_interval: 30s scrapers: cpu: disk: filesystem: memory: network: The hostmetrics receiver is essential for infrastructure monitoring and provides context for application-level metrics. When deployed in Kubernetes, appropriate volumes and volumeMounts are automatically configured when the hostMetrics preset is enabled.\nOTLP Receiver The OTLP receiver accepts metrics data transmitted using the OpenTelemetry Protocol from instrumented applications or upstream collectors.\nSupported transports:\ngRPC (default port 4317) HTTP (default port 4318, endpoint /v1/metrics) Use cases:\nCollecting metrics directly from OpenTelemetry SDKs Multi-tier collector deployments (agent → gateway) Receiving metrics from serverless functions Other Metrics Receivers The ecosystem includes receivers for diverse metrics sources:\nstatsd: Receives StatsD protocol metrics kafka: Consumes metrics from Kafka topics influxdb: Receives InfluxDB line protocol carbon: Receives Graphite carbon metrics collectd: Receives collectd metrics postgresql: Scrapes PostgreSQL metrics redis: Scrapes Redis metrics mongodb: Scrapes MongoDB metrics For a complete list, see the Receiver Components documentation.\nMetrics Processors Processors transform and enrich metrics data as it flows through pipelines.\nMetrics Transform Processor The metrics transform processor modifies metric names, types, and attributes using transformation rules.\nCommon operations:\nRename metrics for consistency Change metric types (e.g., gauge to counter) Add or modify resource attributes Aggregate metrics across dimensions Filter Processor The filter processor drops metrics matching specified conditions, reducing data volume and costs.\nUse cases:\nDropping debug metrics in production Filtering metrics from test environments Excluding high-cardinality metrics Removing specific metric names or attribute values Cumulative to Delta Processor This processor converts cumulative temporality metrics to delta temporality, useful when backends prefer delta metrics.\nAttributes Processor Adds, updates, or deletes metric attributes and resource attributes, enabling:\nEnvironment labeling (environment=production) Team ownership tags (team=platform) Cost allocation labels Normalization across different metric sources Batch Processor Groups metrics before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nConfiguration considerations:\nBatch size: Number of metric data points per batch Timeout: Maximum wait before sending partial batch Memory limits: Prevents unbounded memory growth Metrics Exporters Exporters send processed metrics to observability backends and time-series databases.\nOTLP Exporter The OTLP exporter sends metrics using the OpenTelemetry Protocol to OTLP-compatible backends.\nSupported destinations:\nOpenTelemetry-native backends Cloud vendor endpoints (AWS CloudWatch, Google Cloud Monitoring, Azure Monitor) Commercial observability platforms (Datadog, New Relic, Honeycomb) Important: OTLP is now the recommended protocol for sending metrics to modern backends. For example, Prometheus can now accept OTLP directly:\nexport OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http://localhost:9090/api/v1/otlp/v1/metrics Prometheus Remote Write Exporter The Prometheus Remote Write exporter sends OpenTelemetry metrics to Prometheus remote write compatible backends such as:\nCortex Grafana Mimir Thanos Amazon Managed Service for Prometheus Google Cloud Managed Prometheus Capabilities:\nTLS support (required by default) Queued retry mechanisms Authentication options (basic auth, bearer token, OAuth2) Important limitation: Non-cumulative monotonic, histogram summary, and exponential histogram OTLP metrics are dropped by this exporter.\nFor Grafana Mimir specifically, it’s recommended to use OTLP rather than Prometheus remote write.\nPrometheus Exporter The Prometheus exporter exposes metrics in Prometheus format on an HTTP endpoint for Prometheus servers to scrape.\nUse cases:\nExisting Prometheus deployments Push-based collection converted to pull-based Multi-backend export (push to one, expose for scraping by another) File Exporter Writes metrics to local files for debugging, archival, or processing by batch systems.\nFor additional exporters, see the Exporter Components documentation.\nMetrics Pipeline Flow A typical metrics pipeline demonstrates collection, processing, and export to multiple backends:\ngraph LR A[Prometheus Endpoints] --\u003e B[Prometheus Receiver] C[Host System] --\u003e D[Host Metrics Receiver] E[OTLP SDK] --\u003e F[OTLP Receiver] B --\u003e G[Attributes Processor] D --\u003e G F --\u003e G G --\u003e H[Filter Processor] H --\u003e I[Transform Processor] I --\u003e J[Batch Processor] J --\u003e K[OTLP Exporter] K --\u003e L[Prometheus Server] J --\u003e M[Prometheus Remote Write] M --\u003e N[Grafana Mimir] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#e8f5e9 style M fill:#e8f5e9 style L fill:#e1f5ff style N fill:#e1f5ff Configuration Considerations Temporality Management Different backends have different temporality preferences:\nPrometheus: Requires cumulative temporality StatsD-like systems: Prefer delta temporality Cloud vendors: Often accept both Configure the Collector to convert between temporalities based on backend requirements using the cumulative-to-delta processor.\nCardinality Control High cardinality metrics (many unique label combinations) can overwhelm backends and increase costs. Strategies include:\nFiltering high-cardinality dimensions Aggregating metrics before export Dropping rarely-used labels Using metric relabeling to reduce dimensions Scrape Interval Tuning Balance data freshness with resource consumption:\nShort intervals (5-15s): Real-time monitoring, higher costs Medium intervals (30-60s): Standard monitoring Long intervals (5m+): Capacity planning, cost optimization Exemplar Support Exemplars link metrics to traces by attaching trace IDs to specific metric data points. This enables:\nJumping from a high-latency histogram bucket to example slow traces Finding traces that contributed to an error rate spike Correlating metrics anomalies with detailed trace analysis Enable exemplar support in the Prometheus receiver and ensure trace context propagation in applications.\nIntegration Points BattleBots Metrics Collection For the BattleBots platform, metrics collection would capture:\nGame Metrics:\nMatch duration and outcome distribution Bot action rates (attacks, defenses, moves) Game state transition frequency Performance Metrics:\nRequest latency percentiles WebSocket connection counts Message throughput rates Server CPU and memory usage Business Metrics:\nActive user count Matches per hour Bot creation rate The combination of Prometheus receiver (for Go runtime metrics), hostmetrics receiver (for infrastructure), and OTLP receiver (for custom metrics) provides comprehensive visibility.\nMetrics-Trace Correlation Connecting metrics and traces enables powerful workflows:\nAlerting on metrics: High error rate triggers investigation Drill-down to traces: Click exemplar to see example failing requests Root cause analysis: Examine detailed trace spans to identify cause Fix validation: Monitor metrics to confirm fix effectiveness This requires:\nApplications emit both metrics and traces Exemplars enabled in metric collection Unified storage backend (or cross-backend linking) Cross-Signal Analysis Metrics complement logs and traces:\nMetrics identify anomalies at scale (response time spike) Traces show affected request flows (which service is slow) Logs provide detailed context (exception messages, stack traces) The Collector’s unified data model enables seamless correlation across all three signals.\nFurther Reading Official Documentation OpenTelemetry Metrics Specification Metrics Data Model Receiver Components Processor Components Exporter Components Integration Guides Collecting Prometheus Metrics with the OpenTelemetry Collector Prometheus and OpenTelemetry Collector Integration OpenTelemetry Host Metrics receiver Using Prometheus as your OpenTelemetry backend Configure the OpenTelemetry Collector to write metrics into Mimir How to collect Prometheus metrics with the OpenTelemetry Collector and Grafana Component-Specific Resources Prometheus Remote Write Exporter OpenTelemetry Collector Chart (Kubernetes) Prometheus and OpenTelemetry - Better Together Analysis and Best Practices OTLP Metrics Types Understanding OpenTelemetry Metrics OpenTelemetry Metrics Aggregation How Prometheus Exporters Work With OpenTelemetry Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Traces Support - How the Collector handles distributed traces Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles metrics data, including the metrics data model, receivers, processors, exporters, and temporality concepts.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles metrics data, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/otel-collector-metrics/","tags":"","title":"OpenTelemetry Collector: Metrics Support"},{"body":"Overview The OpenTelemetry Collector is designed as an observable service itself, following the principle that observability infrastructure must be observable. The Collector exposes its own telemetry (metrics, logs, and optionally traces) to enable monitoring health, diagnosing issues, and optimizing performance.\nSelf-monitoring is critical for production deployments—without visibility into the Collector’s operation, data loss or performance degradation can go undetected. The Collector provides built-in telemetry, diagnostic extensions, and debugging capabilities to ensure reliable operation at scale.\nKey capabilities include internal metrics exposed via Prometheus endpoints, health check endpoints for liveness/readiness probes, diagnostic extensions for real-time inspection, and structured logging for troubleshooting pipeline issues.\nKey Concepts Internal Telemetry The OpenTelemetry Collector generates internal telemetry by default to expose its operational state. This self-generated observability data helps operators monitor Collector health and performance.\nTelemetry types:\nMetrics: Quantitative measurements of Collector operation (default: Prometheus format on port 8888) Logs: Structured event records emitted to stderr by default Traces: Optional internal tracing of data flow through pipelines Internal telemetry enables:\nReal-time health monitoring Capacity planning and resource optimization Troubleshooting data loss or pipeline issues Performance profiling and bottleneck identification Observability vs. Debugging The Collector provides two complementary approaches:\nObservability (production):\nContinuous metrics collection Health check endpoints Structured logging External monitoring integration Debugging (development/troubleshooting):\nzPages for live data inspection pprof for performance profiling Debug exporters for pipeline validation Verbose logging modes Internal Metrics The Collector exposes comprehensive metrics about its operation through a Prometheus-compatible endpoint.\nMetrics Endpoint By default, the Collector exposes metrics at http://localhost:8888/metrics in Prometheus format. This endpoint can be scraped by Prometheus or any compatible metrics collector.\nConfiguration:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed # Options: none, basic, normal, detailed Key Metrics Categories Data Ingress Metrics Monitor data received by receivers:\notelcol_receiver_accepted_log_records - Log records accepted otelcol_receiver_accepted_spans - Spans accepted otelcol_receiver_accepted_metric_points - Metric points accepted otelcol_receiver_refused_* - Refused data (errors) These metrics help identify:\nData ingestion rates Receiver errors or rejections Source-specific throughput patterns Data Egress Metrics Monitor data sent by exporters:\notelcol_exporter_sent_log_records - Log records sent otelcol_exporter_sent_spans - Spans sent otelcol_exporter_sent_metric_points - Metric points sent otelcol_exporter_send_failed_* - Failed export attempts These metrics reveal:\nExport success rates Backend connectivity issues Data loss from failed exports Queue Metrics Monitor internal buffer state:\notelcol_exporter_queue_capacity - Queue capacity in batches otelcol_exporter_queue_size - Current queue utilization otelcol_exporter_enqueue_failed_* - Failed enqueues (buffer full) Alert on: Queue size approaching capacity indicates backpressure from slow exporters or high ingestion rates.\nProcessor Metrics Monitor processor operation:\notelcol_processor_batch_batch_send_size - Batch sizes sent otelcol_processor_batch_timeout_trigger_send - Timeouts triggering sends Processor-specific metrics (sampling rates, dropped data, etc.) Resource Metrics Monitor Collector resource usage:\nprocess_runtime_* - Go runtime metrics (memory, goroutines) process_cpu_seconds_total - CPU time consumed process_resident_memory_bytes - Memory usage For comprehensive metric details, see Internal telemetry and How to Monitor Open Telemetry Collector Performance.\nSelf-Monitoring Dashboards Several platforms provide pre-built dashboards for Collector monitoring:\nDynatrace OpenTelemetry Collector Self-Monitoring (June 2025 release) Grafana dashboards from the community Vendor-specific monitoring integrations Extensions for Observability Extensions provide auxiliary functionality that supports Collector operation and debugging.\nHealth Check Extension The health_check extension enables an HTTP endpoint that can be probed to check the Collector’s status.\nConfiguration:\nextensions: health_check: endpoint: 0.0.0.0:13133 path: /health/status check_collector_pipeline: enabled: false # Not recommended—use at own risk service: extensions: [health_check] Endpoints:\n/health/status - Returns 200 OK if the Collector is running Important note: The check_collector_pipeline feature is not working as expected and should not be used. Use metrics-based monitoring instead for pipeline health.\nUse cases:\nKubernetes liveness probes Load balancer health checks Container orchestration health monitoring Service mesh integration For more details, see Health Check Monitoring With OpenTelemetry.\nzPages Extension The zpages extension serves HTTP endpoints that provide live data for debugging different components without depending on external backends.\nConfiguration:\nextensions: zpages: endpoint: 0.0.0.0:55679 service: extensions: [zpages] Available pages:\n/debug/servicez - Service summary and version information /debug/pipelinez - Pipeline configuration and status /debug/extensionz - Loaded extensions /debug/tracez - Sample trace data (if internal tracing enabled) Use cases:\nInspecting live data flowing through pipelines Validating configuration changes Debugging data transformation issues In-process diagnostics during development zPages are particularly useful for answering questions like “Is the Collector receiving data?” and “What does the data look like after processing?”\nFor detailed usage, see Monitoring and Debugging the OpenTelemetry Collector.\npprof Extension The pprof extension enables the Go net/http/pprof endpoint for performance profiling.\nConfiguration:\nextensions: pprof: endpoint: 0.0.0.0:1777 service: extensions: [pprof] Available profiles:\n/debug/pprof/profile - CPU profile /debug/pprof/heap - Memory allocation profile /debug/pprof/goroutine - Goroutine stack traces /debug/pprof/block - Blocking profile /debug/pprof/mutex - Mutex contention profile Use cases:\nInvestigating CPU hotspots Analyzing memory leaks Identifying goroutine leaks Profiling lock contention Collecting profiles:\n# CPU profile (30 seconds) curl http://localhost:1777/debug/pprof/profile?seconds=30 \u003e cpu.prof # Heap profile curl http://localhost:1777/debug/pprof/heap \u003e heap.prof # Analyze with pprof go tool pprof cpu.prof Security note: pprof endpoints should only be exposed internally, never to the public internet, as they can reveal sensitive information and consume resources.\nLogging and Debugging Structured Logging The Collector emits structured logs to stderr by default, which can be redirected to files or collected by log aggregation systems.\nLog levels:\ndebug - Verbose debugging information info - General operational messages (default) warn - Warning conditions error - Error conditions Configuration:\nservice: telemetry: logs: level: info encoding: json # Options: json, console Common log patterns:\nReceiver connection failures Exporter send failures Processor errors Configuration validation warnings Debug Exporter The debug (logging) exporter writes telemetry data to the Collector’s standard output, useful for confirming that data is being received, processed, and exported correctly.\nConfiguration:\nexporters: debug: verbosity: detailed # Options: basic, normal, detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, otlp] # Add debug alongside production exporters Use cases:\nValidating pipeline configuration Inspecting data transformations Troubleshooting receiver issues Confirming data format Warning: Debug exporters should be removed or disabled in production due to performance impact and log volume.\nFor troubleshooting guidance, see the Troubleshooting documentation.\nCommon Issues and Troubleshooting Data Loss Symptoms:\notelcol_exporter_send_failed_* metrics increasing Queue size approaching capacity Export errors in logs Common causes:\nExporter destination unavailable or slow Collector under-provisioned (insufficient CPU/memory) Network connectivity issues Backend rate limiting Solutions:\nIncrease queue size and retry parameters Scale Collector instances horizontally Add buffering through load balancers Implement backpressure handling High Memory Usage Symptoms:\nprocess_resident_memory_bytes continuously increasing OOM kills in container environments Slow garbage collection Common causes:\nLarge batch sizes Tail sampling buffer accumulation Queue size too large Memory leaks in processors Solutions:\nReduce batch size and timeout Tune tail sampling buffer limits Enable memory limiting in batch processor Update to latest Collector version (bug fixes) Use pprof to identify memory leaks Note: Memory usage increases in steps due to Go’s garbage collection characteristics, which is normal.\nCPU Spikes Symptoms:\nprocess_cpu_seconds_total rate spikes Request latency increases Throttled container CPU Common causes:\nBatch processing overhead Complex processor logic High ingestion rates Inefficient regex patterns in processors Solutions:\nOptimize processor configuration Distribute load across multiple instances Use simpler transformation patterns Profile with pprof to identify hotspots For detailed debugging workflows, see Guide — How to Debug OpenTelemetry Pipelines.\nSelf-Monitoring Architecture A production self-monitoring setup exports Collector telemetry to external systems:\ngraph TB A[OTel Collector Instance] --\u003e|Internal Metrics| B[Prometheus Exporter :8888] A --\u003e|Internal Logs| C[Stderr Logs] A --\u003e|Health Checks| D[Health Check Extension :13133] A --\u003e|Debugging| E[zPages Extension :55679] A --\u003e|Profiling| F[pprof Extension :1777] B --\u003e|Scrape| G[Prometheus/Metrics Backend] C --\u003e|Collect| H[Log Aggregation System] D --\u003e|Probe| I[Kubernetes/Load Balancer] G --\u003e J[Alerting \u0026 Dashboards] H --\u003e J K[Monitoring Collector] --\u003e|Scrape :8888| B K --\u003e|Send to Backends| L[External Observability Platform] style A fill:#e1f5ff style B fill:#fff4e6 style C fill:#fff4e6 style D fill:#fff4e6 style E fill:#f3e5f5 style F fill:#f3e5f5 style G fill:#e8f5e9 style H fill:#e8f5e9 style I fill:#e8f5e9 style J fill:#ffe6e6 style K fill:#e1f5ff style L fill:#e8f5e9 Configuration Best Practices Production Monitoring Setup 1. Enable comprehensive internal metrics:\nservice: telemetry: metrics: address: 0.0.0.0:8888 level: detailed logs: level: info encoding: json 2. Deploy monitoring Collector:\nCreate a dedicated Collector instance to scrape other Collectors:\nreceivers: prometheus: config: scrape_configs: - job_name: otel-collector scrape_interval: 15s static_configs: - targets: ['collector-1:8888', 'collector-2:8888'] exporters: otlp: endpoint: monitoring-backend:4317 service: pipelines: metrics: receivers: [prometheus] exporters: [otlp] 3. Configure health checks:\nextensions: health_check: endpoint: 0.0.0.0:13133 service: extensions: [health_check] 4. Set up alerts:\nKey alerts to configure:\nQueue size \u003e 80% capacity Export failure rate \u003e 1% Memory usage \u003e 80% limit CPU throttling detected Receiver refused rate \u003e 0 Development/Debugging Setup Enable all diagnostic extensions:\nextensions: health_check: endpoint: 0.0.0.0:13133 zpages: endpoint: 0.0.0.0:55679 pprof: endpoint: 0.0.0.0:1777 service: extensions: [health_check, zpages, pprof] telemetry: logs: level: debug Add debug exporters:\nexporters: debug: verbosity: detailed service: pipelines: traces: receivers: [otlp] exporters: [debug, jaeger] # Debug alongside production Security Considerations Restrict extension endpoints to internal networks only Never expose pprof to the internet Use TLS for metrics endpoints in production Implement authentication for sensitive endpoints Rate limit health check endpoints to prevent DoS Integration Points BattleBots Collector Monitoring For the BattleBots platform, Collector self-monitoring would track:\nOperational metrics:\nGame event ingestion rate (log records/second) Battle trace throughput (spans/second) Bot performance metric collection (metric points/second) Health indicators:\nExport success rate to observability backends Queue utilization during peak match activity Resource usage (CPU, memory) per Collector instance Alerting scenarios:\nQueue capacity exceeded during tournament events Export failures to game analytics backend High latency in telemetry pipeline affecting real-time dashboards Kubernetes Integration In Kubernetes deployments:\nLiveness probe:\nlivenessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 30 periodSeconds: 10 Readiness probe:\nreadinessProbe: httpGet: path: /health/status port: 13133 initialDelaySeconds: 5 periodSeconds: 5 Metrics scraping:\nannotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8888\" prometheus.io/path: \"/metrics\" Further Reading Official Documentation Internal telemetry Troubleshooting Configuration Extensions README Extension Documentation Health Check Extension zPages Extension Collector Observability Documentation Guides and Best Practices Monitoring and Debugging the OpenTelemetry Collector How to Monitor Open Telemetry Collector Performance Guide — How to Debug OpenTelemetry Pipelines Health Check Monitoring With OpenTelemetry Vendor Resources Dynatrace: Introducing OpenTelemetry Collector Self-Monitoring Dashboards Dynatrace: OpenTelemetry Collector self-monitoring OpenTelemetry Collector from A to Z: A Production-Ready Guide Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Traces Support - How the Collector handles distributed traces ","categories":"","description":"How to observe and debug the OpenTelemetry Collector itself through internal telemetry, extensions, and monitoring strategies.\n","excerpt":"How to observe and debug the OpenTelemetry Collector itself through …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/otel-collector-self-monitoring/","tags":"","title":"OpenTelemetry Collector: Self-Monitoring"},{"body":"Overview The OpenTelemetry Collector provides comprehensive support for distributed tracing, enabling collection, processing, and export of trace data from multiple sources to various backend systems. Distributed tracing tracks requests as they flow through distributed systems, providing visibility into service interactions, latency bottlenecks, and error propagation paths.\nThe Collector acts as a central hub for trace data, accepting traces in multiple formats (OTLP, Jaeger, Zipkin), performing intelligent sampling decisions, and routing to multiple tracing backends simultaneously. This unified approach simplifies observability infrastructure while preserving the ability to use best-of-breed tools for different use cases.\nKey capabilities include protocol translation between trace formats, sophisticated sampling strategies (head-based and tail-based), trace enrichment with resource and span attributes, and correlation with metrics and logs through shared context identifiers.\nKey Concepts Traces and Spans A trace represents the full journey of one request or transaction across services, while a span is a timed unit of work inside that journey such as a function call, database query, or external API call.\nTrace structure:\nA trace consists of one or more spans organized in a tree structure Each span represents an operation with a start time and duration Spans have parent-child relationships forming the call graph The root span represents the initial request entry point Span characteristics:\nName: Describes the operation (e.g., “GET /api/battles”) Start time and duration: Timing information Status: Success, error, or unset Span kind: Client, server, internal, producer, or consumer Span Context and Propagation Span context is the portion of a span that must be serialized and propagated between services to maintain trace continuity.\nContext components:\nTraceId: Unique identifier for the entire trace (shared across all spans) SpanId: Unique identifier for the specific span TraceFlags: Sampling and other flags TraceState: System-specific trace state values Propagation mechanism:\nContext propagation transmits context between services via protocols such as HTTP headers, gRPC metadata, or message queues. The default propagator uses the W3C TraceContext specification with traceparent and tracestate headers.\nExample HTTP headers:\ntraceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01 tracestate: vendor1=value1,vendor2=value2 For detailed propagation concepts, see An overview of Context Propagation in OpenTelemetry.\nSpan Attributes Attributes provide additional context about the operation represented by a span. They are key-value pairs that describe request parameters, database queries, HTTP methods, status codes, and other relevant details.\nCommon attribute categories:\nHTTP attributes: http.method, http.status_code, http.route Database attributes: db.system, db.statement, db.name RPC attributes: rpc.service, rpc.method Network attributes: net.peer.name, net.peer.port Best practice: Set attributes at span creation rather than later, since samplers can only consider information present during span creation.\nSpan Events Span events are structured log messages or annotations on a span, typically used to denote meaningful singular points in time during the span’s duration.\nUse cases:\nException events (including stack traces) Checkpoint markers in long operations State transitions Cache hits/misses Retry attempts Events include a name, timestamp, and optional attributes, providing detailed debugging context without creating separate spans for every sub-operation.\nSpan Links Span links establish relationships between spans in different traces or between causally-related but non-parent-child spans. Common scenarios include:\nBatch processing where one span processes multiple input messages Following redirects across multiple traces Async operations spawned from a parent request Trace Receivers Receivers collect trace data from various sources and convert it into the OpenTelemetry traces data model.\nOTLP Receiver The OTLP receiver accepts trace data transmitted using the OpenTelemetry Protocol, the native and recommended format for OpenTelemetry traces.\nSupported transports:\ngRPC (default port 4317): High-performance binary protocol HTTP (default port 4318): RESTful endpoint at /v1/traces Configuration example:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 Use cases:\nCollecting traces from OpenTelemetry-instrumented applications Multi-tier collector deployments (agent → gateway) Modern observability architectures Important: Jaeger V2 natively supports OTLP, making OTLP the recommended protocol for Jaeger backends.\nJaeger Receiver The Jaeger receiver receives trace data in Jaeger format and translates it to OpenTelemetry format. This enables migration from Jaeger-instrumented applications without requiring code changes.\nSupported protocols:\ngRPC (default port 14250): Binary Jaeger protocol thrift_compact (default port 6831): UDP-based compact Thrift thrift_http (default port 14268): HTTP-based Thrift thrift_binary: TCP-based binary Thrift Configuration example:\nreceivers: jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_http: endpoint: 0.0.0.0:14268 thrift_compact: endpoint: 0.0.0.0:6831 Use cases:\nMigrating from Jaeger agent/collector infrastructure Supporting legacy applications instrumented with Jaeger SDKs Gradual transition to OpenTelemetry Zipkin Receiver The Zipkin receiver receives spans in Zipkin V1 and V2 formats and translates them to OpenTelemetry format.\nConfiguration example:\nreceivers: zipkin: endpoint: 0.0.0.0:9411 Use cases:\nMigrating from Zipkin instrumentation Supporting applications instrumented with Zipkin libraries Integration with Zipkin-compatible systems Protocol Translation The Collector acts as a protocol translator, accepting traces in one format and exporting in another. This enables:\nJaeger-instrumented apps → OTLP export to modern backends OpenTelemetry apps → Zipkin export for legacy systems Unified collection from heterogeneous instrumentation Sampling Strategies Sampling controls which traces are retained for analysis, balancing observability value with storage costs and performance impact.\nHead Sampling Head sampling makes sampling decisions at trace creation time, before seeing the complete trace. The decision applies to the entire trace and propagates to downstream services.\nCommon algorithms:\nAlways On: Sample 100% of traces (development/debugging) Always Off: Sample 0% of traces (disable tracing) TraceID Ratio: Sample a percentage based on TraceId hash (e.g., 10%) Rate Limiting: Sample at most N traces per second Advantages:\nLow latency decision (immediate) Low memory overhead (no buffering) Consistent across distributed services Limitations:\nCannot make decisions based on complete trace data Cannot guarantee capturing all error traces Cannot sample based on span attributes or duration For consistent probability sampling details, see OpenTelemetry Sampling.\nTail Sampling Tail sampling makes sampling decisions after seeing all or most spans in a trace, enabling more intelligent sampling based on trace characteristics.\nAvailable policies:\nLatency: Sample traces exceeding duration threshold Status code: Always sample traces with errors Numeric attribute: Sample based on attribute values (min/max thresholds) Probabilistic: Sample a percentage of traces String attribute: Sample traces matching string attributes Rate limiting: Limit traces per second per policy Composite: Combine multiple policies (AND/OR logic) Configuration example:\nprocessors: tail_sampling: policies: - name: errors-policy type: status_code status_code: status_codes: [ERROR] - name: slow-requests type: latency latency: threshold_ms: 1000 - name: sample-10-percent type: probabilistic probabilistic: sampling_percentage: 10 Architecture requirements:\nAll spans for a given trace MUST be received by the same collector instance for effective sampling decisions. This requires:\nLoad balancing exporter: Routes spans by TraceId to consistent collectors Two-tier architecture: Agent collectors → tail sampling gateway collectors For implementation guidance, see Tail Sampling with OpenTelemetry and New Relic and Sampling at scale with OpenTelemetry.\nAdvantages:\nSample all error traces regardless of volume Capture slow requests while dropping fast ones Make sampling decisions based on complete trace data Challenges:\nHigher memory overhead (buffering complete traces) Increased latency (waiting for trace completion) Requires stateful, coordinated collectors Sampling Best Practices For production deployments:\nUse head sampling for baseline traffic reduction (e.g., 10% sampling) Add tail sampling to always capture errors and slow traces Implement two-tier architecture for tail sampling at scale Monitor sampled vs. unsampled trace ratios Adjust policies based on traffic patterns and costs For recent sampling updates, see OpenTelemetry Sampling update.\nTrace Processors Processors transform and enrich trace data as it flows through pipelines.\nSpan Processor The span processor modifies span names, attributes, and other properties.\nCommon operations:\nRename spans for consistency Add/remove span attributes Set span status Modify span kind Attributes Processor Adds, updates, or deletes span and resource attributes, enabling:\nEnvironment labeling Team ownership tags PII removal Attribute normalization Resource Detection Processor Enriches traces with environment metadata:\nCloud provider information (AWS, GCP, Azure) Kubernetes metadata (pod, namespace, node) Container information Host details This automatic enrichment enables filtering and grouping traces by infrastructure context.\nBatch Processor Groups spans before export, improving throughput and reducing network overhead. Recommended for all production deployments.\nService Graph Processor Generates metrics representing service call relationships from trace data, creating:\nRequest rate between services Error rate between services Latency between services These derived metrics enable service dependency visualization without query-time trace aggregation.\nTrace Exporters Exporters send processed trace data to observability backends and storage systems.\nOTLP Exporter (Recommended) The OTLP exporter sends traces using the OpenTelemetry Protocol to OTLP-compatible backends. OTLP is the recommended choice for new deployments as it’s designed with the OpenTelemetry data model in mind, emitting trace data without loss of information.\nSupported destinations:\nJaeger V2 (native OTLP support) Commercial platforms (Datadog, New Relic, Honeycomb, Dynatrace) Cloud vendor endpoints (AWS X-Ray, Google Cloud Trace, Azure Monitor) Open source backends (Uptrace, Grafana Tempo) Configuration example:\nexporters: otlp: endpoint: jaeger:4317 tls: insecure: false For Jaeger V2 integration, see Using OpenTelemetry to send traces to Jaeger V2.\nJaeger Exporter The Jaeger exporter sends traces to Jaeger backends using the Jaeger gRPC protocol.\nNote: For Jaeger V2, use the OTLP exporter instead. The dedicated Jaeger exporter is maintained for backward compatibility with Jaeger V1 deployments.\nZipkin Exporter The Zipkin exporter sends traces to Zipkin-compatible backends.\nUse cases:\nLegacy Zipkin deployments Systems expecting Zipkin format Gradual migration scenarios Logging Exporter Writes traces to collector standard output for debugging and development.\nFor comprehensive exporter documentation, see OpenTelemetry Collector Exporters.\nTrace Pipeline Flow A sophisticated trace pipeline with multiple receivers, sampling, and multi-backend export:\ngraph LR A[OTLP SDK] --\u003e B[OTLP Receiver] C[Jaeger SDK] --\u003e D[Jaeger Receiver] E[Zipkin SDK] --\u003e F[Zipkin Receiver] B --\u003e G[Resource Detection] D --\u003e G F --\u003e G G --\u003e H[Attributes Processor] H --\u003e I{Load Balancer} I --\u003e|By TraceId| J[Tail Sampling Processor] J --\u003e K[Batch Processor] K --\u003e L[OTLP Exporter] L --\u003e M[Jaeger V2] K --\u003e N[OTLP Exporter] N --\u003e O[Cloud Backend] style A fill:#e1f5ff style C fill:#e1f5ff style E fill:#e1f5ff style B fill:#fff4e6 style D fill:#fff4e6 style F fill:#fff4e6 style G fill:#f3e5f5 style H fill:#f3e5f5 style I fill:#f3e5f5 style J fill:#f3e5f5 style K fill:#f3e5f5 style L fill:#e8f5e9 style N fill:#e8f5e9 style M fill:#e1f5ff style O fill:#e1f5ff Configuration Considerations Context Propagation Ensure consistent propagation across all services:\nConfigure the same propagators in all SDKs Use W3C TraceContext (standard default) Include Baggage propagation if using cross-cutting concerns Test propagation across language boundaries Sampling Trade-offs Balance observability and cost:\nHigh sampling (50-100%): Development, debugging, low-traffic systems Medium sampling (10-30%): Production with moderate traffic Low sampling (1-10%): High-traffic production systems Tail sampling: Always capture errors regardless of base rate Performance Tuning For high-throughput trace collection:\nEnable batching with appropriate size/timeout Use multiple collector instances with load balancing Configure adequate memory for tail sampling buffers Monitor collector CPU and memory usage Consider two-tier architecture (agent + gateway) Storage Optimization Manage trace storage costs:\nImplement retention policies in backends Use sampling to reduce volume Drop high-cardinality attributes if needed Compress trace data before export Integration Points BattleBots Trace Collection For the BattleBots platform, distributed tracing would track:\nRequest flows:\nClient WebSocket connection → authentication → game state sync Player action → validation → state update → broadcast Match creation → bot pairing → game initialization Service interactions:\nAPI gateway → game service → persistence layer Event publisher → message broker → subscriber services Load balancer → multiple game server instances Timing analysis:\nEnd-to-end battle action latency Database query performance WebSocket message propagation time The OTLP receiver collects traces from Go services instrumented with the OpenTelemetry Go SDK, while Jaeger/Zipkin receivers support any legacy instrumentation.\nTrace-Log Correlation Connecting traces and logs enables powerful debugging workflows:\nStart with trace: Identify slow or failing request Find associated logs: Query logs by TraceId and SpanId Examine context: Read detailed log messages and exceptions Understand causation: See timeline of events leading to issue This requires applications to inject trace context into log records, which OpenTelemetry SDKs handle automatically when both signals are instrumented.\nTrace-Metric Correlation Link traces and metrics through exemplars:\nHistogram buckets contain sample trace IDs Click from high-latency metric to example slow trace Correlate error rate spike with specific failing traces Validate fixes by monitoring metrics and inspecting traces Further Reading Official Documentation OpenTelemetry Traces Specification Context Propagation Sampling Receiver Components Processor Components Exporter Components Sampling Resources Tail Sampling Processor OpenTelemetry Sampling Tail Sampling with OpenTelemetry and New Relic Sampling at scale with OpenTelemetry OpenTelemetry Sampling update Integration Guides Getting Started with the Jaeger and Zipkin Receivers Using OpenTelemetry to send traces to Jaeger V2 OpenTelemetry Collector Exporters Context Propagation Deep Dives An overview of Context Propagation in OpenTelemetry OpenTelemetry Context Propagation Explained Related Analysis Documents OpenTelemetry Collector Overview - Core architecture and concepts Logs Support - How the Collector handles log data Metrics Support - How the Collector handles metrics Self-Monitoring - Observing the Collector itself ","categories":"","description":"Deep dive into how the OpenTelemetry Collector handles distributed tracing data, including the span model, receivers, sampling strategies, and exporters.\n","excerpt":"Deep dive into how the OpenTelemetry Collector handles distributed …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/otel-collector/otel-collector-traces/","tags":"","title":"OpenTelemetry Collector: Traces Support"},{"body":"","categories":"","description":"","excerpt":"","ref":"/battlebots/pr-preview/pr-153/tags/","tags":"","title":"Tags"},{"body":"Overview This document provides comprehensive guidance on integrating Grafana Tempo with the OpenTelemetry Collector, addressing two critical questions:\nDoes Tempo support OTLP? → YES - Native OTLP ingestion since version 1.3.0 (January 2022) Can the OTel Collector export to Tempo? → YES - Full integration via otlp (gRPC) or otlphttp (HTTP) exporters The integration enables a vendor-neutral observability pipeline where the OpenTelemetry Collector collects traces from instrumented applications and forwards them to Tempo for cost-effective, long-term trace storage, querying with TraceQL, and correlation with metrics and logs.\nWhy OTLP Matters for Tracing OpenTelemetry Protocol (OTLP) is the native protocol of the OpenTelemetry project, designed as a vendor-neutral standard for telemetry data transmission. Using OTLP with Tempo provides:\nNative Protocol: OTLP is Tempo’s primary ingestion protocol—no translation overhead Future-Proof: OTLP is the CNCF industry standard for distributed tracing Simplified Pipeline: No protocol conversion required (App → OTel SDK → OTel Collector → OTLP → Tempo) Full Fidelity: All span attributes, events, links, and context preserved Unified Stack: Same protocol for logs (Loki), metrics (Mimir), and traces (Tempo) Vendor Independence: Easy migration between OTLP-compatible backends (Jaeger V2, Tempo, cloud vendors) Tempo’s Position in the OTLP Ecosystem Tempo acts as an OTLP-native distributed tracing backend, receiving traces via:\nPrimary Path (Recommended): OpenTelemetry Collector → OTLP/gRPC (port 4317) → Tempo distributor Alternative Path: OpenTelemetry Collector → OTLP/HTTP (port 4318) → Tempo distributor Legacy Paths (Also Supported): Jaeger protocol, Zipkin protocol, OpenCensus Recommendation: Use OTLP/gRPC (port 4317) for best performance and lowest latency. Use OTLP/HTTP (port 4318) when gRPC is not available or firewall-restricted.\nOTLP Support in Tempo Native OTLP Support: YES Status: Grafana Tempo has full native OTLP ingestion support for both gRPC and HTTP protocols.\nSupported Protocols:\n✅ OTLP over gRPC (port 4317): High-performance binary protocol, recommended for production ✅ OTLP over HTTP (port 4318): RESTful endpoint at /v1/traces, firewall-friendly Version History:\nv1.3.0 (January 2022): Updated OpenTelemetry libraries to v0.40.0; changed OTLP gRPC default port from legacy 55680 to standard 4317 v2.7 (2024): Updated OpenTelemetry dependencies to v0.116.0; changed receiver binding from 0.0.0.0 to localhost by default (security improvement) v2.8 (2024): Upgraded OTLP to v1.3.0; removed deprecated InstrumentationLibrary from receivers v2.9 (2024): Migrated internal testing from deprecated Jaeger agent/exporter to standard OTLP exporter Current (2025): Full production-ready OTLP support with gRPC and HTTP protocols Minimum Recommended Version: v1.3.0 or later (for standard OTLP port 4317)\nProduction Recommendation: v2.8 or v2.9+ (latest stable releases with OTLP 1.3.0)\nOTLP Endpoint Configuration Default Endpoints and Ports Tempo’s distributor component exposes OTLP receivers on these default ports:\nOTLP/gRPC: Default Port: 4317/TCP Default Bind: localhost:4317 (Tempo v2.7+) Recommended Bind: 0.0.0.0:4317 (for containerized/networked deployments) OTLP/HTTP: Default Port: 4318/TCP HTTP Path: /v1/traces Default Bind: localhost:4318 (Tempo v2.7+) Recommended Bind: 0.0.0.0:4318 (for containerized/networked deployments) Important Change in Tempo v2.7+: If an endpoint is not explicitly specified, receivers default to binding on localhost only (instead of 0.0.0.0). For containerized environments (Docker, Kubernetes), you must explicitly configure 0.0.0.0 to listen on all interfaces and accept external connections.\nBasic Tempo Configuration Minimal OTLP receiver configuration:\ndistributor: receivers: otlp: protocols: grpc: # Defaults to localhost:4317 (Tempo v2.7+) http: # Defaults to localhost:4318 (Tempo v2.7+) Production configuration (all interfaces):\ndistributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 # Listen on all interfaces http: endpoint: 0.0.0.0:4318 # Listen on all interfaces TLS/mTLS Configuration Enable TLS on OTLP receivers:\ndistributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 tls: cert_file: /certs/server.crt key_file: /certs/server.key http: endpoint: 0.0.0.0:4318 tls: cert_file: /certs/server.crt key_file: /certs/server.key Enable mutual TLS (mTLS):\ndistributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 tls: cert_file: /certs/server.crt key_file: /certs/server.key client_ca_file: /certs/ca.crt require_client_auth: true Multi-Tenancy with OTLP Tempo supports multi-tenancy using the X-Scope-OrgID header. When multi-tenancy is enabled, every OTLP request must include this header to identify the tenant.\nEnable multi-tenancy in Tempo:\nserver: http_listen_port: 3200 distributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 # Enable multi-tenancy multitenancy_enabled: true OpenTelemetry Collector configuration with tenant header:\nexporters: otlp: endpoint: tempo:4317 headers: X-Scope-OrgID: \"tenant-123\" # Tenant identifier tls: insecure: true Multi-tenancy benefits:\nIsolated trace data per tenant Per-tenant retention policies Per-tenant rate limits Per-tenant query isolation OpenTelemetry Collector Export Configuration The OpenTelemetry Collector provides two exporters for sending traces to Tempo:\notlp exporter: Sends traces via OTLP over gRPC (recommended) otlphttp exporter: Sends traces via OTLP over HTTP OTLP Exporter (gRPC) - Recommended The otlp exporter uses gRPC for high-performance trace transmission.\nBasic configuration:\nexporters: otlp: endpoint: tempo:4317 tls: insecure: true # Use TLS in production Production configuration with retry and queue:\nexporters: otlp: endpoint: tempo:4317 tls: insecure: false cert_file: /certs/client.crt key_file: /certs/client.key ca_file: /certs/ca.crt # Retry configuration retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 10m # Sending queue (enables disk buffering) sending_queue: enabled: true queue_size: 1000 # Compression (enabled by default in Tempo 2.7.1+) compression: snappy # Timeout timeout: 30s Key parameters:\nendpoint: Tempo distributor address and port (format: host:port, no http:// prefix) tls.insecure: Set to false for production (enable TLS) retry_on_failure: Handles transient network failures sending_queue: Buffers traces during Tempo downtime (prevents data loss) compression: snappy (recommended), gzip, or none timeout: Max time to wait for Tempo to acknowledge OTLPHTTP Exporter (HTTP) - Alternative The otlphttp exporter uses HTTP/1.1 for trace transmission.\nBasic configuration:\nexporters: otlphttp: endpoint: http://tempo:4318 tls: insecure: true Production configuration:\nexporters: otlphttp: endpoint: https://tempo:4318 tls: insecure: false cert_file: /certs/client.crt key_file: /certs/client.key ca_file: /certs/ca.crt # Headers (multi-tenancy) headers: X-Scope-OrgID: \"tenant-123\" # Retry configuration retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 10m # Sending queue sending_queue: enabled: true queue_size: 1000 # Compression compression: gzip # Timeout timeout: 30s Key differences from gRPC:\nendpoint: Includes http:// or https:// prefix headers: Custom HTTP headers (useful for multi-tenancy) compression: Typically use gzip for HTTP When to use OTLPHTTP:\nFirewall restrictions block gRPC/HTTP2 Need to inspect traffic with HTTP debugging tools Existing infrastructure is HTTP/1.1-only When to use OTLP (gRPC):\nPerformance is critical (gRPC is faster) Low latency requirements High trace volume (gRPC handles backpressure better) Batch Processor Configuration The batch processor is critical for performance—it batches multiple spans before export, reducing network overhead and improving throughput.\nRecommended configuration:\nprocessors: batch: send_batch_size: 1000 # Send when batch reaches 1000 spans send_batch_max_size: 1500 # Max batch size (hard limit) timeout: 10s # Send every 10s regardless of size Parameter guidance:\nsend_batch_size: Grafana Labs internally uses 1,000 spans (recommended baseline) send_batch_max_size: Safety limit to prevent excessive memory usage timeout: Balance latency vs. efficiency (10s typical, 5s for low-latency needs) Effect of batching:\nWithout batching: 10,000 spans = 10,000 network requests With batching (1000/batch): 10,000 spans = 10 network requests (100x reduction) CPU/Memory savings: Larger batches = lower overhead, but higher latency Memory Limiter Processor Prevents the Collector from consuming excessive memory during traffic spikes.\nConfiguration:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 # Hard memory limit (512MB) spike_limit_mib: 128 # Allow 128MB spikes above limit How it works:\nCollector monitors memory usage every check_interval If memory exceeds limit_mib, stop accepting new data Allow temporary spikes up to limit_mib + spike_limit_mib Resume accepting data when memory drops below limit Recommendation: Always use memory_limiter as the first processor in production.\nComplete Pipeline Configuration Full OpenTelemetry Collector configuration for Tempo:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: # Memory limiter (first processor, prevents OOM) memory_limiter: check_interval: 1s limit_mib: 512 spike_limit_mib: 128 # Batch processor (critical for performance) batch: send_batch_size: 1000 send_batch_max_size: 1500 timeout: 10s # Resource detection (adds cloud/K8s metadata) resourcedetection: detectors: [env, system, docker, kubernetes] timeout: 5s # Attributes processor (add/modify span attributes) attributes: actions: - key: environment value: production action: upsert exporters: # OTLP exporter to Tempo (gRPC) otlp: endpoint: tempo:4317 tls: insecure: true retry_on_failure: enabled: true initial_interval: 5s max_interval: 30s max_elapsed_time: 10m sending_queue: enabled: true queue_size: 1000 compression: snappy # Logging exporter (debugging) logging: loglevel: info service: pipelines: traces: receivers: [otlp] processors: [memory_limiter, batch, resourcedetection, attributes] exporters: [otlp, logging] Pipeline flow:\nOTLP Receiver (4317/4318) ↓ Memory Limiter (prevent OOM) ↓ Batch Processor (group spans) ↓ Resource Detection (add metadata) ↓ Attributes Processor (modify spans) ↓ OTLP Exporter → Tempo (4317) Resource Attribute Mapping OpenTelemetry traces include resource attributes (metadata about the entity producing spans, such as service name, host, container ID) and span attributes (operation-specific metadata).\nHow Tempo Handles Attributes Resource Attributes:\nStored with every span in the trace Queryable via TraceQL using resource.* selector Examples: resource.service.name, resource.host.name, resource.k8s.pod.name Span Attributes:\nStored with individual spans Queryable via TraceQL using span.* selector Examples: span.http.method, span.db.statement, span.action.type Key Resource Attributes for BattleBots Service identification:\nresource.service.name = \"game-server\" resource.service.version = \"1.2.3\" resource.service.namespace = \"production\" Infrastructure:\nresource.host.name = \"game-server-01\" resource.k8s.pod.name = \"game-server-abc123\" resource.k8s.namespace.name = \"battlebots\" resource.container.id = \"docker://abc123\" Environment:\nresource.deployment.environment = \"production\" resource.cloud.region = \"us-east-1\" TraceQL Queries with Attributes Find traces by service name:\n{ resource.service.name = \"game-server\" } Find traces by span attribute:\n{ span.http.status_code \u003e= 500 } Combine resource and span attributes:\n{ resource.service.name = \"game-server\" \u0026\u0026 span.action.type = \"bot_move\" \u0026\u0026 duration \u003e 100ms } Best Practices for Attributes Use low-cardinality resource attributes:\n✅ Service name, environment, region (bounded values) ❌ User IDs, trace IDs, timestamps (unbounded values) Use descriptive span attributes:\nInclude operation-specific context: http.method, db.statement, bot.id Add BattleBots-specific attributes: battle.id, action.type, player.id Avoid attribute explosion:\nToo many unique attributes can degrade Tempo performance Keep total unique attribute count \u003c 1000 per service Sampling Strategies in the OTel Collector Sampling controls which traces are sent to Tempo. The OpenTelemetry Collector supports multiple sampling strategies.\nHead-Based Sampling Decision point: Made at span creation time (before seeing complete trace).\nConfiguration:\nprocessors: probabilistic_sampler: sampling_percentage: 10 # Sample 10% of traces Use cases:\nBaseline traffic reduction Simple, predictable sampling rate Low overhead, low latency Limitations:\nCannot make decisions based on trace outcome (errors, latency) May miss rare but important traces Tail-Based Sampling (Recommended for Production) Decision point: Made after seeing all/most spans in a trace.\nConfiguration:\nprocessors: tail_sampling: policies: # Always sample errors - name: errors-policy type: status_code status_code: status_codes: [ERROR] # Always sample slow traces - name: slow-requests type: latency latency: threshold_ms: 1000 # Sample 10% of successful traces - name: sample-10-percent type: probabilistic probabilistic: sampling_percentage: 10 # Sample specific span attributes - name: critical-actions type: string_attribute string_attribute: key: span.action.type values: [bot_death, battle_end] Architecture requirement: All spans for a given trace ID must be routed to the same Collector instance.\nSolution: Use a two-tier architecture:\nApplication Instances ↓ Agent Collectors (no tail sampling) ↓ Load Balancing Exporter (shard by trace ID) ↓ Gateway Collectors (tail sampling) ↓ Tempo Benefits:\nCapture 100% of errors Capture 100% of slow requests Sample normal traffic to reduce volume Sampling Recommendations for BattleBots Development: 100% sampling (no sampling)\nProduction:\nprocessors: tail_sampling: policies: # Always sample errors - name: errors type: status_code status_code: status_codes: [ERROR] # Always sample critical game events - name: critical-events type: string_attribute string_attribute: key: span.event.type values: [bot_death, battle_end, matchmaking_failed] # Always sample slow battles (\u003e 10 seconds) - name: slow-battles type: latency latency: threshold_ms: 10000 # Sample 20% of normal bot actions - name: baseline type: probabilistic probabilistic: sampling_percentage: 20 Complete Working Examples Example 1: Basic OpenTelemetry Collector → Tempo OpenTelemetry Collector config (otel-collector.yaml):\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: batch: send_batch_size: 1000 timeout: 10s exporters: otlp: endpoint: tempo:4317 tls: insecure: true service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp] Tempo config (tempo.yaml):\nserver: http_listen_port: 3200 distributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 storage: trace: backend: local local: path: /var/tempo Docker Compose:\nversion: '3' services: otel-collector: image: otel/opentelemetry-collector:latest command: [\"--config=/etc/otel-collector.yaml\"] volumes: - ./otel-collector.yaml:/etc/otel-collector.yaml ports: - \"4317:4317\" - \"4318:4318\" tempo: image: grafana/tempo:latest command: [\"-config.file=/etc/tempo.yaml\"] volumes: - ./tempo.yaml:/etc/tempo.yaml - tempo-data:/var/tempo ports: - \"3200:3200\" volumes: tempo-data: Example 2: Production with Multi-Tenancy and TLS OpenTelemetry Collector config:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 processors: memory_limiter: check_interval: 1s limit_mib: 512 batch: send_batch_size: 1000 timeout: 10s resourcedetection: detectors: [env, system, kubernetes] exporters: otlp: endpoint: tempo:4317 headers: X-Scope-OrgID: \"tenant-battlebots\" tls: insecure: false cert_file: /certs/client.crt key_file: /certs/client.key ca_file: /certs/ca.crt retry_on_failure: enabled: true sending_queue: enabled: true queue_size: 1000 compression: snappy service: pipelines: traces: receivers: [otlp] processors: [memory_limiter, batch, resourcedetection] exporters: [otlp] Tempo config:\nserver: http_listen_port: 3200 distributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 tls: cert_file: /certs/server.crt key_file: /certs/server.key ingester: lifecycler: ring: replication_factor: 3 storage: trace: backend: s3 s3: bucket: tempo-traces endpoint: s3.amazonaws.com region: us-east-1 multitenancy_enabled: true overrides: per_tenant_override_config: /etc/overrides.yaml Troubleshooting Issue: Connection Refused on Port 4317/4318 Symptoms:\nError: rpc error: code = Unavailable desc = connection error: dial tcp 192.168.1.10:4317: connect: connection refused Causes:\nTempo’s OTLP receivers not configured properly Tempo v2.7+ defaulting to localhost (not accessible from other containers) Firewall blocking ports 4317/4318 Solutions:\nVerify Tempo OTLP configuration:\ndistributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 # Must be 0.0.0.0, not localhost Check Tempo logs:\ndocker logs tempo | grep \"OTLP\" Verify ports are exposed:\ndocker ps | grep tempo # Should show 4317/tcp and 4318/tcp Test connectivity:\n# From OTel Collector container nc -zv tempo 4317 Issue: Traces Not Appearing in Tempo Symptoms:\nOTel Collector shows no errors Traces not visible in Grafana Debugging steps:\nEnable debug logging in OTel Collector:\nexporters: logging: loglevel: debug service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp, logging] # Add logging Check OTel Collector metrics:\ncurl http://otel-collector:8888/metrics | grep otelcol_exporter_sent_spans # Should show \u003e 0 if spans are being sent Check Tempo metrics:\ncurl http://tempo:3200/metrics | grep tempo_distributor_spans_received_total # Should show \u003e 0 if Tempo is receiving spans Wait for ingester flush:\nTraces are buffered in ingesters before flushing to storage Default max_block_duration = 30-60 minutes For POC, set max_block_duration: 5m for faster availability Query by trace ID directly:\ncurl \"http://tempo:3200/api/traces/\u003ctrace-id\u003e\" Issue: High Memory Usage in OTel Collector Symptoms:\nOTel Collector OOM (out of memory) Collector restarts frequently Solutions:\nAdd memory limiter processor:\nprocessors: memory_limiter: check_interval: 1s limit_mib: 512 service: pipelines: traces: processors: [memory_limiter, batch] # memory_limiter first Reduce batch size:\nprocessors: batch: send_batch_size: 500 # Reduce from 1000 timeout: 5s # Reduce from 10s Increase OTel Collector resources:\n# Docker Compose otel-collector: deploy: resources: limits: memory: 1G Issue: Spans Dropped or Missing Symptoms:\nIncomplete traces (missing spans) OTel Collector shows dropped spans Causes:\nRate limiting in Tempo Batch processor queue full Network timeouts Solutions:\nCheck OTel Collector queue metrics:\ncurl http://otel-collector:8888/metrics | grep otelcol_exporter_queue_size Increase queue size:\nexporters: otlp: sending_queue: enabled: true queue_size: 5000 # Increase from 1000 Check Tempo rate limits:\n# tempo.yaml overrides: defaults: ingestion_rate_limit_bytes: 10485760 # 10 MB/s ingestion_burst_size_bytes: 20971520 # 20 MB burst Enable retry on failure:\nexporters: otlp: retry_on_failure: enabled: true max_elapsed_time: 10m Issue: “Unimplemented” or Wrong Endpoint Errors Symptoms:\nError: rpc error: code = Unimplemented desc = unknown service Cause: OpenTelemetry Collector trying to send to wrong endpoint or using wrong protocol.\nSolutions:\nVerify endpoint format:\n✅ Correct: endpoint: tempo:4317 (no protocol prefix for gRPC) ❌ Wrong: endpoint: http://tempo:4317 (http prefix for gRPC) Match exporter to protocol:\nUse otlp exporter for gRPC (port 4317) Use otlphttp exporter for HTTP (port 4318 with http:// prefix) Verify Tempo receiver is enabled:\ndistributor: receivers: otlp: protocols: grpc: # Must be enabled http: # Must be enabled BattleBots Integration Patterns Complete LGTM Stack Configuration OpenTelemetry Collector configuration for BattleBots:\nreceivers: # OTLP receiver for application traces otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 processors: memory_limiter: check_interval: 1s limit_mib: 512 batch: send_batch_size: 1000 timeout: 10s # Add BattleBots-specific attributes attributes: actions: - key: platform value: battlebots action: upsert - key: environment value: production action: upsert # Detect cloud/K8s metadata resourcedetection: detectors: [env, system, docker, kubernetes] # Tail sampling for intelligent trace selection tail_sampling: policies: # Always sample errors - name: errors type: status_code status_code: status_codes: [ERROR] # Always sample critical game events - name: critical-events type: string_attribute string_attribute: key: span.event.type values: [bot_death, battle_end, matchmaking_failed] # Always sample slow operations - name: slow-operations type: latency latency: threshold_ms: 1000 # Sample 20% of normal traffic - name: baseline type: probabilistic probabilistic: sampling_percentage: 20 exporters: # Tempo - traces otlp: endpoint: tempo:4317 tls: insecure: true retry_on_failure: enabled: true sending_queue: enabled: true compression: snappy # Logging for debugging logging: loglevel: info service: pipelines: traces: receivers: [otlp] processors: [memory_limiter, batch, attributes, resourcedetection, tail_sampling] exporters: [otlp, logging] BattleBots-Specific TraceQL Queries Find all battles with errors:\n{ resource.service.name = \"game-server\" \u0026\u0026 status = error } Find slow bot actions (\u003e 100ms):\n{ span.action.type =~ \"bot_.*\" \u0026\u0026 duration \u003e 100ms } Find battles where a specific bot died:\n{ span.event.type = \"bot_death\" \u0026\u0026 span.bot.id = \"bot_xyz123\" } Find database queries in battle processing:\n{ resource.service.name = \"game-server\" \u0026\u0026 span.db.statement != nil \u0026\u0026 duration \u003e 50ms } Aggregate: Count battles by winner:\n{ span.battle.result != nil } | by(span.battle.winner) | count() Aggregate: Average battle duration by game mode:\n{ resource.service.name = \"game-server\" \u0026\u0026 span.name = \"ExecuteBattle\" } | by(span.battle.mode) | avg(duration) Trace-Metric-Log Correlation Example Application instrumentation (Go):\nimport ( \"context\" \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/trace\" \"github.com/sirupsen/logrus\" ) func ProcessBotAction(ctx context.Context, action BotAction) error { // Create span tracer := otel.Tracer(\"game-server\") ctx, span := tracer.Start(ctx, \"ProcessBotAction\") defer span.End() // Add span attributes span.SetAttributes( attribute.String(\"action.type\", action.Type), attribute.String(\"bot.id\", action.BotID), attribute.String(\"battle.id\", action.BattleID), ) // Log with trace context traceID := span.SpanContext().TraceID().String() spanID := span.SpanContext().SpanID().String() logger.WithFields(logrus.Fields{ \"trace_id\": traceID, \"span_id\": spanID, \"bot_id\": action.BotID, }).Info(\"Processing bot action\") // ... process action ... if err != nil { span.RecordError(err) span.SetStatus(codes.Error, \"Action failed\") logger.WithField(\"trace_id\", traceID).Error(\"Action failed\", err) return err } return nil } Workflow in Grafana:\nStart with metric alert: “Bot action latency P99 \u003e 500ms” Click exemplar: Jump to example slow trace Identify span: See ProcessBotAction span took 800ms View logs: Click “Logs for this span” → see detailed error logs with same trace_id Root cause: Logs show “database connection pool exhausted at 12:34:56” Further Reading Official Documentation Configure Tempo Pushing Spans with HTTP Enable Multi-tenancy Tempo Release Notes OpenTelemetry Collector Resources OpenTelemetry Collector Configuration OTLP Exporter OTLPHTTP Exporter Batch Processor Tail Sampling Processor Integration Guides How to Send Traces to Grafana Cloud Tempo with OpenTelemetry Collector End-to-End Distributed Tracing in Kubernetes with Grafana Tempo and OpenTelemetry Send Data to the Grafana Cloud OTLP Endpoint Troubleshooting Resources Tempo Troubleshooting Guide OpenTelemetry Collector Troubleshooting Grafana Community Forums - Tempo Related BattleBots Documentation Tempo Overview - Architecture, deployment, and how to run Tempo OpenTelemetry Collector: Traces Support - Understanding distributed tracing Grafana Loki: OTLP Integration - Log correlation Grafana Mimir: OTLP Integration - Metrics correlation ","categories":"","description":"Deep dive into Grafana Tempo's native OTLP support and integration with the OpenTelemetry Collector, including configuration examples, best practices, and troubleshooting guidance.\n","excerpt":"Deep dive into Grafana Tempo's native OTLP support and integration …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/traces/tempo/tempo-otlp-integration/","tags":"","title":"Tempo: OTLP and OpenTelemetry Collector Integration"},{"body":"Overview Grafana Tempo is a high-volume, minimal dependency distributed tracing backend designed for cost-efficiency and operational simplicity. Unlike traditional distributed tracing systems that require complex database infrastructure like Cassandra or Elasticsearch, Tempo leverages object storage as its only dependency, dramatically reducing operational complexity while providing powerful trace querying capabilities through TraceQL.\nThe core innovation of Tempo is its index-free, object storage-first architecture: traces are stored as immutable blocks in cost-effective object storage (S3, GCS, Azure, MinIO) without requiring expensive indexing infrastructure. While this trades off some search flexibility compared to fully-indexed systems like Jaeger, it enables storing 100% of traces at a fraction of the cost. TraceQL, Tempo’s SQL-like query language, provides sophisticated trace analysis capabilities including filtering by span attributes, duration thresholds, and span relationships.\nTempo integrates seamlessly with the Grafana observability stack, enabling powerful trace-to-metrics correlation through exemplars and trace-to-logs correlation through shared trace IDs. Combined with Loki (logs) and Mimir (metrics), Tempo completes the LGTM stack for unified observability.\nWhat is Grafana Tempo? Grafana Tempo is an open-source distributed tracing backend that fundamentally changes the economics and operational model of trace storage at scale. Launched by Grafana Labs, Tempo addresses the primary pain points of traditional tracing systems: expensive storage infrastructure, complex database management, and prohibitive costs that force aggressive sampling.\nDesign Philosophy:\nCost-Efficient: Object storage costs 90% less than Cassandra or Elasticsearch for trace storage Minimal Dependencies: Only requires object storage—no databases, no indexes, no complex infrastructure Standards-Based: Native OpenTelemetry Protocol (OTLP) support with full compatibility for Jaeger and Zipkin formats High Scale with Simplicity: Handle millions of spans per second while maintaining operational simplicity Deep Integration: Seamless correlation with Grafana, Loki, Mimir, and Prometheus Key Characteristics:\nIndex-Free Architecture: Primary lookup method is by trace ID; advanced queries use TraceQL with on-demand scanning Blocks Storage Model: Traces stored in columnar Apache Parquet format optimized for fast scanning (300 GB/s search speed) Query Language: TraceQL provides SQL-like syntax for powerful trace analysis Multi-Protocol Ingestion: Accepts OTLP, Jaeger, Zipkin, and OpenCensus protocols Multi-Tenancy: Native support for isolated tenant data with per-tenant limits and retention Problem It Solves:\nTraditional tracing backends (Jaeger, Zipkin) require expensive, complex database infrastructure that:\nIncurs high storage costs (forcing aggressive sampling to 1-10% of traces) Requires skilled operators to manage Cassandra or Elasticsearch clusters Creates operational overhead for database tuning, backups, and scaling Limits retention to days or weeks due to cost constraints Tempo eliminates these issues by leveraging cheap object storage and an index-free design, enabling teams to store 100% of traces for weeks or months at a fraction of the cost, with minimal operational burden.\nWhy Research Tempo? For the BattleBots platform’s observability stack, Tempo offers several strategic advantages:\nCost Efficiency at Scale 10x Storage Cost Reduction: Object storage costs $0.023/GB/month (S3) vs. $0.10-0.30/GB/month for databases Store 100% of Traces: No sampling required—capture every battle, every bot action, every error Long-Term Retention: Affordable retention for 30+ days enables historical analysis and debugging Operational Simplicity No Database Management: No Cassandra tuning, no Elasticsearch cluster management Minimal Infrastructure: Deploy Tempo components + object storage (which you likely already have) Simple Scaling: Add more distributors/ingesters/queriers independently as needed Native OpenTelemetry Integration OTLP First-Class Protocol: Native OTLP/gRPC (port 4317) and OTLP/HTTP (port 4318) support since v1.3.0 Zero Translation Overhead: Direct ingestion without protocol conversion Future-Proof: Built on CNCF standard used by entire industry Powerful Query Capabilities TraceQL: SQL-like query language for filtering by span attributes, duration, status, and relationships Structural Queries: Find traces with specific span patterns (e.g., “database call \u003e 100ms following API call”) Aggregations: Generate metrics from traces (span counts, duration distributions) Unified Observability Trace-to-Metrics: Exemplars link metric spikes to example slow traces Trace-to-Logs: Shared trace ID enables jumping from trace span to correlated log lines Grafana Ecosystem: Native integration with Loki, Mimir, Prometheus, and Grafana Battle-Specific Benefits Debug Complete Battles: Trace entire battle workflow from matchmaking → execution → results persistence Bot Performance Analysis: Analyze every bot action’s latency and success/failure patterns Error Investigation: Capture 100% of failed actions without worrying about sampling missing critical traces Historical Analysis: Compare bot performance across battles over weeks or months Key Concepts Understanding Tempo’s data model and terminology is essential for effective deployment and usage.\nTraces and Spans A trace represents the full journey of one request or transaction across distributed services, while a span is a timed unit of work within that journey.\nTrace structure:\nTraces consist of one or more spans organized in a tree/hierarchy Each span represents an operation with a start time and duration Spans have parent-child relationships forming the call graph The root span represents the initial request entry point Spans can have events (annotations at specific timestamps) and links (references to spans in other traces) Example BattleBots trace:\nTrace: Battle Execution (trace_id: abc123) ├─ Root Span: ProcessBattleAction (100ms) ├─ Child Span: ValidateAction (10ms) ├─ Child Span: UpdateGameState (30ms) │ └─ Child Span: PersistState (database) (20ms) └─ Child Span: BroadcastUpdate (WebSocket) (15ms) Span characteristics:\nName: Describes the operation (e.g., “GET /api/battles”, “ProcessBotAction”, “database.query”) Start time and duration: Nanosecond precision timing information Status: Success (ok), error, or unset Span kind: Client, server, internal, producer, or consumer Attributes: Key-value metadata (e.g., http.method=GET, bot.id=bot123, action.type=attack) Events: Timestamped annotations (e.g., exceptions, state changes, cache hits) Links: References to causally-related spans in different traces Trace ID and Span ID Trace ID\nUnique identifier for the entire trace (128-bit or 64-bit hex string) Follows the trace through all services and operations Used as the primary key for trace retrieval in Tempo Example: 4bf92f3577b34da6a3ce929d0e0e4736 Span ID\nUnique identifier for an individual span within a trace (64-bit hex string) Used to reconstruct parent-child relationships Combined with trace ID for precise span references Example: 00f067aa0ba902b7 Parent Span ID\nReferences the span ID of the parent span Enables reconstruction of the trace tree structure Root spans have no parent span ID Blocks Storage Blocks are immutable units of trace data stored in object storage, representing Tempo’s fundamental persistence mechanism.\nBlock characteristics:\nFormat: Apache Parquet columnar storage (vParquet4 as of Tempo 2.5+) Immutability: Once written, blocks are never modified (only compacted/deleted) Time-based: Blocks typically represent traces from a specific time window Columnar: Span attributes stored in columns for efficient filtering Compressed: Parquet provides excellent compression ratios Block lifecycle:\nCreation: Ingesters accumulate spans in memory over configured time period (default 30-60 minutes) Flushing: Ingester writes complete block to object storage with bloom filters and indexes Querying: Queriers read blocks, using bloom filters to skip irrelevant blocks Compaction: Compactor merges small blocks into larger ones for query efficiency Expiration: Compactor deletes blocks older than retention period Performance:\nPrevious block format: ~40-50 GB/s search speed Parquet format (vParquet4): ~300 GB/s search speed (6x improvement) Bloom filters enable skipping 99%+ of irrelevant blocks TraceQL Query Language TraceQL is Tempo’s powerful SQL-like query language for selecting, filtering, and analyzing distributed traces.\nDesign characteristics:\nStructure-aware: Understands trace architecture (root spans, parent-child relationships) Attribute-focused: Query by span attributes, resource attributes, and intrinsics Familiar syntax: Borrows concepts from PromQL and LogQL for consistency Aggregation support: Generate metrics from traces (counts, averages, histograms) Query structure:\n{ \u003cspan-selector\u003e } | \u003cpipeline-operations\u003e Attribute scopes:\nspan.*: Span-level attributes (e.g., span.http.method, span.db.statement) resource.*: Resource-level attributes (e.g., resource.service.name, resource.host.name) event.*: Event attributes link.*: Link attributes to related spans Intrinsics (built-in span properties):\nname: Span name duration: Span duration in nanoseconds status: Span status (ok, error, unset) kind: Span kind (client, server, internal, producer, consumer) Example TraceQL queries:\n# Find all traces with HTTP GET requests { span.http.method = \"GET\" } # Find slow database queries (\u003e 1 second) { span.db.statement =~ \"SELECT.*\" \u0026\u0026 duration \u003e 1s } # Find errors in specific service { resource.service.name = \"game-server\" \u0026\u0026 status = error } # Complex: API calls that resulted in slow database queries { span.http.route = \"/api/battles\" } \u003e\u003e { span.db.statement != nil \u0026\u0026 duration \u003e 100ms } # Aggregate: Count traces by service { } | by(resource.service.name) | count() \u003e 10 # BattleBots: Find all battles where bot actions took \u003e 500ms { resource.service.name = \"game-server\" \u0026\u0026 span.action.type = \"bot_move\" \u0026\u0026 duration \u003e 500ms } Query execution: Tempo evaluates TraceQL queries by:\nIdentifying relevant blocks using time range and bloom filters Scanning blocks in parallel using columnar Parquet format Applying span selectors and filters Executing pipeline operations (aggregations, grouping) Returning matching traces or aggregated results Sampling Strategies Sampling controls which traces are retained for analysis, balancing observability value with storage costs and performance impact.\nHead-Based Sampling\nDecision point: Made at trace creation time, before seeing complete trace Characteristics: Low latency, minimal overhead, consistent across services Use cases: Baseline traffic reduction, predictable sampling rates Limitation: Cannot make decisions based on trace outcome (errors, latency) Tail-Based Sampling\nDecision point: Made after seeing all/most spans in a trace Characteristics: Intelligent decisions based on complete trace data, higher overhead Policies: Always sample errors, sample slow traces (latency threshold), probabilistic, rate limiting Requirement: Multi-tier collector deployment (agent → tail sampling gateway) Use cases: Capture 100% of errors while sampling successful traces Adaptive Sampling\nTraceQL support: with(sample=true) automatically determines optimal strategy Fixed span sampling: with(span_sample=0.10) selects 10% of spans Fixed trace sampling: with(trace_sample=0.10) selects complete traces Recommendation for BattleBots:\nDevelopment: 100% sampling (no sampling) Production: Tail-based sampling with policies: Always sample traces with status = error Always sample traces with duration \u003e 2 seconds Sample 10-20% of successful, fast traces Always sample traces with span.action.type = \"bot_death\" (critical game events) Architecture Components Tempo uses a microservices architecture where each component can be scaled independently or combined into larger deployment targets. The architecture is inspired by Grafana Loki and optimized for object storage.\ngraph TB subgraph \"Instrumented Applications\" APP1[\"Game Server\u003cbr/\u003e(OTel Instrumented)\"] APP2[\"Matchmaking Service\u003cbr/\u003e(OTel Instrumented)\"] APP3[\"Bot Runtime\u003cbr/\u003e(OTel Instrumented)\"] end subgraph \"Ingestion Path\" COLL[\"OpenTelemetry Collector\u003cbr/\u003e(Protocol Conversion)\"] DIST[\"Distributor\u003cbr/\u003e(Load Balancing \u0026 Sharding)\"] end subgraph \"Write Path\" ING1[\"Ingester 1\"] ING2[\"Ingester 2\"] ING3[\"Ingester 3\"] BLOOM[\"Bloom Filters\u003cbr/\u003e\u0026 Indexes\"] end subgraph \"Object Storage\" S3[\"S3 / GCS / Azure / MinIO\"] BLOCKS[\"Parquet Blocks\u003cbr/\u003e(vParquet4)\"] end subgraph \"Maintenance\" COMPACT[\"Compactor\u003cbr/\u003e(Block Merging,\u003cbr/\u003eRetention)\"] METRICS[\"Metrics-Generator\u003cbr/\u003e(Optional)\"] end subgraph \"Read Path\" QFRONT[\"Query Frontend\u003cbr/\u003e(Query Optimization)\"] Q1[\"Querier 1\"] Q2[\"Querier 2\"] Q3[\"Querier 3\"] end subgraph \"Query Interface\" TRACEQL[\"TraceQL Engine\"] GRAFANA[\"Grafana UI\"] end APP1 --\u003e|\"Traces (OTLP)\"| COLL APP2 --\u003e|\"Traces (OTLP)\"| COLL APP3 --\u003e|\"Traces (OTLP)\"| COLL COLL --\u003e|\"OTLP/Jaeger/Zipkin\"| DIST DIST --\u003e|\"Hash(Trace ID)\"| ING1 DIST --\u003e|\"Hash(Trace ID)\"| ING2 DIST --\u003e|\"Hash(Trace ID)\"| ING3 ING1 \u0026 ING2 \u0026 ING3 --\u003e BLOOM BLOOM --\u003e BLOCKS BLOCKS --\u003e S3 COMPACT -.-\u003e|\"Merge \u0026 Optimize\"| BLOCKS ING1 \u0026 ING2 \u0026 ING3 -.-\u003e|\"Span Metrics\"| METRICS GRAFANA --\u003e|\"TraceQL / Trace ID\"| QFRONT QFRONT --\u003e Q1 \u0026 Q2 \u0026 Q3 Q1 \u0026 Q2 \u0026 Q3 --\u003e|\"Recent Data\"| ING1 \u0026 ING2 \u0026 ING3 Q1 \u0026 Q2 \u0026 Q3 --\u003e|\"Bloom + Block Read\"| BLOCKS Q1 \u0026 Q2 \u0026 Q3 --\u003e TRACEQL TRACEQL --\u003e GRAFANA METRICS -.-\u003e|\"Service Graphs\"| GRAFANA style DIST fill:#fff4e6 style ING1 fill:#fff4e6 style ING2 fill:#fff4e6 style ING3 fill:#fff4e6 style QFRONT fill:#e1f5ff style Q1 fill:#e1f5ff style Q2 fill:#e1f5ff style Q3 fill:#e1f5ff style COMPACT fill:#f3e5f5 style METRICS fill:#e8f5e9 style S3 fill:#fce4ec Distributor The distributor is the entry point for all incoming span data, receiving traces in multiple formats and routing them to ingesters.\nResponsibilities:\nAccept spans in multiple protocols: OTLP (gRPC/HTTP), Jaeger (gRPC/Thrift), Zipkin, OpenCensus Validate incoming span data for correctness and completeness Apply rate limiting per tenant (if multi-tenancy enabled) Hash spans by trace ID to determine target ingesters Route spans to ingesters using consistent hashing on trace ID Load balance across available ingesters Key characteristics:\nStateless component (can be horizontally scaled easily) Uses OpenTelemetry Collector’s receiver layer for protocol handling Leverages distributed consistent hash ring for sharding All spans for a given trace ID route to the same ingester set Configuration considerations:\nEnable only the receivers you need (OTLP recommended, disable unused protocols) Configure appropriate rate limits per tenant Set batch sizes to balance latency vs. throughput Enable gRPC compression (snappy) for bandwidth efficiency Ingester The ingester indexes and batches span data for efficient storage, creating the immutable blocks that form Tempo’s storage layer.\nResponsibilities:\nReceive spans from distributors (sharded by trace ID) Buffer spans in memory, organizing by trace ID Generate bloom filters and indexes for fast retrieval Batch spans into blocks based on time or size thresholds Write complete blocks to object storage (S3, GCS, Azure, MinIO) Serve recent, not-yet-persisted trace data to queriers (for real-time queries) Maintain Write-Ahead Log (WAL) for crash recovery Block creation process:\nAccumulate spans in memory for configured duration (default 30-60 min) Partition span data into Apache Parquet columnar schema Generate bloom filters for efficient trace ID lookups Compress data using Parquet compression Write immutable block to object storage Update internal metadata for recent blocks Key characteristics:\nStateful component (requires careful scaling and shutdown) Uses consistent hashing with replication (default RF=3 for durability) Memory usage scales with trace volume and max_block_duration WAL ensures data durability during crashes Configuration considerations:\nmax_block_duration: 30-60 minutes optimal (balance memory vs. block count) max_block_bytes: Default 100GB (prevents excessive block sizes) trace_idle_period: How long to wait before flushing a trace (default 10s) Replication factor: RF=3 recommended for production Querier The querier executes trace searches and retrievals, scanning object storage blocks and querying ingesters for recent data.\nResponsibilities:\nExecute trace ID lookups (primary query method) Execute TraceQL queries across blocks Read bloom filters and indexes from object storage Skip irrelevant blocks using bloom filter checks Scan relevant blocks in parallel using Parquet columnar format Query ingesters for recent, not-yet-persisted traces Merge results from multiple sources (ingesters + blocks) Return matching spans to Query Frontend Query execution:\nReceive query from Query Frontend Determine time range and relevant blocks Check bloom filters to skip blocks without matching trace IDs Read relevant blocks from object storage in parallel Query ingesters for recent data Apply TraceQL filters and aggregations Return results Key characteristics:\nStateless component (horizontally scalable) Query performance depends on block count, trace size, and time range Uses caching to improve repeated query performance Configuration considerations:\nScale based on query concurrency and complexity Configure appropriate CPU/memory for parallel block scanning Enable query result caching Query Frontend The query Frontend optimizes and coordinates incoming trace queries, sharding large queries across multiple queriers in parallel.\nResponsibilities:\nAct as the interface between Grafana and the tracing backend Split large time-range queries into smaller sub-queries (query sharding) Distribute sub-queries across multiple queriers in parallel Aggregate and concatenate results from queriers Manage query queuing to prevent querier overload Cache query results to improve performance Key characteristics:\nStateless component (horizontally scalable) Significantly improves query performance for large time ranges Required for high availability (minimum 2 replicas recommended) Configuration considerations:\nDeploy at least 2 replicas for HA Configure query sharding parameters based on trace volume Enable query result caching Compactor The compactor maintains and optimizes stored blocks, running on scheduled intervals to compress and deduplicate data.\nResponsibilities:\nMerge small blocks into larger blocks (reduces block count by ~90%) Deduplicate spans from replicated writes (RF=3 creates duplicates) Apply retention policies by deleting blocks older than configured retention Optimize bloom filters for merged blocks Update block metadata Compaction process:\nList all blocks in object storage for a given time window Download blocks to be compacted Merge and deduplicate spans Generate new, larger block with optimized bloom filters Upload merged block to object storage Delete source blocks Key characteristics:\nI/O-bound component (minimal CPU requirements) Only one compactor should run per tenant (to avoid conflicts) Critical for long-term storage efficiency and query performance Configuration considerations:\nblock_retention: 30 days default (configurable per tenant) compacted_block_retention: 0-1 hour (how long to keep source blocks after compaction) Schedule compaction during low-traffic periods if possible Metrics-Generator (Optional) The metrics-generator derives metrics from ingested traces, enabling span metrics and service graphs.\nResponsibilities:\nAnalyze incoming span data Extract span metrics (request rates, error rates, latencies by service) Generate service graph metrics (call relationships between services) Write derived metrics to metrics storage (Prometheus, Mimir) Enable trace-to-metrics correlation through exemplars Generated metrics:\nSpan metrics: traces_spanmetrics_calls_total, traces_spanmetrics_latency_bucket Service graph metrics: Request rate, error rate, latency between service pairs Key characteristics:\nOptional component (not required for core tracing functionality) Enables powerful trace-to-metrics workflows in Grafana Adds memory overhead to ingesters Configuration considerations:\nConfigure remote write endpoint (Prometheus, Mimir) Set histogram buckets for latency metrics Enable exemplar support for trace correlation Deployment Modes Tempo supports three deployment modes, each balancing simplicity against scalability and operational flexibility. The architecture is similar to Loki’s deployment model.\nMonolithic Mode In monolithic mode, all Tempo components run in a single process. This is the simplest deployment option, ideal for POC, development, and small-scale production.\nConfiguration:\ntempo -target=all -config.file=tempo.yaml Characteristics:\nSingle binary or container runs all components All components share memory and resources Minimal operational complexity Limited horizontal scalability (vertical scaling only) Suitable for development and small deployments When to use:\nDevelopment and testing environments Proof-of-concept deployments Small-scale production (\u003c 1,000 traces/second) Single-server or minimal infrastructure deployments Resource requirements:\n2-4 CPU cores 4-8GB RAM Local SSD for fast block storage (50GB+) Object storage access (S3, GCS, MinIO, or local filesystem) Limitations:\nCannot scale components independently Single point of failure Resource contention between components (ingesters and queriers compete for CPU/memory) Not recommended for Kubernetes production deployments Recommended for BattleBots POC: This mode is perfect for initial evaluation and development.\nScalable Single Binary Mode Scalable single binary mode (or “scalable mode”) runs all components within one process, but multiple instances can be deployed with read-write separation, providing horizontal scalability.\nConfiguration:\n# Set target for each instance tempo -target=scalable-single-binary -config.file=tempo.yaml Characteristics:\nComponents act as if in distributed setup but packaged as single binary Multiple instances deployed for redundancy and scale Read and write paths can be scaled independently Balanced approach between monolithic simplicity and microservices scalability Suitable for small-to-medium volume traces (10K-100K+ traces/second) When to use:\nMedium-scale production deployments Transitioning from monolithic to highly available setup Kubernetes environments using Helm charts Teams wanting operational simplicity with growth capacity Resource requirements:\n4 CPU cores, 8-12GB memory per instance 3-5 instances typical Shared object storage (S3, GCS, etc.) Typical throughput: 10K-50K traces/second Benefits:\nHorizontal scalability without full microservices complexity Reduced operational burden vs. microservices Suitable for Kubernetes deployments Components benefit from distributed hash ring Limitations:\nLess robust than fully distributed deployment Still some resource contention within single process Cannot optimize per-component scaling Recommended for BattleBots production: This mode is ideal for initial production deployment, providing scalability and HA without excessive complexity.\nMicroservices Mode Microservices mode runs each Tempo component as a separate deployment with independent replicas, providing maximum flexibility and scale.\nComponents deployed separately:\nDistributor (multiple instances, stateless) Ingester (multiple instances, stateful) Querier (multiple instances, stateless) Query Frontend (multiple instances, stateless) Compactor (single instance per tenant) Metrics-Generator (optional, multiple instances) Characteristics:\nEach component scaled independently based on workload Fine-grained resource allocation per component Granular failure domains (one component failure doesn’t affect others) Most complex to deploy and maintain Supports enterprise-scale deployments (multi-million traces/second) When to use:\nProduction environments with high-availability requirements Large-scale trace volumes (100K+ traces/second) Multi-tenant deployments Organizations with dedicated observability operations teams Need for independent component scaling (e.g., scale queriers independently during high query load) Resource requirements (large deployment example):\nDistributors: 4-8 instances (2 CPU, 2GB each) Ingesters: 6-12 instances (2.5 CPU, 8-12GB each) Queriers: 4-8 instances (2 CPU, 4-8GB each) Query Frontend: 2 instances (1 CPU, 4GB each) Compactor: 2-3 instances (1 CPU, 4GB each) Typical throughput: 500K-2M+ traces/second Benefits:\nFully horizontally scaled—each component has dedicated replicas Data replication prevents catastrophic failures (RF=3) Granular failure domains Flexible, independent scaling per component Can temporarily scale during traffic spikes with no adverse impact Challenges:\nSignificantly increased operational complexity More components to orchestrate, monitor, and maintain Requires sophisticated orchestration (Kubernetes with Helm) Network communication overhead between components Higher total cost of ownership (TCO) Not recommended for BattleBots initially: Start with scalable mode; migrate to microservices only if scale demands it.\nDeployment Mode Comparison Aspect Monolithic Scalable Single Binary Microservices Best Use Case POC, dev, demos Production (small-medium) Production (large scale) Deployment Complexity Very Simple Moderate Complex Horizontal Scaling None Instance-level Per-component (flexible) Failure Isolation None (SPOF) Basic (full process) Excellent (per-component) Data Replication Not available Limited Full (RF=3) Operational Overhead Minimal Moderate High Resource Efficiency High (no duplication) Moderate Lower (replication overhead) Configuration Complexity Minimal Moderate High Traces/Second Capacity \u003c 1K 10K-100K+ 100K-2M+ Recommended for Battle Bots POC/Dev Production Not initially How to Run Tempo This section provides practical guidance for running Grafana Tempo, focusing on Docker Compose for POC and development environments.\nQuick Start with Docker Compose The following Docker Compose setup includes Tempo, Grafana, Prometheus, and tools for generating and visualizing traces.\nStep 1: Create directory structure\nmkdir tempo-poc \u0026\u0026 cd tempo-poc mkdir -p tempo-data Step 2: Create tempo.yaml configuration\nCreate a file named tempo.yaml:\nstream_over_http_enabled: true server: http_listen_port: 3200 grpc_listen_port: 9095 log_level: info distributor: log_received_spans: enabled: true log_sampling_fraction: 0.1 receivers: jaeger: protocols: thrift_http: endpoint: \"0.0.0.0:14268\" grpc: endpoint: \"0.0.0.0:14250\" zipkin: endpoint: \"0.0.0.0:9411\" otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" ingester: lifecycler: ring: replication_factor: 1 kvstore: store: inmemory max_block_duration: 5m trace_idle_period: 5s compactor: compaction: block_retention: 720h # 30 days compacted_block_retention: 0h query_frontend: search: duration_slo: 5s throughput_bytes_slo: 1.073741824e+09 trace_by_id: duration_slo: 100ms metrics_generator: registry: external_labels: source: tempo cluster: docker-compose storage: path: /var/tempo/generator/wal remote_write: - url: http://prometheus:9090/api/v1/write send_exemplars: true processor: service_graphs: histogram_buckets: [0.1, 0.5, 1, 2, 5, 10] span_metrics: histogram_buckets: [0.1, 0.5, 1, 2, 5, 10] storage: trace: backend: local wal: path: /var/tempo/wal local: path: /var/tempo/blocks overrides: defaults: metrics_generator: processors: [service-graphs, span-metrics] Step 3: Create docker-compose.yaml\nCreate a file named docker-compose.yaml:\nversion: '3' services: # Initialize directory with proper permissions init: image: busybox:latest user: root entrypoint: sh -c 'mkdir -p /var/tempo/wal /var/tempo/blocks /var/tempo/generator/wal \u0026\u0026 chmod -R 777 /var/tempo' volumes: - tempo-data:/var/tempo # Tempo - distributed tracing backend tempo: image: grafana/tempo:latest command: [ \"-config.file=/etc/tempo.yaml\" ] volumes: - ./tempo.yaml:/etc/tempo.yaml - tempo-data:/var/tempo ports: # OTLP API - \"4317:4317\" # gRPC - \"4318:4318\" # HTTP # Jaeger API - \"14250:14250\" # gRPC - \"14268:14268\" # Thrift HTTP # Zipkin API - \"9411:9411\" # Tempo HTTP API - \"3200:3200\" # Tempo gRPC API - \"9095:9095\" depends_on: init: condition: service_completed_successfully # Prometheus - metrics storage for metrics-generator prometheus: image: prom/prometheus:latest command: - '--config.file=/etc/prometheus/prometheus.yaml' - '--storage.tsdb.path=/prometheus' - '--storage.tsdb.retention.time=24h' volumes: - ./prometheus.yaml:/etc/prometheus/prometheus.yaml - prometheus-data:/prometheus ports: - \"9090:9090\" depends_on: - tempo # Grafana - visualization and dashboards grafana: image: grafana/grafana:12.2.1 environment: GF_AUTH_ANONYMOUS_ENABLED: \"true\" GF_AUTH_ANONYMOUS_ORG_ROLE: \"Admin\" GF_SECURITY_ADMIN_PASSWORD: \"admin\" volumes: - grafana-data:/var/lib/grafana - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml ports: - \"3000:3000\" depends_on: - tempo - prometheus volumes: tempo-data: prometheus-data: grafana-data: Step 4: Create prometheus.yaml\nCreate a file named prometheus.yaml:\nglobal: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'tempo' static_configs: - targets: ['tempo:3200'] Step 5: Create grafana-datasources.yaml\nCreate a file named grafana-datasources.yaml:\napiVersion: 1 datasources: - name: Tempo type: tempo uid: tempo-uid access: proxy url: http://tempo:3200 basicAuth: false isDefault: true editable: true jsonData: httpMethod: GET tracesToMetrics: datasourceUid: prometheus-uid nodeGraph: enabled: true - name: Prometheus type: prometheus uid: prometheus-uid access: proxy url: http://prometheus:9090 basicAuth: false isDefault: false editable: true jsonData: timeInterval: 15s exemplarTraceIdDestinations: - name: trace_id datasourceUid: tempo-uid Step 6: Start the stack\ndocker compose up -d Step 7: Verify deployment\nCheck that all services are running:\ndocker compose ps Expected output:\nNAME COMMAND SERVICE STATUS tempo-poc-grafana-1 \"/run.sh\" grafana Up tempo-poc-init-1 \"sh -c 'mkdir -p /va…\" init Exited (0) tempo-poc-prometheus-1 \"/bin/prometheus --c…\" prometheus Up tempo-poc-tempo-1 \"/tempo -config.file…\" tempo Up Step 8: Verify Tempo is healthy\ncurl http://localhost:3200/ready Expected response: ready (HTTP 200)\nCheck Tempo status:\ncurl http://localhost:3200/status/services | jq Step 9: Access Grafana\nNavigate to http://localhost:3000 Login: admin / admin Go to Explore tab (left sidebar, compass icon) Select Tempo datasource from dropdown Click Search tab to find traces Step 10: Send test traces\nUsing curl with OTLP/HTTP:\ncurl -X POST http://localhost:4318/v1/traces \\ -H \"Content-Type: application/json\" \\ -d '{ \"resourceSpans\": [{ \"resource\": { \"attributes\": [{ \"key\": \"service.name\", \"value\": {\"stringValue\": \"test-service\"} }] }, \"scopeSpans\": [{ \"spans\": [{ \"traceId\": \"5b8aa5a2d2c872e8321cf37308d69df2\", \"spanId\": \"051581bf3cb55c13\", \"name\": \"test-operation\", \"kind\": 1, \"startTimeUnixNano\": \"1000000000000000000\", \"endTimeUnixNano\": \"1000000001000000000\" }] }] }] }' Verify the trace was received:\n# Wait a few seconds for ingestion sleep 5 # Query trace by ID curl \"http://localhost:3200/api/traces/5b8aa5a2d2c872e8321cf37308d69df2\" | jq Step 11: Query traces in Grafana\nIn Grafana Explore, select Tempo datasource Click Search tab Click Run query to see all traces Filter by service name: service.name = \"test-service\" Click on a trace to view the trace details Configuration Breakdown Key configuration sections explained:\nDistributor receivers:\nreceivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" # Listen on all interfaces (required for Docker) http: endpoint: \"0.0.0.0:4318\" 0.0.0.0: Required for Docker containers to accept external connections In Tempo v2.7+, defaults to localhost (which only works within same container) OTLP is the recommended protocol for new instrumentation Ingester configuration:\ningester: max_block_duration: 5m # Flush blocks every 5 minutes (faster for POC) trace_idle_period: 5s # Flush trace if no new spans for 5 seconds Shorter durations = faster trace availability, more blocks created Production: Use 30-60 minutes for max_block_duration Storage configuration:\nstorage: trace: backend: local # Use local filesystem (POC only) local: path: /var/tempo/blocks # Path inside container POC: Use local backend Production: Use s3, gcs, or azure backend Metrics generator:\nmetrics_generator: storage: remote_write: - url: http://prometheus:9090/api/v1/write send_exemplars: true # Enable trace-to-metrics correlation Generates service graph and span metrics Enables powerful trace-to-metrics workflows in Grafana Exposed Ports and Their Purposes Port Protocol Purpose 3200 HTTP Tempo HTTP API (queries, health checks) 9095 gRPC Tempo gRPC API 4317 gRPC OTLP gRPC receiver (recommended) 4318 HTTP OTLP HTTP receiver (recommended) 14250 gRPC Jaeger gRPC receiver 14268 HTTP Jaeger Thrift HTTP receiver 9411 HTTP Zipkin HTTP receiver 3000 HTTP Grafana web UI 9090 HTTP Prometheus web UI Recommendation: For new instrumentation, use OTLP on port 4317 (gRPC) or 4318 (HTTP).\nHealth Check Endpoints Tempo ready check:\ncurl http://localhost:3200/ready # Returns: ready (HTTP 200) Tempo health status:\ncurl http://localhost:3200/status/services | jq Tempo metrics (Prometheus format):\ncurl http://localhost:3200/metrics Grafana health:\ncurl http://localhost:3000/api/health # Returns: {\"status\":\"ok\"} Troubleshooting Common Issues Issue: “permission denied” on tempo-data directory\nSolution: The init service automatically fixes permissions with chmod -R 777. Ensure init completes successfully before Tempo starts.\nIssue: Tempo fails to start\nCheck logs:\ndocker compose logs tempo Verify config syntax:\ndocker compose exec tempo tempo --verify-config -config.file=/etc/tempo.yaml Issue: No traces appear in Grafana\nVerify Tempo is receiving spans:\ndocker compose logs tempo | grep \"received\" Check Tempo metrics for ingestion:\ncurl http://localhost:3200/metrics | grep tempo_distributor_spans_received_total Verify data source configuration in Grafana:\nGo to Configuration → Data Sources Click Tempo Click Save \u0026 Test Should see: “Data source is working” Wait a few seconds—traces need to be flushed from ingester\nIssue: Connection refused on port 4317/4318\nEnsure endpoint: \"0.0.0.0:4317\" is set (not localhost) in tempo.yaml. Tempo v2.7+ defaults to localhost, which doesn’t work in Docker.\nShutting Down Stop all services:\ndocker compose down Remove data volumes (WARNING: deletes all traces):\ndocker compose down -v rm -rf tempo-data/ Best Practices Sampling Strategies Development environments:\n100% sampling (no sampling) Capture all traces for comprehensive debugging Production environments:\nTail-based sampling (recommended):\nAlways sample traces with status = error Always sample traces with duration \u003e P99 latency threshold Sample 10-20% of successful, fast traces Use OpenTelemetry Collector tail sampling processor Head-based sampling (simpler alternative):\nSample 10-30% of all traces Trade-off: May miss rare errors BattleBots-specific:\nAlways sample traces with critical game events: span.action.type = \"bot_death\" span.event.type = \"battle_end\" status = error Sample 20% of normal bot actions Sample 100% of battles with unusual duration (\u003e 5 minutes or \u003c 10 seconds) Storage Optimization Block configuration:\nmax_block_duration: 30-60 minutes (balance memory vs. block count) max_block_bytes: Default 100GB (prevents excessive block sizes) trace_idle_period: 10 seconds default (flush traces after idle period) Retention policies:\nblock_retention: 30 days recommended (industry sweet spot) compacted_block_retention: 0-1 hour (delete source blocks after compaction) Object storage lifecycle:\nSet S3/GCS lifecycle policies to delete objects 1-2 days post-retention Prevents orphaned data accumulation Storage cost formula:\nStorage Cost = (Ingested Spans/Day × Average Span Size × Retention Days) × Storage Rate Example:\n1M spans/day × 1KB/span × 30 days × $0.023/GB/month = $0.69/month Compare to Jaeger with Cassandra: ~$50-100/month for same volume.\nConfiguration Best Practices Distributor:\nEnable only needed receivers (disable unused protocols) Use OTLP as primary protocol (best performance) Configure rate limiting for multi-tenant deployments Ingester:\nDeploy with RF=3 (replication factor 3) for production Enable WAL (Write-Ahead Log) for crash recovery Set lifecycle hooks for graceful shutdown (flush before termination) Querier:\nScale based on query concurrency and complexity Enable query result caching Monitor querier CPU/memory usage Compactor:\nSchedule during low-traffic periods if possible Monitor compaction lag (blocks pending compaction) Only one compactor per tenant (avoid conflicts) Performance Tuning Ingestion throughput:\nUse OTLP protocol (recommended, most efficient) Enable gRPC compression (snappy) for bandwidth efficiency Increase concurrent_flushes for parallelized block writing Reduce trace_idle_period for faster memory clearing Query performance:\nUse trace ID lookups when possible (fastest) TraceQL queries: Filter by indexed attributes first Limit time range to reduce blocks scanned Use with(span_sample=0.1) for approximate queries on large datasets Resource allocation (microservices mode):\nDistributors: 1 replica per 10MB/s of received traffic (2 CPU, 2GB) Ingesters: 1 replica per 3-5MB/s of traffic (2.5 CPU, 8-12GB) Queriers: 1 replica per 1-2MB/s of traffic (2 CPU, 4-8GB) Monitoring Tempo Itself Critical metrics to monitor:\ntempo_distributor_spans_received_total: Ingestion volume tempo_distributor_spans_rejected_total: Rejected spans (rate limiting) tempo_ingester_blocks_flushed_total: Block flush operations tempo_ingester_memory_usage: Memory consumption tempo_query_frontend_bytes_inspected_total: Query load tempo_compactor_blocks_compacted_total: Compaction activity Set up alerts for:\nIngester OOM risk (memory trending toward limits) Distributor span rejection rate \u003e 1% Compactor failures or lag Query response time \u003e SLA threshold Use Tempo’s built-in dashboards:\nImport from Tempo GitHub repo Includes: Tempo Reads, Tempo Writes, Tempo Resources, Tempo Operational When to Use Tempo Ideal Use Cases for Tempo 1. Cost-Conscious Organizations\nNeed to store high volumes of traces affordably Want to avoid expensive Cassandra or Elasticsearch infrastructure Require long-term retention (30+ days) at reasonable cost 2. Cloud-Native Applications\nAlready using or planning to use Grafana ecosystem (Loki, Mimir, Prometheus) Need trace-metric-log correlation for unified observability Deployed on Kubernetes with object storage available 3. Trace-ID-First Workflows\nDebugging starts with trace IDs from logs or metrics Use exemplars to jump from metric spikes to traces Don’t require ad-hoc exploratory search across all traces 4. High-Volume Tracing\nWant to store 100% of traces (no sampling) Traffic volume makes traditional backends prohibitively expensive Need to scale from thousands to millions of traces/second 5. Multi-Tenant Platforms\nSaaS platforms requiring tenant isolation Per-tenant retention and limits Cost attribution per tenant When to Choose Tempo vs. Jaeger Factor Choose Tempo Choose Jaeger Cost High trace volume, limited budget Cost not a primary concern Search Trace ID lookups + TraceQL queries Need powerful tag-based ad-hoc search Operations Want minimal infrastructure Have Cassandra/Elasticsearch expertise Integration Using Grafana ecosystem Need standalone solution Maturity Comfortable with newer project Require CNCF graduated project Sampling Want to store 100% of traces Acceptable to sample aggressively Tempo Advantages:\n10x+ lower storage costs Minimal operational complexity (no database management) Native Grafana integration with trace-metric-log correlation TraceQL provides powerful query capabilities Multi-tenancy support Jaeger Advantages:\nMature, battle-tested (CNCF graduated since 2019) Powerful tag-based search without needing trace ID Better for exploratory debugging of unknown issues Larger community and ecosystem More UI features out-of-the-box Migration path: Tempo can ingest Jaeger-format traces, enabling gradual migration.\nWhen to Choose Tempo vs. Zipkin Factor Choose Tempo Choose Zipkin Architecture Modern, cloud-native Traditional, database-backed Cost Object storage (very low) Database storage (medium-high) Query TraceQL (powerful) Tag-based search (simpler) Scale Millions of traces/second Moderate scale Integration Grafana ecosystem Standalone, Zipkin UI Tempo Advantages:\nMuch lower cost at scale More powerful query language (TraceQL) Better integration with modern observability stacks Horizontal scalability Zipkin Advantages:\nLonger history (since 2012), very mature Simpler architecture for small deployments Native Zipkin UI Migration path: Tempo can ingest Zipkin-format traces natively.\nWhen NOT to Use Tempo 1. Exploratory Debugging is Primary Workflow\nIf you frequently need to search traces without knowing trace IDs If you need to find “all traces for customer_id=123 with errors” ad-hoc Consider: Jaeger’s full indexing better suits this workflow 2. Standalone Tracing Requirements\nIf you need tracing independent of logs/metrics If you’re not using or planning to use Grafana Consider: Jaeger or Zipkin provide standalone UIs 3. Very Low Trace Volume\nIf you’re ingesting \u003c 1,000 traces/day If cost is not a concern Consider: Jaeger may be simpler for small scale 4. Need Immediate Full-Text Search\nIf you require searching trace content (span names, attribute values) without trace ID If TraceQL’s capabilities are insufficient Consider: Jaeger or Elasticsearch-backed solutions 5. Require Managed Solution\nIf you don’t want to manage infrastructure If you need enterprise support and SLAs Consider: Grafana Cloud Tempo, Honeycomb, or Lightstep BattleBots Integration Battle Workflow Tracing Tempo enables comprehensive tracing of the entire battle lifecycle in BattleBots:\nMatchmaking → Battle Execution → Results Persistence:\nTrace: Complete Battle (trace_id: battle_abc123) ├─ Span: MatchmakingRequest (50ms) │ ├─ Span: FindAvailablePlayers (database query, 30ms) │ └─ Span: CreateBattleSession (20ms) ├─ Span: InitializeBattle (200ms) │ ├─ Span: LoadBot1 (container start, 100ms) │ ├─ Span: LoadBot2 (container start, 100ms) │ └─ Span: InitializeGameState (10ms) ├─ Span: ExecuteBattle (5000ms) │ ├─ Span: GameLoop-Tick-1 (10ms) │ │ ├─ Span: Bot1Action-Move (5ms) │ │ ├─ Span: Bot2Action-Attack (5ms) │ │ └─ Span: UpdateState (2ms) │ ├─ Span: GameLoop-Tick-2 (10ms) │ │ ... │ └─ Span: GameLoop-Tick-N (10ms) └─ Span: PersistResults (100ms) ├─ Span: SaveBattleResults (database, 50ms) └─ Span: UpdateLeaderboard (50ms) Span attributes for battles:\nspan.battle.id = \"battle_abc123\" span.battle.mode = \"1v1\" span.bot.1.id = \"bot_xyz\" span.bot.2.id = \"bot_def\" span.game.tick = 42 span.action.type = \"attack\" span.action.success = true span.result.winner = \"bot_xyz\" TraceQL Queries for BattleBots Find slow bot actions:\n{ resource.service.name = \"game-server\" \u0026\u0026 span.action.type = \"bot_move\" \u0026\u0026 duration \u003e 100ms } Find all battles where a bot died:\n{ span.event.type = \"bot_death\" } Find battles with database latency issues:\n{ span.db.statement != nil \u0026\u0026 duration \u003e 500ms } Find error traces in matchmaking:\n{ resource.service.name = \"matchmaking\" \u0026\u0026 status = error } Aggregate: Count battles by result:\n{ span.result.winner != nil } | by(span.result.winner) | count() Unified Observability Workflows Workflow 1: Metric Spike → Trace → Logs\nMetric alert: Battle action latency P99 \u003e 500ms Jump to exemplar trace: Click exemplar in Grafana metric graph Identify slow span: See PersistState (database) span is 450ms View correlated logs: Click span → view logs with same trace_id Root cause: Logs show “database connection pool exhausted” Workflow 2: Trace → Metrics\nIdentify slow trace: Find trace with 10-second battle duration View service metrics: Click “Related Metrics” in Grafana Correlate: See CPU usage spike at same timestamp Conclusion: Resource contention caused slow battle Workflow 3: Log → Trace\nError log: See log line “failed to process bot action” Extract trace_id: Log contains trace_id=abc123 Query trace: Search Tempo for abc123 Analyze: See exact span where failure occurred with full context Integration with Loki and Mimir Complete LGTM Stack for BattleBots:\n┌─────────────────────────────────────────────────────┐ │ BattleBots Observability Stack │ ├─────────────────────────────────────────────────────┤ │ │ │ Game Servers / Bot Runtimes / Matchmaking │ │ (Instrumented with OpenTelemetry) │ │ ↓ │ │ OpenTelemetry Collector │ │ (receives logs, metrics, traces via OTLP) │ │ ↓ │ │ ┌───────────┼───────────┐ │ │ ↓ ↓ ↓ │ │ Loki Mimir Tempo │ │ (Logs) (Metrics) (Traces) │ │ ↓ ↓ ↓ │ │ Grafana │ │ (Unified Visualization \u0026 Correlation) │ │ │ │ Features: │ │ - Exemplars: Metric → Trace │ │ - Trace ID: Log → Trace │ │ - Service Graphs: Trace → Metrics │ │ - Unified Dashboards: All signals in one view │ └─────────────────────────────────────────────────────┘ Configuration for correlation:\nApplication instrumentation (Go example):\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/trace\" \"github.com/sirupsen/logrus\" ) // Inject trace ID into logs func logWithTrace(ctx context.Context, msg string) { span := trace.SpanFromContext(ctx) logger.WithFields(logrus.Fields{ \"trace_id\": span.SpanContext().TraceID().String(), \"span_id\": span.SpanContext().SpanID().String(), }).Info(msg) } Grafana datasource configuration:\ndatasources: - name: Tempo type: tempo jsonData: tracesToLogs: datasourceUid: loki-uid tags: ['trace_id'] tracesToMetrics: datasourceUid: mimir-uid - name: Loki type: loki jsonData: derivedFields: - name: TraceID matcherRegex: \"trace_id=(\\\\w+)\" url: \"$${__value.raw}\" datasourceUid: tempo-uid - name: Mimir type: prometheus jsonData: exemplarTraceIdDestinations: - name: trace_id datasourceUid: tempo-uid Further Reading Official Documentation Grafana Tempo Official Documentation Tempo GitHub Repository TraceQL Query Language Reference Tempo Configuration Reference Tempo Architecture Deep Dive Deployment and Operations Deploy Tempo using Docker Compose Monolithic and Microservices Modes Size the Cluster Best Practices for Traces Monitor Tempo TraceQL and Querying TraceQL Query Language Construct a TraceQL Query TraceQL Metrics Integration Guides Configure Tempo Data Source in Grafana Trace Discovery using Exemplars and Loki Introduction to Exemplars LGTM Stack: Loki, Grafana, Tempo, Mimir Comparisons and Alternatives Grafana Tempo vs Jaeger Jaeger vs Tempo - Key Features and Differences Open-Source Tracing Tools: Jaeger vs Zipkin vs Tempo Community Resources Grafana Labs Blog - Tempo Grafana Community Forums - Tempo CNCF OpenTelemetry Project Related BattleBots Documentation OpenTelemetry Collector: Traces Support - Understanding distributed tracing concepts Grafana Loki Overview - Log aggregation for correlation Grafana Mimir Overview - Metrics storage for correlation Observability Stack Overview - How Tempo fits into the complete observability architecture ","categories":"","description":"Comprehensive overview of Grafana Tempo distributed tracing backend, covering architecture, deployment modes, and operational best practices.\n","excerpt":"Comprehensive overview of Grafana Tempo distributed tracing backend, …","ref":"/battlebots/pr-preview/pr-153/research_and_development/analysis/observability/traces/tempo/tempo-overview/","tags":"","title":"Tempo: Overview"}]